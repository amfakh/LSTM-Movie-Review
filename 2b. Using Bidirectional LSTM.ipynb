{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "02. BLSTM v4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amfakh/LSTM-Movie-Review/blob/master/2b.%20Using%20Bidirectional%20LSTM\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0RAuwjiBIAv",
        "colab_type": "text"
      },
      "source": [
        "## Force Tensorflow version 1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcQfl2Xw3wj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "e4051152-e62b-42f3-9787-d46a42a2dd01"
      },
      "source": [
        "!pip install tensorflow==1.13.1\n",
        "!pip install -qq -U cufflinks\n",
        "!pip install matplotlib==3.1.0\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.17.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (3.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (42.0.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: matplotlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (1.17.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (2.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (2.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib==3.1.0) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.1.0) (42.0.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUdDGKLNhH7m",
        "colab_type": "text"
      },
      "source": [
        "## Access Google Drive files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "danJ2_x2WArZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f70d6376-442e-466e-f650-7fac33cf7124"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FebLd3UKY6bp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58b1bd6c-c348-4694-f567-a3d52d433941"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2TTdQzHVxtY",
        "colab_type": "text"
      },
      "source": [
        "# Read all the Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8R-ttCYh25P",
        "colab_type": "text"
      },
      "source": [
        "## Read the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B5dzs6P8BFTi",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKGJfAyLkaH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"df.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_SWTGHmjLA9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "73b31b21-b111-4172-f9af-5a98ef84cd0a"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>rating</th>\n",
              "      <th>title</th>\n",
              "      <th>review</th>\n",
              "      <th>number</th>\n",
              "      <th>cleaned</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>10/10</td>\n",
              "      <td>Unlike anything ever done in the history of ci...</td>\n",
              "      <td>This movie is the beginning of the culmination...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>beginning culmination masterfully woven cinema...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10/10</td>\n",
              "      <td>This movie will blow your mind and break your ...</td>\n",
              "      <td>Over the past decade, Marvel has earned itself...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>past decade earned benefit doubt consistently ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10/10</td>\n",
              "      <td>Way better than endgame</td>\n",
              "      <td>This film is way better than endgame!\\nThe act...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>way better action better writing better dialog...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>10/10</td>\n",
              "      <td>A Summer Film That IS Even Better Than The Hype</td>\n",
              "      <td>Summer movies often hype themselves as spectac...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>summer often hype spectacular event missed ad ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9/10</td>\n",
              "      <td>Excellent Film</td>\n",
              "      <td>I was amazed to see so many negative reviews; ...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>amazed negative impossible please hour long co...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... sentiment\n",
              "0           0  ...         1\n",
              "1           1  ...         1\n",
              "2           2  ...         1\n",
              "3           3  ...         1\n",
              "4           4  ...         1\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfVnOiW7Vxuw",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDWlhG7riIej",
        "colab_type": "text"
      },
      "source": [
        "## Split dataset into datatrain and datatest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKZ_qtURVxuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# for sent in df:\n",
        "train_review = df['cleaned'].values\n",
        "y_train = df['sentiment'].values\n",
        "\n",
        "    # train_review, test_review, y_train, y_test = train_test_split(\n",
        "    #     review, y, test_size=0.3, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_V1qN1DlR6C",
        "colab_type": "text"
      },
      "source": [
        "## Convert text to Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnxXFsJOj75b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b981939-7cdd-45d3-f7c1-3db2754bda0f"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Make sure the data type is str\n",
        "review_train = train_review.astype(str)\n",
        "# review_test = test_review.astype(str)\n",
        "\n",
        "tokenizer = Tokenizer(char_level=False)\n",
        "tokenizer.fit_on_texts(review_train)\n",
        "text_train = tokenizer.texts_to_sequences(review_train)\n",
        "# text_test = tokenizer.texts_to_sequences(review_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RKaFdrumRU8",
        "colab_type": "text"
      },
      "source": [
        "### (Additional) Declare the vocab size to maximum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeDzKUe2mPnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44b50a7b-e6b4-4859-fc06-424d75b40cf4"
      },
      "source": [
        "max_words = len(tokenizer.word_index) + 1  \n",
        "# Adding 1 because of reserved 0 index\n",
        "print('%s unique words.' % max_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37657 unique words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqSYqlz9ma2J",
        "colab_type": "text"
      },
      "source": [
        "### (Additional) Declare maximum length to maximum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdm00yw7Vxvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "10d3b4a9-f1db-44b3-d6df-14a51012db7a"
      },
      "source": [
        "text_len = [len(r) for r in text_train]\n",
        "print(\"average length: %0.1f\" % np.mean(text_len))\n",
        "print(\"max length: %d\" % max(text_len))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average length: 75.4\n",
            "max length: 899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRnRzsuJVxvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.hist(text_len, bins=500);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7ud35jqtATi",
        "colab_type": "text"
      },
      "source": [
        "## Declare max_words and max_len"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUIFIA_KVxvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of words to consider as features\n",
        "# max_words = 50000\n",
        "# Cut texts after this number of words (among top max_features most common words)\n",
        "max_len = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhpgG68KmjPc",
        "colab_type": "text"
      },
      "source": [
        "## Pad Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QuVFfpS8TCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4exEzKqvVxvq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6350c057-4800-459e-c20b-9c577967590d"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# max_length = max(text_len)\n",
        "\n",
        "# pad sequences with 0s\n",
        "x = pad_sequences(text_train, maxlen=max_len)\n",
        "# x_test = pad_sequences(text_test, maxlen=max_len)\n",
        "print('Shape of data tensor:', x.shape)\n",
        "# print('Shape of data test tensor:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (25136, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAAG8Xuint1U",
        "colab_type": "text"
      },
      "source": [
        "### Check the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gTZu3xAVxvs",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# print(review_train[4])\n",
        "# print(x_train[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avB7woI-nYFq",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# print(y_train[0])\n",
        "# print(y_test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJzDy6xEn-FK",
        "colab_type": "text"
      },
      "source": [
        "Convert label to float"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN4W32bErzB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
        "# y_test = np.asarray(y_test).astype('float32').reshape((-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aYup7IgoCah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4940062e-7eb0-46c2-fc2c-88ac4906c98e"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYYmmalwOsVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras import utils\n",
        "# num_classes = 2\n",
        "\n",
        "# y_train_binary = utils.to_categorical(y_train, num_classes)\n",
        "# y_test_binary = utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nR_WOPVPB72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(y_train_binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g70IGUjUBxxb",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAOsnygio29e",
        "colab_type": "text"
      },
      "source": [
        "## Import stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "906eUoh_o1zm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "76c5a0e3-6677-4d21-c71e-5abaa9a73bbc"
      },
      "source": [
        "from tensorflow.python.keras.layers import Dropout, Dense, Embedding\n",
        "from tensorflow.python.keras.layers import SpatialDropout1D, GlobalMaxPool1D\n",
        "from tensorflow.python.keras.layers import LSTM, Input, Bidirectional, GRU\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n",
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF9_O4lIVxvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = x.shape[1]  # Number of features\n",
        "embedding_dim = 128\n",
        "learning_rate = 2e-5\n",
        "epochs = 500\n",
        "decay_rate = 1e-3 # learning_rate / epochs\n",
        "\n",
        "def make_model(batch_size=None):\n",
        "  source = Input(shape=(max_len,), name='Input')\n",
        "  embedding = Embedding(input_dim=max_words,\n",
        "                        output_dim=embedding_dim,\n",
        "                        input_length=max_len)(source)\n",
        "  # spatial = SpatialDropout1D(0.4)(embedding)\n",
        "  lstm = Bidirectional(LSTM(128, \n",
        "              # activation='softsign',\n",
        "              dropout=0.3,\n",
        "              recurrent_dropout=0.3,\n",
        "              # use_bias=True\n",
        "              # ,return_sequences=True\n",
        "              ))(embedding)\n",
        "  # pool = GlobalMaxPool1D()(lstm)\n",
        "  # dense = Dense(24)(lstm)\n",
        "  # drop = Dropout(0.4)(dense)\n",
        "\n",
        "  # lstm = Bidirectional(LSTM(196, activation='softsign',\n",
        "  #           dropout=0.7,\n",
        "  #           recurrent_dropout=0.7,\n",
        "  #           use_bias=True\n",
        "  #           # ,return_sequences=True\n",
        "  #           ))(drop)\n",
        "  # dense = Dense(50)(lstm)\n",
        "  # drop = Dropout(0.7)(dense)\n",
        "\n",
        "  predict = Dense(1, activation='sigmoid')(lstm)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[source], outputs=[predict])\n",
        "\n",
        "  rmsprop = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=decay_rate)\n",
        "  model.compile(\n",
        "      optimizer=rmsprop,\n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EenkZdsjvHsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "# tpu_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rnwt-lEyKTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1ChsH4VVxv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Train acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Train and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Train loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Train and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUDhBL7kv_9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve,roc_auc_score\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "# sns.set()\n",
        "# sns.heatmap(confusion.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "#             xticklabels=np.unique(y_pred),\n",
        "#             yticklabels=np.unique(y_pred))\n",
        "# plt.xlabel('true label')\n",
        "# plt.ylabel('predicted label')\n",
        "# plt.title('Confusion Matrix Prediction')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0h77kZoUcym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd8dc93e-0355-4f9b-922f-4da56cfe6a55"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import time, math\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\n",
        "\n",
        "tr_acc_array = []\n",
        "tr_loss_array = []\n",
        "te_acc_array = []\n",
        "te_loss_array = []\n",
        "time_array = []\n",
        "rmse_array = []\n",
        "\n",
        "n=0\n",
        "\n",
        "\n",
        "for train, test in kfold.split(x, y):\n",
        "  n+=1\n",
        "  print(\"--- Fold %d ---\" % (n))\n",
        "  tf.keras.backend.clear_session()\n",
        "  training_model = make_model(batch_size = 128)\n",
        "  # training_model.summary()\n",
        "\n",
        "  tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    training_model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "  start_time = time.time()\n",
        "  history = tpu_model.fit(x[train], y[train]\n",
        "                    ,epochs=epochs, verbose=1 \n",
        "                    ,validation_split=0.2\n",
        "                    ,batch_size=128 * 8\n",
        "                    ,validation_data=(x[test], y[test])\n",
        "                    ,callbacks=[es]\n",
        "                   )\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  time_array.append(time.time() - start_time)\n",
        "\n",
        "  model = tpu_model.sync_to_cpu()\n",
        "\n",
        "  tr_loss, tr_accuracy = model.evaluate(x[train], y[train], verbose=1)\n",
        "  te_loss, te_accuracy = model.evaluate(x[test], y[test], verbose=1)\n",
        "  print(\"Train Accuracy: {:.4f}\".format(tr_accuracy))\n",
        "  tr_acc_array.append(tr_accuracy)\n",
        "  print(\"Train Loss: {:.4f}\".format(tr_loss))\n",
        "  tr_loss_array.append(tr_loss)\n",
        "  print(\"Validation Accuracy:  {:.4f}\".format(te_accuracy))\n",
        "  te_acc_array.append(te_accuracy)\n",
        "  print(\"Validation Loss: {:.4f}\".format(te_loss))\n",
        "  te_loss_array.append(te_loss)\n",
        "\n",
        "  pred = model.predict(x[test], batch_size=1000, verbose = 1)\n",
        "  y_pred = y_pred = (pred > 0.5)\n",
        "\n",
        "  print(confusion_matrix(y[test], y_pred))\n",
        "  # sns.set()\n",
        "  # sns.heatmap(confusion.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "  #             xticklabels=np.unique(y_pred),\n",
        "  #             yticklabels=np.unique(y_pred))\n",
        "  # plt.xlabel('true label')\n",
        "  # plt.ylabel('predicted label')\n",
        "  # plt.title('Confusion Matrix Prediction')\n",
        "\n",
        "\n",
        "  print(classification_report(y[test], y_pred))\n",
        "\n",
        "  # calculate root mean squared error\n",
        "  rmse = math.sqrt(mean_squared_error(y[test], y_pred))\n",
        "  print('RMSE: %.4f' % (rmse))\n",
        "  rmse_array.append(rmse)\n",
        "  # testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "  # print('Test Score: %.2f RMSE' % (testScore))\n",
        "\n",
        "  # auc_score=roc_auc_score(y[test], y_pred)  \n",
        "  # print('AUC: %.2f' % (auc_score))\n",
        "\n",
        "  # cvscores.append(scores[1] * 100)\n",
        "\n",
        "  # plot_history(history)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Fold 1 ---\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.76.36.210:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1277633526131077744)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6387720827749024011)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10496896205887535107)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6969718561809677301)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12961930590832392281)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 103926089697679672)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8238627829294133264)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2041167189725682759)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6959296792125428166)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4386377055783322133)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 302002105411306474)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20108 samples, validate on 5028 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 7.224149942398071 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20108 [==========================>...] - ETA: 2s - loss: 0.6886 - acc: 0.6517INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 8.167717218399048 secs\n",
            "19456/20108 [============================>.] - ETA: 1s - loss: 0.6884 - acc: 0.6542INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 6.71899676322937 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 8.135157823562622 secs\n",
            "20108/20108 [==============================] - 62s 3ms/sample - loss: 0.6882 - acc: 0.6564 - val_loss: 0.6837 - val_acc: 0.7002\n",
            "Epoch 2/500\n",
            "20108/20108 [==============================] - 6s 322us/sample - loss: 0.6797 - acc: 0.7010 - val_loss: 0.6752 - val_acc: 0.7014\n",
            "Epoch 3/500\n",
            "20108/20108 [==============================] - 7s 333us/sample - loss: 0.6710 - acc: 0.7017 - val_loss: 0.6660 - val_acc: 0.7014\n",
            "Epoch 4/500\n",
            "20108/20108 [==============================] - 6s 321us/sample - loss: 0.6611 - acc: 0.7017 - val_loss: 0.6559 - val_acc: 0.7014\n",
            "Epoch 5/500\n",
            "20108/20108 [==============================] - 7s 325us/sample - loss: 0.6507 - acc: 0.7016 - val_loss: 0.6443 - val_acc: 0.7014\n",
            "Epoch 6/500\n",
            "20108/20108 [==============================] - 7s 336us/sample - loss: 0.6382 - acc: 0.7017 - val_loss: 0.6313 - val_acc: 0.7014\n",
            "Epoch 7/500\n",
            "20108/20108 [==============================] - 7s 336us/sample - loss: 0.6247 - acc: 0.7016 - val_loss: 0.6173 - val_acc: 0.7014\n",
            "Epoch 8/500\n",
            "20108/20108 [==============================] - 7s 335us/sample - loss: 0.6144 - acc: 0.7017 - val_loss: 0.6104 - val_acc: 0.7014\n",
            "Epoch 9/500\n",
            "20108/20108 [==============================] - 7s 338us/sample - loss: 0.6123 - acc: 0.7017 - val_loss: 0.6101 - val_acc: 0.7014\n",
            "Epoch 10/500\n",
            "20108/20108 [==============================] - 7s 335us/sample - loss: 0.6111 - acc: 0.7016 - val_loss: 0.6098 - val_acc: 0.7014\n",
            "Epoch 11/500\n",
            "20108/20108 [==============================] - 6s 315us/sample - loss: 0.6112 - acc: 0.7017 - val_loss: 0.6098 - val_acc: 0.7014\n",
            "Epoch 12/500\n",
            "20108/20108 [==============================] - 6s 322us/sample - loss: 0.6113 - acc: 0.7016 - val_loss: 0.6095 - val_acc: 0.7014\n",
            "Epoch 13/500\n",
            "20108/20108 [==============================] - 6s 320us/sample - loss: 0.6108 - acc: 0.7017 - val_loss: 0.6092 - val_acc: 0.7014\n",
            "Epoch 14/500\n",
            "20108/20108 [==============================] - 7s 344us/sample - loss: 0.6108 - acc: 0.7018 - val_loss: 0.6091 - val_acc: 0.7014\n",
            "Epoch 15/500\n",
            "20108/20108 [==============================] - 7s 330us/sample - loss: 0.6103 - acc: 0.7016 - val_loss: 0.6090 - val_acc: 0.7014\n",
            "Epoch 16/500\n",
            "20108/20108 [==============================] - 7s 334us/sample - loss: 0.6099 - acc: 0.7017 - val_loss: 0.6087 - val_acc: 0.7014\n",
            "Epoch 17/500\n",
            "20108/20108 [==============================] - 7s 332us/sample - loss: 0.6093 - acc: 0.7017 - val_loss: 0.6084 - val_acc: 0.7014\n",
            "Epoch 18/500\n",
            "20108/20108 [==============================] - 6s 318us/sample - loss: 0.6090 - acc: 0.7017 - val_loss: 0.6082 - val_acc: 0.7014\n",
            "Epoch 19/500\n",
            "20108/20108 [==============================] - 7s 327us/sample - loss: 0.6096 - acc: 0.7016 - val_loss: 0.6079 - val_acc: 0.7014\n",
            "Epoch 20/500\n",
            "20108/20108 [==============================] - 7s 333us/sample - loss: 0.6087 - acc: 0.7016 - val_loss: 0.6076 - val_acc: 0.7014\n",
            "Epoch 21/500\n",
            "20108/20108 [==============================] - 7s 333us/sample - loss: 0.6083 - acc: 0.7017 - val_loss: 0.6073 - val_acc: 0.7014\n",
            "Epoch 22/500\n",
            "20108/20108 [==============================] - 6s 322us/sample - loss: 0.6076 - acc: 0.7017 - val_loss: 0.6070 - val_acc: 0.7014\n",
            "Epoch 23/500\n",
            "20108/20108 [==============================] - 7s 342us/sample - loss: 0.6078 - acc: 0.7017 - val_loss: 0.6066 - val_acc: 0.7014\n",
            "Epoch 24/500\n",
            "20108/20108 [==============================] - 7s 328us/sample - loss: 0.6074 - acc: 0.7017 - val_loss: 0.6062 - val_acc: 0.7014\n",
            "Epoch 25/500\n",
            "20108/20108 [==============================] - 7s 326us/sample - loss: 0.6067 - acc: 0.7017 - val_loss: 0.6058 - val_acc: 0.7014\n",
            "Epoch 26/500\n",
            "20108/20108 [==============================] - 6s 323us/sample - loss: 0.6063 - acc: 0.7016 - val_loss: 0.6053 - val_acc: 0.7014\n",
            "Epoch 27/500\n",
            "20108/20108 [==============================] - 7s 337us/sample - loss: 0.6052 - acc: 0.7017 - val_loss: 0.6048 - val_acc: 0.7014\n",
            "Epoch 28/500\n",
            "20108/20108 [==============================] - 7s 345us/sample - loss: 0.6047 - acc: 0.7017 - val_loss: 0.6042 - val_acc: 0.7014\n",
            "Epoch 29/500\n",
            "20108/20108 [==============================] - 7s 333us/sample - loss: 0.6041 - acc: 0.7017 - val_loss: 0.6036 - val_acc: 0.7014\n",
            "Epoch 30/500\n",
            "20108/20108 [==============================] - 7s 330us/sample - loss: 0.6035 - acc: 0.7016 - val_loss: 0.6028 - val_acc: 0.7014\n",
            "Epoch 31/500\n",
            "20108/20108 [==============================] - 7s 329us/sample - loss: 0.6033 - acc: 0.7017 - val_loss: 0.6022 - val_acc: 0.7014\n",
            "Epoch 32/500\n",
            "20108/20108 [==============================] - 7s 330us/sample - loss: 0.6017 - acc: 0.7017 - val_loss: 0.6011 - val_acc: 0.7014\n",
            "Epoch 33/500\n",
            "20108/20108 [==============================] - 7s 329us/sample - loss: 0.6006 - acc: 0.7017 - val_loss: 0.6002 - val_acc: 0.7014\n",
            "Epoch 34/500\n",
            "20108/20108 [==============================] - 6s 318us/sample - loss: 0.6001 - acc: 0.7017 - val_loss: 0.5992 - val_acc: 0.7014\n",
            "Epoch 35/500\n",
            "20108/20108 [==============================] - 7s 332us/sample - loss: 0.5987 - acc: 0.7017 - val_loss: 0.5979 - val_acc: 0.7014\n",
            "Epoch 36/500\n",
            "20108/20108 [==============================] - 7s 324us/sample - loss: 0.5974 - acc: 0.7017 - val_loss: 0.5965 - val_acc: 0.7014\n",
            "Epoch 37/500\n",
            "20108/20108 [==============================] - 7s 331us/sample - loss: 0.5958 - acc: 0.7016 - val_loss: 0.5950 - val_acc: 0.7014\n",
            "Epoch 38/500\n",
            "20108/20108 [==============================] - 7s 328us/sample - loss: 0.5946 - acc: 0.7017 - val_loss: 0.5934 - val_acc: 0.7014\n",
            "Epoch 39/500\n",
            "20108/20108 [==============================] - 6s 317us/sample - loss: 0.5926 - acc: 0.7017 - val_loss: 0.5915 - val_acc: 0.7014\n",
            "Epoch 40/500\n",
            "20108/20108 [==============================] - 7s 332us/sample - loss: 0.5908 - acc: 0.7016 - val_loss: 0.5895 - val_acc: 0.7014\n",
            "Epoch 41/500\n",
            "20108/20108 [==============================] - 7s 325us/sample - loss: 0.5886 - acc: 0.7017 - val_loss: 0.5870 - val_acc: 0.7014\n",
            "Epoch 42/500\n",
            "20108/20108 [==============================] - 7s 331us/sample - loss: 0.5861 - acc: 0.7017 - val_loss: 0.5843 - val_acc: 0.7014\n",
            "Epoch 43/500\n",
            "20108/20108 [==============================] - 6s 323us/sample - loss: 0.5823 - acc: 0.7017 - val_loss: 0.5811 - val_acc: 0.7014\n",
            "Epoch 44/500\n",
            "20108/20108 [==============================] - 7s 343us/sample - loss: 0.5802 - acc: 0.7016 - val_loss: 0.5782 - val_acc: 0.7014\n",
            "Epoch 45/500\n",
            "20108/20108 [==============================] - 7s 336us/sample - loss: 0.5763 - acc: 0.7018 - val_loss: 0.5737 - val_acc: 0.7014\n",
            "Epoch 46/500\n",
            "20108/20108 [==============================] - 7s 327us/sample - loss: 0.5722 - acc: 0.7017 - val_loss: 0.5694 - val_acc: 0.7014\n",
            "Epoch 47/500\n",
            "20108/20108 [==============================] - 7s 326us/sample - loss: 0.5688 - acc: 0.7017 - val_loss: 0.5652 - val_acc: 0.7014\n",
            "Epoch 48/500\n",
            "20108/20108 [==============================] - 7s 333us/sample - loss: 0.5646 - acc: 0.7017 - val_loss: 0.5602 - val_acc: 0.7014\n",
            "Epoch 49/500\n",
            "20108/20108 [==============================] - 7s 324us/sample - loss: 0.5596 - acc: 0.7018 - val_loss: 0.5535 - val_acc: 0.7014\n",
            "Epoch 50/500\n",
            "20108/20108 [==============================] - 7s 336us/sample - loss: 0.5530 - acc: 0.7019 - val_loss: 0.5489 - val_acc: 0.7016\n",
            "Epoch 51/500\n",
            "20108/20108 [==============================] - 7s 341us/sample - loss: 0.5494 - acc: 0.7026 - val_loss: 0.5425 - val_acc: 0.7016\n",
            "Epoch 52/500\n",
            "20108/20108 [==============================] - 7s 332us/sample - loss: 0.5424 - acc: 0.7047 - val_loss: 0.5353 - val_acc: 0.7022\n",
            "Epoch 53/500\n",
            "20108/20108 [==============================] - 7s 334us/sample - loss: 0.5361 - acc: 0.7068 - val_loss: 0.5276 - val_acc: 0.7032\n",
            "Epoch 54/500\n",
            "20108/20108 [==============================] - 7s 341us/sample - loss: 0.5314 - acc: 0.7119 - val_loss: 0.5223 - val_acc: 0.7076\n",
            "Epoch 55/500\n",
            "20108/20108 [==============================] - 6s 320us/sample - loss: 0.5243 - acc: 0.7170 - val_loss: 0.5142 - val_acc: 0.7132\n",
            "Epoch 56/500\n",
            "20108/20108 [==============================] - 7s 335us/sample - loss: 0.5191 - acc: 0.7266 - val_loss: 0.5076 - val_acc: 0.7199\n",
            "Epoch 57/500\n",
            "20108/20108 [==============================] - 7s 333us/sample - loss: 0.5126 - acc: 0.7327 - val_loss: 0.5009 - val_acc: 0.7263\n",
            "Epoch 58/500\n",
            "20108/20108 [==============================] - 7s 331us/sample - loss: 0.5069 - acc: 0.7422 - val_loss: 0.4915 - val_acc: 0.7327\n",
            "Epoch 59/500\n",
            "20108/20108 [==============================] - 7s 325us/sample - loss: 0.4998 - acc: 0.7467 - val_loss: 0.4823 - val_acc: 0.7416\n",
            "Epoch 60/500\n",
            "20108/20108 [==============================] - 7s 336us/sample - loss: 0.4929 - acc: 0.7594 - val_loss: 0.4792 - val_acc: 0.7558\n",
            "Epoch 61/500\n",
            "20108/20108 [==============================] - 7s 339us/sample - loss: 0.4864 - acc: 0.7666 - val_loss: 0.4730 - val_acc: 0.7683\n",
            "Epoch 62/500\n",
            "20108/20108 [==============================] - 7s 327us/sample - loss: 0.4813 - acc: 0.7740 - val_loss: 0.4705 - val_acc: 0.7747\n",
            "Epoch 63/500\n",
            "20108/20108 [==============================] - 7s 328us/sample - loss: 0.4757 - acc: 0.7796 - val_loss: 0.4603 - val_acc: 0.7830\n",
            "Epoch 64/500\n",
            "20108/20108 [==============================] - 7s 335us/sample - loss: 0.4694 - acc: 0.7896 - val_loss: 0.4552 - val_acc: 0.7894\n",
            "Epoch 65/500\n",
            "20108/20108 [==============================] - 7s 332us/sample - loss: 0.4628 - acc: 0.7927 - val_loss: 0.4474 - val_acc: 0.7960\n",
            "Epoch 66/500\n",
            "20108/20108 [==============================] - 7s 327us/sample - loss: 0.4583 - acc: 0.7954 - val_loss: 0.4392 - val_acc: 0.7982\n",
            "Epoch 67/500\n",
            "20108/20108 [==============================] - 7s 335us/sample - loss: 0.4499 - acc: 0.8025 - val_loss: 0.4332 - val_acc: 0.8041\n",
            "Epoch 68/500\n",
            "20108/20108 [==============================] - 7s 329us/sample - loss: 0.4468 - acc: 0.8056 - val_loss: 0.4287 - val_acc: 0.8095\n",
            "Epoch 69/500\n",
            "20108/20108 [==============================] - 7s 336us/sample - loss: 0.4427 - acc: 0.8090 - val_loss: 0.4302 - val_acc: 0.8093\n",
            "Epoch 70/500\n",
            "20108/20108 [==============================] - 7s 333us/sample - loss: 0.4383 - acc: 0.8128 - val_loss: 0.4235 - val_acc: 0.8119\n",
            "Epoch 71/500\n",
            "20108/20108 [==============================] - 6s 317us/sample - loss: 0.4329 - acc: 0.8125 - val_loss: 0.4233 - val_acc: 0.8141\n",
            "Epoch 72/500\n",
            "20108/20108 [==============================] - 7s 340us/sample - loss: 0.4273 - acc: 0.8139 - val_loss: 0.4168 - val_acc: 0.8159\n",
            "Epoch 73/500\n",
            "20108/20108 [==============================] - 7s 331us/sample - loss: 0.4259 - acc: 0.8175 - val_loss: 0.4142 - val_acc: 0.8179\n",
            "Epoch 74/500\n",
            "20108/20108 [==============================] - 7s 332us/sample - loss: 0.4184 - acc: 0.8216 - val_loss: 0.4060 - val_acc: 0.8195\n",
            "Epoch 75/500\n",
            "20108/20108 [==============================] - 7s 338us/sample - loss: 0.4151 - acc: 0.8206 - val_loss: 0.4017 - val_acc: 0.8201\n",
            "Epoch 76/500\n",
            "20108/20108 [==============================] - 7s 331us/sample - loss: 0.4134 - acc: 0.8211 - val_loss: 0.4007 - val_acc: 0.8213\n",
            "Epoch 77/500\n",
            "20108/20108 [==============================] - 7s 340us/sample - loss: 0.4115 - acc: 0.8279 - val_loss: 0.4006 - val_acc: 0.8240\n",
            "Epoch 78/500\n",
            "20108/20108 [==============================] - 7s 331us/sample - loss: 0.4046 - acc: 0.8274 - val_loss: 0.3950 - val_acc: 0.8268\n",
            "Epoch 79/500\n",
            "20108/20108 [==============================] - 6s 319us/sample - loss: 0.4017 - acc: 0.8298 - val_loss: 0.3883 - val_acc: 0.8254\n",
            "Epoch 80/500\n",
            "20108/20108 [==============================] - 7s 326us/sample - loss: 0.4002 - acc: 0.8296 - val_loss: 0.3898 - val_acc: 0.8292\n",
            "Epoch 81/500\n",
            "20108/20108 [==============================] - 7s 332us/sample - loss: 0.3946 - acc: 0.8363 - val_loss: 0.3853 - val_acc: 0.8314\n",
            "Epoch 82/500\n",
            "20108/20108 [==============================] - 6s 320us/sample - loss: 0.3918 - acc: 0.8339 - val_loss: 0.3839 - val_acc: 0.8318\n",
            "Epoch 83/500\n",
            "20108/20108 [==============================] - 6s 321us/sample - loss: 0.3900 - acc: 0.8348 - val_loss: 0.3853 - val_acc: 0.8314\n",
            "Epoch 84/500\n",
            "20108/20108 [==============================] - 7s 325us/sample - loss: 0.3880 - acc: 0.8365 - val_loss: 0.3783 - val_acc: 0.8354\n",
            "Epoch 85/500\n",
            "20108/20108 [==============================] - 7s 331us/sample - loss: 0.3834 - acc: 0.8385 - val_loss: 0.3795 - val_acc: 0.8356\n",
            "Epoch 86/500\n",
            "20108/20108 [==============================] - 7s 330us/sample - loss: 0.3828 - acc: 0.8396 - val_loss: 0.3751 - val_acc: 0.8372\n",
            "Epoch 87/500\n",
            "20108/20108 [==============================] - 6s 322us/sample - loss: 0.3775 - acc: 0.8418 - val_loss: 0.3714 - val_acc: 0.8384\n",
            "Epoch 88/500\n",
            "20108/20108 [==============================] - 7s 325us/sample - loss: 0.3743 - acc: 0.8437 - val_loss: 0.3693 - val_acc: 0.8394\n",
            "Epoch 89/500\n",
            "20108/20108 [==============================] - 7s 353us/sample - loss: 0.3746 - acc: 0.8419 - val_loss: 0.3702 - val_acc: 0.8394\n",
            "Epoch 90/500\n",
            "20108/20108 [==============================] - 7s 326us/sample - loss: 0.3697 - acc: 0.8455 - val_loss: 0.3703 - val_acc: 0.8382\n",
            "Epoch 91/500\n",
            "20108/20108 [==============================] - 7s 326us/sample - loss: 0.3673 - acc: 0.8474 - val_loss: 0.3665 - val_acc: 0.8392\n",
            "Epoch 92/500\n",
            "20108/20108 [==============================] - 7s 334us/sample - loss: 0.3639 - acc: 0.8479 - val_loss: 0.3656 - val_acc: 0.8402\n",
            "Epoch 93/500\n",
            "20108/20108 [==============================] - 6s 320us/sample - loss: 0.3638 - acc: 0.8495 - val_loss: 0.3663 - val_acc: 0.8400\n",
            "Epoch 94/500\n",
            "20108/20108 [==============================] - 7s 329us/sample - loss: 0.3651 - acc: 0.8489 - val_loss: 0.3642 - val_acc: 0.8402\n",
            "Epoch 95/500\n",
            "20108/20108 [==============================] - 7s 327us/sample - loss: 0.3607 - acc: 0.8518 - val_loss: 0.3604 - val_acc: 0.8420\n",
            "Epoch 96/500\n",
            "20108/20108 [==============================] - 6s 323us/sample - loss: 0.3569 - acc: 0.8531 - val_loss: 0.3622 - val_acc: 0.8412\n",
            "Epoch 97/500\n",
            "20108/20108 [==============================] - 7s 331us/sample - loss: 0.3555 - acc: 0.8529 - val_loss: 0.3591 - val_acc: 0.8406\n",
            "Epoch 98/500\n",
            "20108/20108 [==============================] - 7s 328us/sample - loss: 0.3522 - acc: 0.8533 - val_loss: 0.3576 - val_acc: 0.8420\n",
            "Epoch 99/500\n",
            "20108/20108 [==============================] - 7s 338us/sample - loss: 0.3562 - acc: 0.8527 - val_loss: 0.3599 - val_acc: 0.8420\n",
            "Epoch 100/500\n",
            "20108/20108 [==============================] - 7s 343us/sample - loss: 0.3546 - acc: 0.8512 - val_loss: 0.3585 - val_acc: 0.8418\n",
            "Epoch 101/500\n",
            "20108/20108 [==============================] - 6s 323us/sample - loss: 0.3475 - acc: 0.8565 - val_loss: 0.3563 - val_acc: 0.8430\n",
            "Epoch 102/500\n",
            "20108/20108 [==============================] - 7s 338us/sample - loss: 0.3464 - acc: 0.8590 - val_loss: 0.3543 - val_acc: 0.8424\n",
            "Epoch 103/500\n",
            "20108/20108 [==============================] - 7s 335us/sample - loss: 0.3468 - acc: 0.8573 - val_loss: 0.3529 - val_acc: 0.8447\n",
            "Epoch 104/500\n",
            "20108/20108 [==============================] - 6s 321us/sample - loss: 0.3477 - acc: 0.8567 - val_loss: 0.3549 - val_acc: 0.8426\n",
            "Epoch 105/500\n",
            "20108/20108 [==============================] - 7s 332us/sample - loss: 0.3445 - acc: 0.8594 - val_loss: 0.3533 - val_acc: 0.8436\n",
            "Epoch 106/500\n",
            "20108/20108 [==============================] - 7s 326us/sample - loss: 0.3442 - acc: 0.8572 - val_loss: 0.3544 - val_acc: 0.8432\n",
            "Epoch 107/500\n",
            "20108/20108 [==============================] - 7s 328us/sample - loss: 0.3418 - acc: 0.8598 - val_loss: 0.3521 - val_acc: 0.8441\n",
            "Epoch 108/500\n",
            "20108/20108 [==============================] - 6s 320us/sample - loss: 0.3386 - acc: 0.8607 - val_loss: 0.3519 - val_acc: 0.8447\n",
            "Epoch 109/500\n",
            "20108/20108 [==============================] - 7s 327us/sample - loss: 0.3376 - acc: 0.8618 - val_loss: 0.3503 - val_acc: 0.8467\n",
            "Epoch 110/500\n",
            "20108/20108 [==============================] - 7s 325us/sample - loss: 0.3385 - acc: 0.8610 - val_loss: 0.3497 - val_acc: 0.8465\n",
            "Epoch 111/500\n",
            "20108/20108 [==============================] - 7s 325us/sample - loss: 0.3346 - acc: 0.8649 - val_loss: 0.3502 - val_acc: 0.8469\n",
            "Epoch 112/500\n",
            "20108/20108 [==============================] - 7s 330us/sample - loss: 0.3342 - acc: 0.8635 - val_loss: 0.3498 - val_acc: 0.8451\n",
            "Epoch 113/500\n",
            "20108/20108 [==============================] - 7s 334us/sample - loss: 0.3306 - acc: 0.8657 - val_loss: 0.3486 - val_acc: 0.8463\n",
            "Epoch 114/500\n",
            "20108/20108 [==============================] - 6s 319us/sample - loss: 0.3338 - acc: 0.8648 - val_loss: 0.3478 - val_acc: 0.8481\n",
            "Epoch 115/500\n",
            "20108/20108 [==============================] - 7s 329us/sample - loss: 0.3301 - acc: 0.8651 - val_loss: 0.3476 - val_acc: 0.8497\n",
            "Epoch 116/500\n",
            "20108/20108 [==============================] - 7s 337us/sample - loss: 0.3290 - acc: 0.8671 - val_loss: 0.3473 - val_acc: 0.8491\n",
            "Epoch 117/500\n",
            "20108/20108 [==============================] - 7s 341us/sample - loss: 0.3276 - acc: 0.8673 - val_loss: 0.3467 - val_acc: 0.8495\n",
            "Epoch 118/500\n",
            "20108/20108 [==============================] - 7s 335us/sample - loss: 0.3227 - acc: 0.8681 - val_loss: 0.3466 - val_acc: 0.8497\n",
            "Epoch 119/500\n",
            "20108/20108 [==============================] - 6s 323us/sample - loss: 0.3250 - acc: 0.8671 - val_loss: 0.3459 - val_acc: 0.8517\n",
            "Epoch 120/500\n",
            "20108/20108 [==============================] - 7s 328us/sample - loss: 0.3242 - acc: 0.8687 - val_loss: 0.3458 - val_acc: 0.8509\n",
            "Epoch 121/500\n",
            "20108/20108 [==============================] - 6s 319us/sample - loss: 0.3223 - acc: 0.8681 - val_loss: 0.3457 - val_acc: 0.8501\n",
            "Epoch 122/500\n",
            "20108/20108 [==============================] - 7s 327us/sample - loss: 0.3216 - acc: 0.8707 - val_loss: 0.3446 - val_acc: 0.8537\n",
            "Epoch 123/500\n",
            "20108/20108 [==============================] - 7s 328us/sample - loss: 0.3205 - acc: 0.8715 - val_loss: 0.3444 - val_acc: 0.8533\n",
            "Epoch 124/500\n",
            "20108/20108 [==============================] - 6s 322us/sample - loss: 0.3198 - acc: 0.8705 - val_loss: 0.3439 - val_acc: 0.8547\n",
            "Epoch 125/500\n",
            "20108/20108 [==============================] - 7s 329us/sample - loss: 0.3173 - acc: 0.8730 - val_loss: 0.3444 - val_acc: 0.8525\n",
            "Epoch 126/500\n",
            "20108/20108 [==============================] - 7s 336us/sample - loss: 0.3162 - acc: 0.8727 - val_loss: 0.3438 - val_acc: 0.8533\n",
            "Epoch 127/500\n",
            "20108/20108 [==============================] - 6s 319us/sample - loss: 0.3152 - acc: 0.8734 - val_loss: 0.3434 - val_acc: 0.8529\n",
            "Epoch 128/500\n",
            "20108/20108 [==============================] - 7s 327us/sample - loss: 0.3133 - acc: 0.8705 - val_loss: 0.3433 - val_acc: 0.8535\n",
            "Epoch 129/500\n",
            "20108/20108 [==============================] - 7s 326us/sample - loss: 0.3121 - acc: 0.8741 - val_loss: 0.3430 - val_acc: 0.8529\n",
            "Epoch 130/500\n",
            "20108/20108 [==============================] - 6s 321us/sample - loss: 0.3132 - acc: 0.8750 - val_loss: 0.3426 - val_acc: 0.8545\n",
            "Epoch 131/500\n",
            "20108/20108 [==============================] - 6s 322us/sample - loss: 0.3101 - acc: 0.8756 - val_loss: 0.3430 - val_acc: 0.8525\n",
            "Epoch 132/500\n",
            "20108/20108 [==============================] - 7s 344us/sample - loss: 0.3068 - acc: 0.8743 - val_loss: 0.3426 - val_acc: 0.8531\n",
            "Epoch 133/500\n",
            "20108/20108 [==============================] - 6s 321us/sample - loss: 0.3106 - acc: 0.8758 - val_loss: 0.3420 - val_acc: 0.8535\n",
            "Epoch 134/500\n",
            "20108/20108 [==============================] - 7s 330us/sample - loss: 0.3088 - acc: 0.8761 - val_loss: 0.3424 - val_acc: 0.8531\n",
            "Epoch 135/500\n",
            "20108/20108 [==============================] - 6s 321us/sample - loss: 0.3024 - acc: 0.8787 - val_loss: 0.3424 - val_acc: 0.8539\n",
            "Epoch 136/500\n",
            "20108/20108 [==============================] - 6s 320us/sample - loss: 0.3058 - acc: 0.8785 - val_loss: 0.3418 - val_acc: 0.8527\n",
            "Epoch 137/500\n",
            "20108/20108 [==============================] - 7s 332us/sample - loss: 0.3048 - acc: 0.8781 - val_loss: 0.3414 - val_acc: 0.8529\n",
            "Epoch 138/500\n",
            "20108/20108 [==============================] - 6s 318us/sample - loss: 0.3063 - acc: 0.8757 - val_loss: 0.3416 - val_acc: 0.8533\n",
            "Epoch 139/500\n",
            "20108/20108 [==============================] - 7s 329us/sample - loss: 0.3023 - acc: 0.8800 - val_loss: 0.3413 - val_acc: 0.8531\n",
            "Epoch 140/500\n",
            "20108/20108 [==============================] - 6s 317us/sample - loss: 0.3058 - acc: 0.8776 - val_loss: 0.3413 - val_acc: 0.8531\n",
            "Epoch 141/500\n",
            "20108/20108 [==============================] - 7s 333us/sample - loss: 0.3034 - acc: 0.8779 - val_loss: 0.3414 - val_acc: 0.8535\n",
            "Epoch 142/500\n",
            "20108/20108 [==============================] - 7s 331us/sample - loss: 0.2994 - acc: 0.8817 - val_loss: 0.3419 - val_acc: 0.8537\n",
            "Epoch 143/500\n",
            "20108/20108 [==============================] - 7s 332us/sample - loss: 0.3028 - acc: 0.8784 - val_loss: 0.3413 - val_acc: 0.8531\n",
            "Epoch 144/500\n",
            "20108/20108 [==============================] - 7s 323us/sample - loss: 0.2995 - acc: 0.8817 - val_loss: 0.3415 - val_acc: 0.8533\n",
            "Epoch 145/500\n",
            "20108/20108 [==============================] - 7s 323us/sample - loss: 0.2988 - acc: 0.8801 - val_loss: 0.3415 - val_acc: 0.8535\n",
            "Epoch 146/500\n",
            "20108/20108 [==============================] - 7s 329us/sample - loss: 0.2993 - acc: 0.8813 - val_loss: 0.3413 - val_acc: 0.8521\n",
            "Epoch 00146: early stopping\n",
            "--- 1024.1281175613403 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20108/20108 [==============================] - 79s 4ms/sample - loss: 0.2801 - acc: 0.8888\n",
            "5028/5028 [==============================] - 19s 4ms/sample - loss: 0.3415 - acc: 0.8514\n",
            "Train Accuracy: 0.8888\n",
            "Train Loss: 0.2801\n",
            "Validation Accuracy:  0.8514\n",
            "Validation Loss: 0.3415\n",
            "5028/5028 [==============================] - 6s 1ms/sample\n",
            "[[1059  441]\n",
            " [ 306 3222]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.71      0.74      1500\n",
            "         1.0       0.88      0.91      0.90      3528\n",
            "\n",
            "    accuracy                           0.85      5028\n",
            "   macro avg       0.83      0.81      0.82      5028\n",
            "weighted avg       0.85      0.85      0.85      5028\n",
            "\n",
            "RMSE: 0.3854\n",
            "--- Fold 2 ---\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.76.36.210:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1277633526131077744)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6387720827749024011)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10496896205887535107)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6969718561809677301)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12961930590832392281)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 103926089697679672)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8238627829294133264)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2041167189725682759)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6959296792125428166)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4386377055783322133)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 302002105411306474)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20109 samples, validate on 5027 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 7.160129547119141 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20109 [==========================>...] - ETA: 2s - loss: 0.6877 - acc: 0.6737INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 9.31347918510437 secs\n",
            "19456/20109 [============================>.] - ETA: 1s - loss: 0.6874 - acc: 0.6757INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 7.768522262573242 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 9.107888460159302 secs\n",
            "20109/20109 [==============================] - 66s 3ms/sample - loss: 0.6872 - acc: 0.6772 - val_loss: 0.6825 - val_acc: 0.7020\n",
            "Epoch 2/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.6784 - acc: 0.7011 - val_loss: 0.6734 - val_acc: 0.7020\n",
            "Epoch 3/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.6696 - acc: 0.7016 - val_loss: 0.6637 - val_acc: 0.7020\n",
            "Epoch 4/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.6595 - acc: 0.7017 - val_loss: 0.6531 - val_acc: 0.7020\n",
            "Epoch 5/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.6489 - acc: 0.7017 - val_loss: 0.6410 - val_acc: 0.7020\n",
            "Epoch 6/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.6368 - acc: 0.7016 - val_loss: 0.6272 - val_acc: 0.7020\n",
            "Epoch 7/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.6240 - acc: 0.7017 - val_loss: 0.6127 - val_acc: 0.7020\n",
            "Epoch 8/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.6139 - acc: 0.7016 - val_loss: 0.6060 - val_acc: 0.7020\n",
            "Epoch 9/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.6130 - acc: 0.7017 - val_loss: 0.6059 - val_acc: 0.7020\n",
            "Epoch 10/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.6123 - acc: 0.7016 - val_loss: 0.6057 - val_acc: 0.7020\n",
            "Epoch 11/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.6109 - acc: 0.7016 - val_loss: 0.6056 - val_acc: 0.7020\n",
            "Epoch 12/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.6108 - acc: 0.7017 - val_loss: 0.6057 - val_acc: 0.7020\n",
            "Epoch 13/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.6104 - acc: 0.7017 - val_loss: 0.6056 - val_acc: 0.7020\n",
            "Epoch 14/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.6101 - acc: 0.7016 - val_loss: 0.6059 - val_acc: 0.7020\n",
            "Epoch 15/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.6095 - acc: 0.7016 - val_loss: 0.6057 - val_acc: 0.7020\n",
            "Epoch 16/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.6098 - acc: 0.7017 - val_loss: 0.6055 - val_acc: 0.7020\n",
            "Epoch 17/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.6089 - acc: 0.7017 - val_loss: 0.6055 - val_acc: 0.7020\n",
            "Epoch 18/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.6084 - acc: 0.7016 - val_loss: 0.6053 - val_acc: 0.7020\n",
            "Epoch 19/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.6076 - acc: 0.7016 - val_loss: 0.6053 - val_acc: 0.7020\n",
            "Epoch 20/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.6072 - acc: 0.7016 - val_loss: 0.6048 - val_acc: 0.7020\n",
            "Epoch 21/500\n",
            "20109/20109 [==============================] - 7s 344us/sample - loss: 0.6067 - acc: 0.7016 - val_loss: 0.6045 - val_acc: 0.7020\n",
            "Epoch 22/500\n",
            "20109/20109 [==============================] - 7s 323us/sample - loss: 0.6060 - acc: 0.7017 - val_loss: 0.6040 - val_acc: 0.7020\n",
            "Epoch 23/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.6056 - acc: 0.7017 - val_loss: 0.6038 - val_acc: 0.7020\n",
            "Epoch 24/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.6052 - acc: 0.7017 - val_loss: 0.6035 - val_acc: 0.7020\n",
            "Epoch 25/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.6042 - acc: 0.7017 - val_loss: 0.6025 - val_acc: 0.7020\n",
            "Epoch 26/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.6033 - acc: 0.7017 - val_loss: 0.6018 - val_acc: 0.7020\n",
            "Epoch 27/500\n",
            "20109/20109 [==============================] - 7s 358us/sample - loss: 0.6029 - acc: 0.7017 - val_loss: 0.6011 - val_acc: 0.7020\n",
            "Epoch 28/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.6015 - acc: 0.7016 - val_loss: 0.6001 - val_acc: 0.7020\n",
            "Epoch 29/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.6004 - acc: 0.7016 - val_loss: 0.5991 - val_acc: 0.7020\n",
            "Epoch 30/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.5994 - acc: 0.7016 - val_loss: 0.5979 - val_acc: 0.7020\n",
            "Epoch 31/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.5978 - acc: 0.7016 - val_loss: 0.5965 - val_acc: 0.7020\n",
            "Epoch 32/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.5961 - acc: 0.7017 - val_loss: 0.5950 - val_acc: 0.7020\n",
            "Epoch 33/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.5943 - acc: 0.7017 - val_loss: 0.5934 - val_acc: 0.7020\n",
            "Epoch 34/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.5921 - acc: 0.7017 - val_loss: 0.5912 - val_acc: 0.7020\n",
            "Epoch 35/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.5898 - acc: 0.7017 - val_loss: 0.5885 - val_acc: 0.7020\n",
            "Epoch 36/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.5866 - acc: 0.7017 - val_loss: 0.5854 - val_acc: 0.7020\n",
            "Epoch 37/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.5832 - acc: 0.7017 - val_loss: 0.5823 - val_acc: 0.7020\n",
            "Epoch 38/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.5799 - acc: 0.7017 - val_loss: 0.5781 - val_acc: 0.7020\n",
            "Epoch 39/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.5765 - acc: 0.7017 - val_loss: 0.5748 - val_acc: 0.7020\n",
            "Epoch 40/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.5736 - acc: 0.7016 - val_loss: 0.5708 - val_acc: 0.7020\n",
            "Epoch 41/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.5680 - acc: 0.7017 - val_loss: 0.5668 - val_acc: 0.7020\n",
            "Epoch 42/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.5629 - acc: 0.7016 - val_loss: 0.5616 - val_acc: 0.7020\n",
            "Epoch 43/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.5589 - acc: 0.7018 - val_loss: 0.5573 - val_acc: 0.7020\n",
            "Epoch 44/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.5543 - acc: 0.7025 - val_loss: 0.5533 - val_acc: 0.7020\n",
            "Epoch 45/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.5493 - acc: 0.7045 - val_loss: 0.5483 - val_acc: 0.7024\n",
            "Epoch 46/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.5425 - acc: 0.7067 - val_loss: 0.5402 - val_acc: 0.7026\n",
            "Epoch 47/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.5402 - acc: 0.7088 - val_loss: 0.5368 - val_acc: 0.7048\n",
            "Epoch 48/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.5334 - acc: 0.7116 - val_loss: 0.5291 - val_acc: 0.7080\n",
            "Epoch 49/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.5272 - acc: 0.7164 - val_loss: 0.5229 - val_acc: 0.7120\n",
            "Epoch 50/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.5221 - acc: 0.7220 - val_loss: 0.5165 - val_acc: 0.7164\n",
            "Epoch 51/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.5150 - acc: 0.7308 - val_loss: 0.5135 - val_acc: 0.7235\n",
            "Epoch 52/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.5092 - acc: 0.7380 - val_loss: 0.5008 - val_acc: 0.7247\n",
            "Epoch 53/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.5033 - acc: 0.7412 - val_loss: 0.5010 - val_acc: 0.7365\n",
            "Epoch 54/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.4968 - acc: 0.7529 - val_loss: 0.4888 - val_acc: 0.7363\n",
            "Epoch 55/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.4906 - acc: 0.7545 - val_loss: 0.4873 - val_acc: 0.7518\n",
            "Epoch 56/500\n",
            "20109/20109 [==============================] - 7s 357us/sample - loss: 0.4826 - acc: 0.7662 - val_loss: 0.4760 - val_acc: 0.7592\n",
            "Epoch 57/500\n",
            "20109/20109 [==============================] - 7s 349us/sample - loss: 0.4783 - acc: 0.7714 - val_loss: 0.4739 - val_acc: 0.7635\n",
            "Epoch 58/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.4757 - acc: 0.7759 - val_loss: 0.4673 - val_acc: 0.7661\n",
            "Epoch 59/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.4675 - acc: 0.7849 - val_loss: 0.4597 - val_acc: 0.7753\n",
            "Epoch 60/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.4601 - acc: 0.7880 - val_loss: 0.4531 - val_acc: 0.7814\n",
            "Epoch 61/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.4530 - acc: 0.7965 - val_loss: 0.4495 - val_acc: 0.7944\n",
            "Epoch 62/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.4494 - acc: 0.8015 - val_loss: 0.4409 - val_acc: 0.7932\n",
            "Epoch 63/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.4430 - acc: 0.8031 - val_loss: 0.4364 - val_acc: 0.8018\n",
            "Epoch 64/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.4386 - acc: 0.8067 - val_loss: 0.4313 - val_acc: 0.7998\n",
            "Epoch 65/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.4327 - acc: 0.8135 - val_loss: 0.4238 - val_acc: 0.8055\n",
            "Epoch 66/500\n",
            "20109/20109 [==============================] - 7s 347us/sample - loss: 0.4255 - acc: 0.8127 - val_loss: 0.4212 - val_acc: 0.8137\n",
            "Epoch 67/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.4234 - acc: 0.8173 - val_loss: 0.4180 - val_acc: 0.8105\n",
            "Epoch 68/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.4183 - acc: 0.8194 - val_loss: 0.4155 - val_acc: 0.8169\n",
            "Epoch 69/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.4135 - acc: 0.8251 - val_loss: 0.4104 - val_acc: 0.8181\n",
            "Epoch 70/500\n",
            "20109/20109 [==============================] - 7s 346us/sample - loss: 0.4101 - acc: 0.8264 - val_loss: 0.4109 - val_acc: 0.8209\n",
            "Epoch 71/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.4084 - acc: 0.8239 - val_loss: 0.4052 - val_acc: 0.8185\n",
            "Epoch 72/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.4073 - acc: 0.8267 - val_loss: 0.4075 - val_acc: 0.8211\n",
            "Epoch 73/500\n",
            "20109/20109 [==============================] - 7s 323us/sample - loss: 0.3998 - acc: 0.8271 - val_loss: 0.3975 - val_acc: 0.8238\n",
            "Epoch 74/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.3958 - acc: 0.8314 - val_loss: 0.3972 - val_acc: 0.8262\n",
            "Epoch 75/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.3940 - acc: 0.8330 - val_loss: 0.3918 - val_acc: 0.8266\n",
            "Epoch 76/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3891 - acc: 0.8334 - val_loss: 0.3909 - val_acc: 0.8278\n",
            "Epoch 77/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.3891 - acc: 0.8353 - val_loss: 0.3894 - val_acc: 0.8290\n",
            "Epoch 78/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.3851 - acc: 0.8372 - val_loss: 0.3889 - val_acc: 0.8298\n",
            "Epoch 79/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.3831 - acc: 0.8379 - val_loss: 0.3858 - val_acc: 0.8298\n",
            "Epoch 80/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3780 - acc: 0.8414 - val_loss: 0.3819 - val_acc: 0.8300\n",
            "Epoch 81/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.3786 - acc: 0.8403 - val_loss: 0.3806 - val_acc: 0.8354\n",
            "Epoch 82/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.3742 - acc: 0.8435 - val_loss: 0.3780 - val_acc: 0.8364\n",
            "Epoch 83/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.3717 - acc: 0.8441 - val_loss: 0.3778 - val_acc: 0.8358\n",
            "Epoch 84/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.3699 - acc: 0.8467 - val_loss: 0.3786 - val_acc: 0.8356\n",
            "Epoch 85/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3655 - acc: 0.8470 - val_loss: 0.3761 - val_acc: 0.8356\n",
            "Epoch 86/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.3633 - acc: 0.8498 - val_loss: 0.3737 - val_acc: 0.8372\n",
            "Epoch 87/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.3656 - acc: 0.8494 - val_loss: 0.3721 - val_acc: 0.8388\n",
            "Epoch 88/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.3632 - acc: 0.8474 - val_loss: 0.3716 - val_acc: 0.8382\n",
            "Epoch 89/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3604 - acc: 0.8512 - val_loss: 0.3712 - val_acc: 0.8388\n",
            "Epoch 90/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.3564 - acc: 0.8515 - val_loss: 0.3673 - val_acc: 0.8414\n",
            "Epoch 91/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3567 - acc: 0.8514 - val_loss: 0.3690 - val_acc: 0.8388\n",
            "Epoch 92/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.3565 - acc: 0.8520 - val_loss: 0.3670 - val_acc: 0.8400\n",
            "Epoch 93/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.3517 - acc: 0.8546 - val_loss: 0.3645 - val_acc: 0.8426\n",
            "Epoch 94/500\n",
            "20109/20109 [==============================] - 7s 342us/sample - loss: 0.3501 - acc: 0.8549 - val_loss: 0.3653 - val_acc: 0.8412\n",
            "Epoch 95/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3491 - acc: 0.8544 - val_loss: 0.3622 - val_acc: 0.8447\n",
            "Epoch 96/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3468 - acc: 0.8575 - val_loss: 0.3625 - val_acc: 0.8443\n",
            "Epoch 97/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3432 - acc: 0.8562 - val_loss: 0.3616 - val_acc: 0.8436\n",
            "Epoch 98/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3455 - acc: 0.8572 - val_loss: 0.3621 - val_acc: 0.8430\n",
            "Epoch 99/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.3404 - acc: 0.8602 - val_loss: 0.3600 - val_acc: 0.8459\n",
            "Epoch 100/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3419 - acc: 0.8594 - val_loss: 0.3603 - val_acc: 0.8445\n",
            "Epoch 101/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.3398 - acc: 0.8638 - val_loss: 0.3584 - val_acc: 0.8469\n",
            "Epoch 102/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.3404 - acc: 0.8595 - val_loss: 0.3583 - val_acc: 0.8459\n",
            "Epoch 103/500\n",
            "20109/20109 [==============================] - 7s 346us/sample - loss: 0.3364 - acc: 0.8616 - val_loss: 0.3590 - val_acc: 0.8453\n",
            "Epoch 104/500\n",
            "20109/20109 [==============================] - 7s 353us/sample - loss: 0.3340 - acc: 0.8629 - val_loss: 0.3560 - val_acc: 0.8471\n",
            "Epoch 105/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3344 - acc: 0.8640 - val_loss: 0.3586 - val_acc: 0.8459\n",
            "Epoch 106/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3341 - acc: 0.8626 - val_loss: 0.3550 - val_acc: 0.8473\n",
            "Epoch 107/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.3300 - acc: 0.8651 - val_loss: 0.3549 - val_acc: 0.8481\n",
            "Epoch 108/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3309 - acc: 0.8643 - val_loss: 0.3522 - val_acc: 0.8505\n",
            "Epoch 109/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3277 - acc: 0.8672 - val_loss: 0.3544 - val_acc: 0.8489\n",
            "Epoch 110/500\n",
            "20109/20109 [==============================] - 7s 354us/sample - loss: 0.3257 - acc: 0.8687 - val_loss: 0.3533 - val_acc: 0.8495\n",
            "Epoch 111/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.3271 - acc: 0.8653 - val_loss: 0.3519 - val_acc: 0.8511\n",
            "Epoch 112/500\n",
            "20109/20109 [==============================] - 7s 351us/sample - loss: 0.3250 - acc: 0.8682 - val_loss: 0.3513 - val_acc: 0.8499\n",
            "Epoch 113/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3250 - acc: 0.8680 - val_loss: 0.3513 - val_acc: 0.8501\n",
            "Epoch 114/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3244 - acc: 0.8664 - val_loss: 0.3519 - val_acc: 0.8487\n",
            "Epoch 115/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.3243 - acc: 0.8692 - val_loss: 0.3506 - val_acc: 0.8505\n",
            "Epoch 116/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.3211 - acc: 0.8681 - val_loss: 0.3493 - val_acc: 0.8515\n",
            "Epoch 117/500\n",
            "20109/20109 [==============================] - 7s 323us/sample - loss: 0.3200 - acc: 0.8706 - val_loss: 0.3503 - val_acc: 0.8495\n",
            "Epoch 118/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3200 - acc: 0.8702 - val_loss: 0.3503 - val_acc: 0.8495\n",
            "Epoch 119/500\n",
            "20109/20109 [==============================] - 7s 323us/sample - loss: 0.3183 - acc: 0.8728 - val_loss: 0.3496 - val_acc: 0.8503\n",
            "Epoch 120/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.3148 - acc: 0.8729 - val_loss: 0.3478 - val_acc: 0.8511\n",
            "Epoch 121/500\n",
            "20109/20109 [==============================] - 7s 323us/sample - loss: 0.3137 - acc: 0.8734 - val_loss: 0.3480 - val_acc: 0.8511\n",
            "Epoch 122/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3156 - acc: 0.8731 - val_loss: 0.3485 - val_acc: 0.8515\n",
            "Epoch 123/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3107 - acc: 0.8723 - val_loss: 0.3476 - val_acc: 0.8513\n",
            "Epoch 124/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3116 - acc: 0.8727 - val_loss: 0.3473 - val_acc: 0.8511\n",
            "Epoch 125/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.3086 - acc: 0.8772 - val_loss: 0.3476 - val_acc: 0.8497\n",
            "Epoch 126/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.3100 - acc: 0.8751 - val_loss: 0.3475 - val_acc: 0.8519\n",
            "Epoch 127/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.3107 - acc: 0.8766 - val_loss: 0.3469 - val_acc: 0.8519\n",
            "Epoch 128/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.3105 - acc: 0.8739 - val_loss: 0.3475 - val_acc: 0.8519\n",
            "Epoch 129/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.3088 - acc: 0.8767 - val_loss: 0.3468 - val_acc: 0.8515\n",
            "Epoch 130/500\n",
            "20109/20109 [==============================] - 7s 347us/sample - loss: 0.3066 - acc: 0.8774 - val_loss: 0.3466 - val_acc: 0.8509\n",
            "Epoch 131/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.3028 - acc: 0.8769 - val_loss: 0.3458 - val_acc: 0.8523\n",
            "Epoch 132/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.3054 - acc: 0.8761 - val_loss: 0.3465 - val_acc: 0.8507\n",
            "Epoch 133/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.3047 - acc: 0.8755 - val_loss: 0.3457 - val_acc: 0.8523\n",
            "Epoch 134/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.3050 - acc: 0.8767 - val_loss: 0.3463 - val_acc: 0.8507\n",
            "Epoch 135/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.3036 - acc: 0.8824 - val_loss: 0.3447 - val_acc: 0.8533\n",
            "Epoch 136/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.3008 - acc: 0.8804 - val_loss: 0.3446 - val_acc: 0.8533\n",
            "Epoch 137/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.3014 - acc: 0.8804 - val_loss: 0.3442 - val_acc: 0.8533\n",
            "Epoch 138/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.2991 - acc: 0.8790 - val_loss: 0.3445 - val_acc: 0.8519\n",
            "Epoch 139/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.2996 - acc: 0.8778 - val_loss: 0.3436 - val_acc: 0.8527\n",
            "Epoch 140/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.2977 - acc: 0.8811 - val_loss: 0.3440 - val_acc: 0.8521\n",
            "Epoch 141/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.2986 - acc: 0.8795 - val_loss: 0.3442 - val_acc: 0.8513\n",
            "Epoch 142/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.2968 - acc: 0.8813 - val_loss: 0.3439 - val_acc: 0.8537\n",
            "Epoch 143/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.2977 - acc: 0.8806 - val_loss: 0.3439 - val_acc: 0.8539\n",
            "Epoch 144/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.2950 - acc: 0.8830 - val_loss: 0.3437 - val_acc: 0.8535\n",
            "Epoch 145/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.2972 - acc: 0.8802 - val_loss: 0.3439 - val_acc: 0.8533\n",
            "Epoch 146/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.2943 - acc: 0.8842 - val_loss: 0.3434 - val_acc: 0.8525\n",
            "Epoch 147/500\n",
            "20109/20109 [==============================] - 6s 320us/sample - loss: 0.2940 - acc: 0.8840 - val_loss: 0.3439 - val_acc: 0.8523\n",
            "Epoch 148/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.2929 - acc: 0.8837 - val_loss: 0.3428 - val_acc: 0.8531\n",
            "Epoch 149/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.2925 - acc: 0.8842 - val_loss: 0.3433 - val_acc: 0.8535\n",
            "Epoch 150/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.2935 - acc: 0.8830 - val_loss: 0.3440 - val_acc: 0.8513\n",
            "Epoch 151/500\n",
            "20109/20109 [==============================] - 7s 348us/sample - loss: 0.2898 - acc: 0.8828 - val_loss: 0.3429 - val_acc: 0.8529\n",
            "Epoch 152/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.2873 - acc: 0.8844 - val_loss: 0.3419 - val_acc: 0.8543\n",
            "Epoch 153/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.2871 - acc: 0.8854 - val_loss: 0.3420 - val_acc: 0.8551\n",
            "Epoch 154/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.2882 - acc: 0.8845 - val_loss: 0.3419 - val_acc: 0.8541\n",
            "Epoch 155/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.2868 - acc: 0.8869 - val_loss: 0.3426 - val_acc: 0.8525\n",
            "Epoch 156/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.2874 - acc: 0.8857 - val_loss: 0.3417 - val_acc: 0.8537\n",
            "Epoch 157/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.2879 - acc: 0.8846 - val_loss: 0.3422 - val_acc: 0.8521\n",
            "Epoch 158/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.2859 - acc: 0.8872 - val_loss: 0.3419 - val_acc: 0.8523\n",
            "Epoch 159/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.2852 - acc: 0.8875 - val_loss: 0.3424 - val_acc: 0.8515\n",
            "Epoch 160/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.2843 - acc: 0.8872 - val_loss: 0.3423 - val_acc: 0.8513\n",
            "Epoch 161/500\n",
            "20109/20109 [==============================] - 6s 320us/sample - loss: 0.2839 - acc: 0.8877 - val_loss: 0.3424 - val_acc: 0.8513\n",
            "Epoch 162/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.2834 - acc: 0.8875 - val_loss: 0.3421 - val_acc: 0.8519\n",
            "Epoch 163/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.2817 - acc: 0.8862 - val_loss: 0.3423 - val_acc: 0.8509\n",
            "Epoch 00163: early stopping\n",
            "--- 1148.033050775528 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20109/20109 [==============================] - 77s 4ms/sample - loss: 0.2645 - acc: 0.8944\n",
            "5027/5027 [==============================] - 19s 4ms/sample - loss: 0.3420 - acc: 0.8510\n",
            "Train Accuracy: 0.8944\n",
            "Train Loss: 0.2645\n",
            "Validation Accuracy:  0.8510\n",
            "Validation Loss: 0.3420\n",
            "5027/5027 [==============================] - 7s 1ms/sample\n",
            "[[1087  412]\n",
            " [ 337 3191]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.73      0.74      1499\n",
            "         1.0       0.89      0.90      0.89      3528\n",
            "\n",
            "    accuracy                           0.85      5027\n",
            "   macro avg       0.82      0.81      0.82      5027\n",
            "weighted avg       0.85      0.85      0.85      5027\n",
            "\n",
            "RMSE: 0.3860\n",
            "--- Fold 3 ---\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.76.36.210:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1277633526131077744)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6387720827749024011)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10496896205887535107)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6969718561809677301)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12961930590832392281)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 103926089697679672)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8238627829294133264)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2041167189725682759)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6959296792125428166)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4386377055783322133)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 302002105411306474)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20109 samples, validate on 5027 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 7.242166757583618 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20109 [==========================>...] - ETA: 2s - loss: 0.6865 - acc: 0.6781INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 9.064348220825195 secs\n",
            "19456/20109 [============================>.] - ETA: 1s - loss: 0.6862 - acc: 0.6799INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 7.684441804885864 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 8.950719356536865 secs\n",
            "20109/20109 [==============================] - 68s 3ms/sample - loss: 0.6859 - acc: 0.6827 - val_loss: 0.6802 - val_acc: 0.7012\n",
            "Epoch 2/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.6750 - acc: 0.7015 - val_loss: 0.6690 - val_acc: 0.7016\n",
            "Epoch 3/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.6632 - acc: 0.7016 - val_loss: 0.6570 - val_acc: 0.7016\n",
            "Epoch 4/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.6503 - acc: 0.7017 - val_loss: 0.6434 - val_acc: 0.7016\n",
            "Epoch 5/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.6364 - acc: 0.7018 - val_loss: 0.6282 - val_acc: 0.7016\n",
            "Epoch 6/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.6215 - acc: 0.7016 - val_loss: 0.6139 - val_acc: 0.7016\n",
            "Epoch 7/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.6136 - acc: 0.7016 - val_loss: 0.6107 - val_acc: 0.7016\n",
            "Epoch 8/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.6109 - acc: 0.7018 - val_loss: 0.6100 - val_acc: 0.7016\n",
            "Epoch 9/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.6119 - acc: 0.7018 - val_loss: 0.6097 - val_acc: 0.7016\n",
            "Epoch 10/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.6114 - acc: 0.7017 - val_loss: 0.6095 - val_acc: 0.7016\n",
            "Epoch 11/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.6108 - acc: 0.7017 - val_loss: 0.6091 - val_acc: 0.7016\n",
            "Epoch 12/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.6102 - acc: 0.7017 - val_loss: 0.6089 - val_acc: 0.7016\n",
            "Epoch 13/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.6100 - acc: 0.7017 - val_loss: 0.6086 - val_acc: 0.7016\n",
            "Epoch 14/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.6092 - acc: 0.7017 - val_loss: 0.6084 - val_acc: 0.7016\n",
            "Epoch 15/500\n",
            "20109/20109 [==============================] - 7s 347us/sample - loss: 0.6087 - acc: 0.7018 - val_loss: 0.6080 - val_acc: 0.7016\n",
            "Epoch 16/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.6086 - acc: 0.7017 - val_loss: 0.6077 - val_acc: 0.7016\n",
            "Epoch 17/500\n",
            "20109/20109 [==============================] - 7s 348us/sample - loss: 0.6073 - acc: 0.7017 - val_loss: 0.6073 - val_acc: 0.7016\n",
            "Epoch 18/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.6071 - acc: 0.7017 - val_loss: 0.6069 - val_acc: 0.7016\n",
            "Epoch 19/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.6070 - acc: 0.7017 - val_loss: 0.6066 - val_acc: 0.7016\n",
            "Epoch 20/500\n",
            "20109/20109 [==============================] - 7s 346us/sample - loss: 0.6062 - acc: 0.7017 - val_loss: 0.6060 - val_acc: 0.7016\n",
            "Epoch 21/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.6066 - acc: 0.7017 - val_loss: 0.6056 - val_acc: 0.7016\n",
            "Epoch 22/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.6054 - acc: 0.7017 - val_loss: 0.6050 - val_acc: 0.7016\n",
            "Epoch 23/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.6049 - acc: 0.7017 - val_loss: 0.6044 - val_acc: 0.7016\n",
            "Epoch 24/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.6047 - acc: 0.7017 - val_loss: 0.6037 - val_acc: 0.7016\n",
            "Epoch 25/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.6030 - acc: 0.7016 - val_loss: 0.6029 - val_acc: 0.7016\n",
            "Epoch 26/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.6029 - acc: 0.7017 - val_loss: 0.6020 - val_acc: 0.7016\n",
            "Epoch 27/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.6013 - acc: 0.7017 - val_loss: 0.6011 - val_acc: 0.7016\n",
            "Epoch 28/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.6003 - acc: 0.7018 - val_loss: 0.6001 - val_acc: 0.7016\n",
            "Epoch 29/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.5992 - acc: 0.7017 - val_loss: 0.5990 - val_acc: 0.7016\n",
            "Epoch 30/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.5977 - acc: 0.7017 - val_loss: 0.5974 - val_acc: 0.7016\n",
            "Epoch 31/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.5958 - acc: 0.7018 - val_loss: 0.5960 - val_acc: 0.7016\n",
            "Epoch 32/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.5950 - acc: 0.7017 - val_loss: 0.5944 - val_acc: 0.7016\n",
            "Epoch 33/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.5926 - acc: 0.7017 - val_loss: 0.5924 - val_acc: 0.7016\n",
            "Epoch 34/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.5905 - acc: 0.7016 - val_loss: 0.5904 - val_acc: 0.7016\n",
            "Epoch 35/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.5883 - acc: 0.7017 - val_loss: 0.5880 - val_acc: 0.7016\n",
            "Epoch 36/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.5854 - acc: 0.7018 - val_loss: 0.5853 - val_acc: 0.7016\n",
            "Epoch 37/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.5829 - acc: 0.7017 - val_loss: 0.5823 - val_acc: 0.7016\n",
            "Epoch 38/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.5794 - acc: 0.7016 - val_loss: 0.5789 - val_acc: 0.7016\n",
            "Epoch 39/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.5752 - acc: 0.7017 - val_loss: 0.5751 - val_acc: 0.7016\n",
            "Epoch 40/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.5715 - acc: 0.7017 - val_loss: 0.5700 - val_acc: 0.7016\n",
            "Epoch 41/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.5656 - acc: 0.7018 - val_loss: 0.5644 - val_acc: 0.7016\n",
            "Epoch 42/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.5607 - acc: 0.7018 - val_loss: 0.5580 - val_acc: 0.7016\n",
            "Epoch 43/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.5540 - acc: 0.7020 - val_loss: 0.5519 - val_acc: 0.7018\n",
            "Epoch 44/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.5466 - acc: 0.7032 - val_loss: 0.5440 - val_acc: 0.7032\n",
            "Epoch 45/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.5398 - acc: 0.7061 - val_loss: 0.5365 - val_acc: 0.7072\n",
            "Epoch 46/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.5330 - acc: 0.7124 - val_loss: 0.5274 - val_acc: 0.7108\n",
            "Epoch 47/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.5254 - acc: 0.7211 - val_loss: 0.5187 - val_acc: 0.7156\n",
            "Epoch 48/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.5158 - acc: 0.7292 - val_loss: 0.5081 - val_acc: 0.7219\n",
            "Epoch 49/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.5103 - acc: 0.7413 - val_loss: 0.5064 - val_acc: 0.7361\n",
            "Epoch 50/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.5026 - acc: 0.7491 - val_loss: 0.4965 - val_acc: 0.7426\n",
            "Epoch 51/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.4976 - acc: 0.7535 - val_loss: 0.4867 - val_acc: 0.7510\n",
            "Epoch 52/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.4902 - acc: 0.7618 - val_loss: 0.4823 - val_acc: 0.7605\n",
            "Epoch 53/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.4822 - acc: 0.7741 - val_loss: 0.4746 - val_acc: 0.7679\n",
            "Epoch 54/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.4776 - acc: 0.7797 - val_loss: 0.4698 - val_acc: 0.7828\n",
            "Epoch 55/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.4727 - acc: 0.7848 - val_loss: 0.4701 - val_acc: 0.7896\n",
            "Epoch 56/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.4637 - acc: 0.7940 - val_loss: 0.4568 - val_acc: 0.7928\n",
            "Epoch 57/500\n",
            "20109/20109 [==============================] - 7s 347us/sample - loss: 0.4559 - acc: 0.7960 - val_loss: 0.4495 - val_acc: 0.8061\n",
            "Epoch 58/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.4565 - acc: 0.7958 - val_loss: 0.4494 - val_acc: 0.8049\n",
            "Epoch 59/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.4455 - acc: 0.8027 - val_loss: 0.4418 - val_acc: 0.8079\n",
            "Epoch 60/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.4426 - acc: 0.8060 - val_loss: 0.4417 - val_acc: 0.8107\n",
            "Epoch 61/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.4410 - acc: 0.8039 - val_loss: 0.4333 - val_acc: 0.8099\n",
            "Epoch 62/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.4329 - acc: 0.8080 - val_loss: 0.4333 - val_acc: 0.8113\n",
            "Epoch 63/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.4268 - acc: 0.8151 - val_loss: 0.4268 - val_acc: 0.8129\n",
            "Epoch 64/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.4222 - acc: 0.8157 - val_loss: 0.4223 - val_acc: 0.8151\n",
            "Epoch 65/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.4216 - acc: 0.8147 - val_loss: 0.4234 - val_acc: 0.8161\n",
            "Epoch 66/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.4183 - acc: 0.8182 - val_loss: 0.4179 - val_acc: 0.8163\n",
            "Epoch 67/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.4123 - acc: 0.8217 - val_loss: 0.4149 - val_acc: 0.8191\n",
            "Epoch 68/500\n",
            "20109/20109 [==============================] - 6s 320us/sample - loss: 0.4107 - acc: 0.8247 - val_loss: 0.4121 - val_acc: 0.8225\n",
            "Epoch 69/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.4061 - acc: 0.8272 - val_loss: 0.4102 - val_acc: 0.8221\n",
            "Epoch 70/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.4035 - acc: 0.8276 - val_loss: 0.4073 - val_acc: 0.8234\n",
            "Epoch 71/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.3982 - acc: 0.8293 - val_loss: 0.4044 - val_acc: 0.8254\n",
            "Epoch 72/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.3941 - acc: 0.8319 - val_loss: 0.4025 - val_acc: 0.8282\n",
            "Epoch 73/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.3938 - acc: 0.8329 - val_loss: 0.3981 - val_acc: 0.8290\n",
            "Epoch 74/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.3868 - acc: 0.8346 - val_loss: 0.3932 - val_acc: 0.8286\n",
            "Epoch 75/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3849 - acc: 0.8372 - val_loss: 0.3903 - val_acc: 0.8306\n",
            "Epoch 76/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.3830 - acc: 0.8395 - val_loss: 0.3971 - val_acc: 0.8300\n",
            "Epoch 77/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.3793 - acc: 0.8411 - val_loss: 0.3884 - val_acc: 0.8318\n",
            "Epoch 78/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.3784 - acc: 0.8413 - val_loss: 0.3888 - val_acc: 0.8312\n",
            "Epoch 79/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.3755 - acc: 0.8404 - val_loss: 0.3872 - val_acc: 0.8320\n",
            "Epoch 80/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3743 - acc: 0.8434 - val_loss: 0.3841 - val_acc: 0.8336\n",
            "Epoch 81/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.3682 - acc: 0.8464 - val_loss: 0.3823 - val_acc: 0.8336\n",
            "Epoch 82/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.3644 - acc: 0.8490 - val_loss: 0.3798 - val_acc: 0.8346\n",
            "Epoch 83/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.3706 - acc: 0.8445 - val_loss: 0.3800 - val_acc: 0.8352\n",
            "Epoch 84/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.3649 - acc: 0.8458 - val_loss: 0.3807 - val_acc: 0.8364\n",
            "Epoch 85/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.3637 - acc: 0.8501 - val_loss: 0.3764 - val_acc: 0.8360\n",
            "Epoch 86/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.3586 - acc: 0.8502 - val_loss: 0.3763 - val_acc: 0.8364\n",
            "Epoch 87/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.3551 - acc: 0.8537 - val_loss: 0.3754 - val_acc: 0.8382\n",
            "Epoch 88/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3570 - acc: 0.8502 - val_loss: 0.3749 - val_acc: 0.8380\n",
            "Epoch 89/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.3514 - acc: 0.8551 - val_loss: 0.3741 - val_acc: 0.8396\n",
            "Epoch 90/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.3534 - acc: 0.8551 - val_loss: 0.3736 - val_acc: 0.8396\n",
            "Epoch 91/500\n",
            "20109/20109 [==============================] - 7s 342us/sample - loss: 0.3495 - acc: 0.8541 - val_loss: 0.3727 - val_acc: 0.8408\n",
            "Epoch 92/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.3482 - acc: 0.8542 - val_loss: 0.3701 - val_acc: 0.8418\n",
            "Epoch 93/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3471 - acc: 0.8544 - val_loss: 0.3700 - val_acc: 0.8420\n",
            "Epoch 94/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.3451 - acc: 0.8562 - val_loss: 0.3671 - val_acc: 0.8441\n",
            "Epoch 95/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3456 - acc: 0.8578 - val_loss: 0.3674 - val_acc: 0.8432\n",
            "Epoch 96/500\n",
            "20109/20109 [==============================] - 7s 344us/sample - loss: 0.3424 - acc: 0.8590 - val_loss: 0.3656 - val_acc: 0.8451\n",
            "Epoch 97/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.3407 - acc: 0.8588 - val_loss: 0.3653 - val_acc: 0.8447\n",
            "Epoch 98/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.3379 - acc: 0.8596 - val_loss: 0.3664 - val_acc: 0.8416\n",
            "Epoch 99/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3353 - acc: 0.8623 - val_loss: 0.3645 - val_acc: 0.8436\n",
            "Epoch 100/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.3330 - acc: 0.8640 - val_loss: 0.3648 - val_acc: 0.8439\n",
            "Epoch 101/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3339 - acc: 0.8639 - val_loss: 0.3643 - val_acc: 0.8420\n",
            "Epoch 102/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.3311 - acc: 0.8619 - val_loss: 0.3631 - val_acc: 0.8439\n",
            "Epoch 103/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.3310 - acc: 0.8653 - val_loss: 0.3622 - val_acc: 0.8451\n",
            "Epoch 104/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.3303 - acc: 0.8635 - val_loss: 0.3623 - val_acc: 0.8457\n",
            "Epoch 105/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.3294 - acc: 0.8658 - val_loss: 0.3608 - val_acc: 0.8457\n",
            "Epoch 106/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.3296 - acc: 0.8638 - val_loss: 0.3603 - val_acc: 0.8469\n",
            "Epoch 107/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.3246 - acc: 0.8662 - val_loss: 0.3607 - val_acc: 0.8459\n",
            "Epoch 108/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.3241 - acc: 0.8656 - val_loss: 0.3606 - val_acc: 0.8459\n",
            "Epoch 109/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.3230 - acc: 0.8679 - val_loss: 0.3593 - val_acc: 0.8465\n",
            "Epoch 110/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3238 - acc: 0.8681 - val_loss: 0.3591 - val_acc: 0.8463\n",
            "Epoch 111/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.3214 - acc: 0.8677 - val_loss: 0.3587 - val_acc: 0.8471\n",
            "Epoch 112/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3192 - acc: 0.8691 - val_loss: 0.3574 - val_acc: 0.8463\n",
            "Epoch 113/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3205 - acc: 0.8691 - val_loss: 0.3581 - val_acc: 0.8473\n",
            "Epoch 114/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.3161 - acc: 0.8701 - val_loss: 0.3575 - val_acc: 0.8483\n",
            "Epoch 115/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3157 - acc: 0.8703 - val_loss: 0.3568 - val_acc: 0.8481\n",
            "Epoch 116/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3148 - acc: 0.8728 - val_loss: 0.3569 - val_acc: 0.8497\n",
            "Epoch 117/500\n",
            "20109/20109 [==============================] - 7s 323us/sample - loss: 0.3119 - acc: 0.8718 - val_loss: 0.3566 - val_acc: 0.8491\n",
            "Epoch 118/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.3119 - acc: 0.8727 - val_loss: 0.3569 - val_acc: 0.8487\n",
            "Epoch 119/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3136 - acc: 0.8716 - val_loss: 0.3556 - val_acc: 0.8499\n",
            "Epoch 120/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.3071 - acc: 0.8751 - val_loss: 0.3549 - val_acc: 0.8493\n",
            "Epoch 121/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3105 - acc: 0.8732 - val_loss: 0.3547 - val_acc: 0.8491\n",
            "Epoch 122/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.3080 - acc: 0.8748 - val_loss: 0.3553 - val_acc: 0.8489\n",
            "Epoch 123/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.3074 - acc: 0.8754 - val_loss: 0.3545 - val_acc: 0.8499\n",
            "Epoch 124/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.3061 - acc: 0.8760 - val_loss: 0.3542 - val_acc: 0.8493\n",
            "Epoch 125/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3046 - acc: 0.8770 - val_loss: 0.3541 - val_acc: 0.8489\n",
            "Epoch 126/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.3061 - acc: 0.8782 - val_loss: 0.3551 - val_acc: 0.8471\n",
            "Epoch 127/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3043 - acc: 0.8759 - val_loss: 0.3547 - val_acc: 0.8477\n",
            "Epoch 128/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.3026 - acc: 0.8780 - val_loss: 0.3537 - val_acc: 0.8483\n",
            "Epoch 129/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.3040 - acc: 0.8782 - val_loss: 0.3536 - val_acc: 0.8483\n",
            "Epoch 130/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3028 - acc: 0.8769 - val_loss: 0.3537 - val_acc: 0.8487\n",
            "Epoch 131/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3001 - acc: 0.8781 - val_loss: 0.3532 - val_acc: 0.8483\n",
            "Epoch 132/500\n",
            "20109/20109 [==============================] - 7s 342us/sample - loss: 0.3014 - acc: 0.8780 - val_loss: 0.3533 - val_acc: 0.8475\n",
            "Epoch 133/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.2984 - acc: 0.8803 - val_loss: 0.3531 - val_acc: 0.8481\n",
            "Epoch 134/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.2947 - acc: 0.8803 - val_loss: 0.3530 - val_acc: 0.8503\n",
            "Epoch 135/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.2942 - acc: 0.8807 - val_loss: 0.3528 - val_acc: 0.8483\n",
            "Epoch 136/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.2951 - acc: 0.8817 - val_loss: 0.3528 - val_acc: 0.8485\n",
            "Epoch 137/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.2958 - acc: 0.8833 - val_loss: 0.3529 - val_acc: 0.8469\n",
            "Epoch 138/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.2940 - acc: 0.8807 - val_loss: 0.3529 - val_acc: 0.8499\n",
            "Epoch 139/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.2977 - acc: 0.8801 - val_loss: 0.3526 - val_acc: 0.8479\n",
            "Epoch 140/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.2925 - acc: 0.8831 - val_loss: 0.3526 - val_acc: 0.8481\n",
            "Epoch 141/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.2934 - acc: 0.8818 - val_loss: 0.3523 - val_acc: 0.8483\n",
            "Epoch 142/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.2904 - acc: 0.8833 - val_loss: 0.3527 - val_acc: 0.8481\n",
            "Epoch 143/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.2898 - acc: 0.8824 - val_loss: 0.3527 - val_acc: 0.8503\n",
            "Epoch 144/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.2912 - acc: 0.8847 - val_loss: 0.3523 - val_acc: 0.8483\n",
            "Epoch 145/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.2906 - acc: 0.8826 - val_loss: 0.3520 - val_acc: 0.8473\n",
            "Epoch 146/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.2892 - acc: 0.8846 - val_loss: 0.3520 - val_acc: 0.8475\n",
            "Epoch 147/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.2850 - acc: 0.8854 - val_loss: 0.3523 - val_acc: 0.8485\n",
            "Epoch 148/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.2857 - acc: 0.8869 - val_loss: 0.3526 - val_acc: 0.8495\n",
            "Epoch 149/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.2863 - acc: 0.8885 - val_loss: 0.3520 - val_acc: 0.8463\n",
            "Epoch 150/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.2837 - acc: 0.8866 - val_loss: 0.3521 - val_acc: 0.8471\n",
            "Epoch 151/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.2830 - acc: 0.8877 - val_loss: 0.3525 - val_acc: 0.8487\n",
            "Epoch 152/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.2847 - acc: 0.8855 - val_loss: 0.3521 - val_acc: 0.8483\n",
            "Epoch 153/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.2847 - acc: 0.8871 - val_loss: 0.3519 - val_acc: 0.8473\n",
            "Epoch 154/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.2833 - acc: 0.8861 - val_loss: 0.3519 - val_acc: 0.8473\n",
            "Epoch 155/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.2817 - acc: 0.8882 - val_loss: 0.3526 - val_acc: 0.8481\n",
            "Epoch 156/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.2799 - acc: 0.8888 - val_loss: 0.3524 - val_acc: 0.8481\n",
            "Epoch 157/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.2782 - acc: 0.8893 - val_loss: 0.3525 - val_acc: 0.8483\n",
            "Epoch 158/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.2783 - acc: 0.8883 - val_loss: 0.3526 - val_acc: 0.8471\n",
            "Epoch 159/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.2789 - acc: 0.8884 - val_loss: 0.3527 - val_acc: 0.8479\n",
            "Epoch 160/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.2808 - acc: 0.8881 - val_loss: 0.3527 - val_acc: 0.8479\n",
            "Epoch 161/500\n",
            "20109/20109 [==============================] - 7s 342us/sample - loss: 0.2797 - acc: 0.8903 - val_loss: 0.3534 - val_acc: 0.8483\n",
            "Epoch 00161: early stopping\n",
            "--- 1136.338399887085 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20109/20109 [==============================] - 79s 4ms/sample - loss: 0.2576 - acc: 0.8971\n",
            "5027/5027 [==============================] - 19s 4ms/sample - loss: 0.3539 - acc: 0.8478\n",
            "Train Accuracy: 0.8971\n",
            "Train Loss: 0.2576\n",
            "Validation Accuracy:  0.8478\n",
            "Validation Loss: 0.3539\n",
            "5027/5027 [==============================] - 7s 1ms/sample\n",
            "[[1068  432]\n",
            " [ 333 3194]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.71      0.74      1500\n",
            "         1.0       0.88      0.91      0.89      3527\n",
            "\n",
            "    accuracy                           0.85      5027\n",
            "   macro avg       0.82      0.81      0.81      5027\n",
            "weighted avg       0.85      0.85      0.85      5027\n",
            "\n",
            "RMSE: 0.3901\n",
            "--- Fold 4 ---\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.76.36.210:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1277633526131077744)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6387720827749024011)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10496896205887535107)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6969718561809677301)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12961930590832392281)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 103926089697679672)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8238627829294133264)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2041167189725682759)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6959296792125428166)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4386377055783322133)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 302002105411306474)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20109 samples, validate on 5027 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 7.614917278289795 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20109 [==========================>...] - ETA: 2s - loss: 0.6953 - acc: 0.4243INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 9.244533061981201 secs\n",
            "19456/20109 [============================>.] - ETA: 1s - loss: 0.6950 - acc: 0.4337INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 8.209857940673828 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 9.529520750045776 secs\n",
            "20109/20109 [==============================] - 71s 4ms/sample - loss: 0.6949 - acc: 0.4383 - val_loss: 0.6902 - val_acc: 0.6917\n",
            "Epoch 2/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.6862 - acc: 0.6865 - val_loss: 0.6815 - val_acc: 0.7012\n",
            "Epoch 3/500\n",
            "20109/20109 [==============================] - 7s 323us/sample - loss: 0.6773 - acc: 0.7016 - val_loss: 0.6727 - val_acc: 0.7014\n",
            "Epoch 4/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.6685 - acc: 0.7016 - val_loss: 0.6635 - val_acc: 0.7014\n",
            "Epoch 5/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.6590 - acc: 0.7016 - val_loss: 0.6535 - val_acc: 0.7014\n",
            "Epoch 6/500\n",
            "20109/20109 [==============================] - 7s 342us/sample - loss: 0.6484 - acc: 0.7017 - val_loss: 0.6426 - val_acc: 0.7014\n",
            "Epoch 7/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.6373 - acc: 0.7018 - val_loss: 0.6307 - val_acc: 0.7014\n",
            "Epoch 8/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.6253 - acc: 0.7017 - val_loss: 0.6185 - val_acc: 0.7014\n",
            "Epoch 9/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.6156 - acc: 0.7016 - val_loss: 0.6112 - val_acc: 0.7014\n",
            "Epoch 10/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.6115 - acc: 0.7016 - val_loss: 0.6103 - val_acc: 0.7014\n",
            "Epoch 11/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.6116 - acc: 0.7016 - val_loss: 0.6100 - val_acc: 0.7014\n",
            "Epoch 12/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.6107 - acc: 0.7017 - val_loss: 0.6097 - val_acc: 0.7014\n",
            "Epoch 13/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.6108 - acc: 0.7018 - val_loss: 0.6093 - val_acc: 0.7014\n",
            "Epoch 14/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.6098 - acc: 0.7019 - val_loss: 0.6093 - val_acc: 0.7014\n",
            "Epoch 15/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.6097 - acc: 0.7017 - val_loss: 0.6089 - val_acc: 0.7014\n",
            "Epoch 16/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.6102 - acc: 0.7017 - val_loss: 0.6088 - val_acc: 0.7014\n",
            "Epoch 17/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.6098 - acc: 0.7017 - val_loss: 0.6086 - val_acc: 0.7014\n",
            "Epoch 18/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.6098 - acc: 0.7016 - val_loss: 0.6086 - val_acc: 0.7014\n",
            "Epoch 19/500\n",
            "20109/20109 [==============================] - 7s 350us/sample - loss: 0.6094 - acc: 0.7017 - val_loss: 0.6083 - val_acc: 0.7014\n",
            "Epoch 20/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.6085 - acc: 0.7017 - val_loss: 0.6082 - val_acc: 0.7014\n",
            "Epoch 21/500\n",
            "20109/20109 [==============================] - 7s 344us/sample - loss: 0.6087 - acc: 0.7017 - val_loss: 0.6078 - val_acc: 0.7014\n",
            "Epoch 22/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.6083 - acc: 0.7018 - val_loss: 0.6075 - val_acc: 0.7014\n",
            "Epoch 23/500\n",
            "20109/20109 [==============================] - 7s 344us/sample - loss: 0.6080 - acc: 0.7017 - val_loss: 0.6073 - val_acc: 0.7014\n",
            "Epoch 24/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.6074 - acc: 0.7017 - val_loss: 0.6068 - val_acc: 0.7014\n",
            "Epoch 25/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.6069 - acc: 0.7017 - val_loss: 0.6063 - val_acc: 0.7014\n",
            "Epoch 26/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.6062 - acc: 0.7017 - val_loss: 0.6059 - val_acc: 0.7014\n",
            "Epoch 27/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.6060 - acc: 0.7018 - val_loss: 0.6056 - val_acc: 0.7014\n",
            "Epoch 28/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.6055 - acc: 0.7018 - val_loss: 0.6051 - val_acc: 0.7014\n",
            "Epoch 29/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.6048 - acc: 0.7016 - val_loss: 0.6046 - val_acc: 0.7014\n",
            "Epoch 30/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.6044 - acc: 0.7017 - val_loss: 0.6038 - val_acc: 0.7014\n",
            "Epoch 31/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.6036 - acc: 0.7016 - val_loss: 0.6032 - val_acc: 0.7014\n",
            "Epoch 32/500\n",
            "20109/20109 [==============================] - 7s 361us/sample - loss: 0.6027 - acc: 0.7017 - val_loss: 0.6023 - val_acc: 0.7014\n",
            "Epoch 33/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.6014 - acc: 0.7017 - val_loss: 0.6013 - val_acc: 0.7014\n",
            "Epoch 34/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.6005 - acc: 0.7017 - val_loss: 0.6001 - val_acc: 0.7014\n",
            "Epoch 35/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.5994 - acc: 0.7016 - val_loss: 0.5990 - val_acc: 0.7014\n",
            "Epoch 36/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.5980 - acc: 0.7016 - val_loss: 0.5974 - val_acc: 0.7014\n",
            "Epoch 37/500\n",
            "20109/20109 [==============================] - 7s 351us/sample - loss: 0.5957 - acc: 0.7016 - val_loss: 0.5957 - val_acc: 0.7014\n",
            "Epoch 38/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.5941 - acc: 0.7016 - val_loss: 0.5935 - val_acc: 0.7014\n",
            "Epoch 39/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.5913 - acc: 0.7017 - val_loss: 0.5909 - val_acc: 0.7014\n",
            "Epoch 40/500\n",
            "20109/20109 [==============================] - 7s 342us/sample - loss: 0.5886 - acc: 0.7017 - val_loss: 0.5878 - val_acc: 0.7014\n",
            "Epoch 41/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.5854 - acc: 0.7017 - val_loss: 0.5846 - val_acc: 0.7014\n",
            "Epoch 42/500\n",
            "20109/20109 [==============================] - 7s 344us/sample - loss: 0.5829 - acc: 0.7018 - val_loss: 0.5812 - val_acc: 0.7014\n",
            "Epoch 43/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.5783 - acc: 0.7017 - val_loss: 0.5773 - val_acc: 0.7014\n",
            "Epoch 44/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.5749 - acc: 0.7017 - val_loss: 0.5736 - val_acc: 0.7014\n",
            "Epoch 45/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.5716 - acc: 0.7018 - val_loss: 0.5692 - val_acc: 0.7014\n",
            "Epoch 46/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.5667 - acc: 0.7018 - val_loss: 0.5647 - val_acc: 0.7014\n",
            "Epoch 47/500\n",
            "20109/20109 [==============================] - 7s 349us/sample - loss: 0.5626 - acc: 0.7023 - val_loss: 0.5589 - val_acc: 0.7014\n",
            "Epoch 48/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.5593 - acc: 0.7035 - val_loss: 0.5567 - val_acc: 0.7018\n",
            "Epoch 49/500\n",
            "20109/20109 [==============================] - 7s 344us/sample - loss: 0.5549 - acc: 0.7033 - val_loss: 0.5518 - val_acc: 0.7018\n",
            "Epoch 50/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.5515 - acc: 0.7055 - val_loss: 0.5480 - val_acc: 0.7022\n",
            "Epoch 51/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.5464 - acc: 0.7062 - val_loss: 0.5424 - val_acc: 0.7030\n",
            "Epoch 52/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.5437 - acc: 0.7094 - val_loss: 0.5388 - val_acc: 0.7044\n",
            "Epoch 53/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.5394 - acc: 0.7104 - val_loss: 0.5328 - val_acc: 0.7066\n",
            "Epoch 54/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.5336 - acc: 0.7157 - val_loss: 0.5283 - val_acc: 0.7096\n",
            "Epoch 55/500\n",
            "20109/20109 [==============================] - 7s 346us/sample - loss: 0.5295 - acc: 0.7204 - val_loss: 0.5219 - val_acc: 0.7100\n",
            "Epoch 56/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.5281 - acc: 0.7183 - val_loss: 0.5189 - val_acc: 0.7142\n",
            "Epoch 57/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.5182 - acc: 0.7269 - val_loss: 0.5132 - val_acc: 0.7205\n",
            "Epoch 58/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.5137 - acc: 0.7356 - val_loss: 0.5067 - val_acc: 0.7259\n",
            "Epoch 59/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.5070 - acc: 0.7415 - val_loss: 0.4976 - val_acc: 0.7329\n",
            "Epoch 60/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.5036 - acc: 0.7448 - val_loss: 0.4962 - val_acc: 0.7355\n",
            "Epoch 61/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.4995 - acc: 0.7494 - val_loss: 0.4937 - val_acc: 0.7500\n",
            "Epoch 62/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.4931 - acc: 0.7588 - val_loss: 0.4841 - val_acc: 0.7490\n",
            "Epoch 63/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.4851 - acc: 0.7618 - val_loss: 0.4751 - val_acc: 0.7582\n",
            "Epoch 64/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.4821 - acc: 0.7668 - val_loss: 0.4730 - val_acc: 0.7661\n",
            "Epoch 65/500\n",
            "20109/20109 [==============================] - 6s 320us/sample - loss: 0.4752 - acc: 0.7746 - val_loss: 0.4689 - val_acc: 0.7755\n",
            "Epoch 66/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.4695 - acc: 0.7798 - val_loss: 0.4599 - val_acc: 0.7811\n",
            "Epoch 67/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.4643 - acc: 0.7866 - val_loss: 0.4533 - val_acc: 0.7870\n",
            "Epoch 68/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.4597 - acc: 0.7912 - val_loss: 0.4478 - val_acc: 0.7888\n",
            "Epoch 69/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.4538 - acc: 0.7939 - val_loss: 0.4480 - val_acc: 0.7968\n",
            "Epoch 70/500\n",
            "20109/20109 [==============================] - 7s 346us/sample - loss: 0.4463 - acc: 0.8021 - val_loss: 0.4384 - val_acc: 0.7992\n",
            "Epoch 71/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.4422 - acc: 0.8044 - val_loss: 0.4337 - val_acc: 0.8055\n",
            "Epoch 72/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.4373 - acc: 0.8073 - val_loss: 0.4281 - val_acc: 0.8111\n",
            "Epoch 73/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.4305 - acc: 0.8120 - val_loss: 0.4209 - val_acc: 0.8147\n",
            "Epoch 74/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.4259 - acc: 0.8161 - val_loss: 0.4214 - val_acc: 0.8161\n",
            "Epoch 75/500\n",
            "20109/20109 [==============================] - 7s 342us/sample - loss: 0.4198 - acc: 0.8210 - val_loss: 0.4132 - val_acc: 0.8205\n",
            "Epoch 76/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.4155 - acc: 0.8230 - val_loss: 0.4083 - val_acc: 0.8219\n",
            "Epoch 77/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.4101 - acc: 0.8251 - val_loss: 0.4052 - val_acc: 0.8221\n",
            "Epoch 78/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.4099 - acc: 0.8247 - val_loss: 0.4036 - val_acc: 0.8217\n",
            "Epoch 79/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.4070 - acc: 0.8250 - val_loss: 0.4007 - val_acc: 0.8225\n",
            "Epoch 80/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.4027 - acc: 0.8268 - val_loss: 0.3977 - val_acc: 0.8254\n",
            "Epoch 81/500\n",
            "20109/20109 [==============================] - 7s 347us/sample - loss: 0.3994 - acc: 0.8320 - val_loss: 0.3951 - val_acc: 0.8274\n",
            "Epoch 82/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3969 - acc: 0.8314 - val_loss: 0.3922 - val_acc: 0.8292\n",
            "Epoch 83/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.3926 - acc: 0.8333 - val_loss: 0.3906 - val_acc: 0.8288\n",
            "Epoch 84/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3885 - acc: 0.8356 - val_loss: 0.3860 - val_acc: 0.8298\n",
            "Epoch 85/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.3826 - acc: 0.8398 - val_loss: 0.3804 - val_acc: 0.8314\n",
            "Epoch 86/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3831 - acc: 0.8370 - val_loss: 0.3811 - val_acc: 0.8326\n",
            "Epoch 87/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.3799 - acc: 0.8399 - val_loss: 0.3808 - val_acc: 0.8340\n",
            "Epoch 88/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3759 - acc: 0.8426 - val_loss: 0.3775 - val_acc: 0.8336\n",
            "Epoch 89/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.3739 - acc: 0.8401 - val_loss: 0.3754 - val_acc: 0.8346\n",
            "Epoch 90/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.3725 - acc: 0.8449 - val_loss: 0.3725 - val_acc: 0.8360\n",
            "Epoch 91/500\n",
            "20109/20109 [==============================] - 7s 346us/sample - loss: 0.3695 - acc: 0.8441 - val_loss: 0.3713 - val_acc: 0.8350\n",
            "Epoch 92/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3709 - acc: 0.8434 - val_loss: 0.3730 - val_acc: 0.8390\n",
            "Epoch 93/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3661 - acc: 0.8469 - val_loss: 0.3706 - val_acc: 0.8388\n",
            "Epoch 94/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.3651 - acc: 0.8462 - val_loss: 0.3680 - val_acc: 0.8386\n",
            "Epoch 95/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.3624 - acc: 0.8480 - val_loss: 0.3672 - val_acc: 0.8414\n",
            "Epoch 96/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3629 - acc: 0.8482 - val_loss: 0.3670 - val_acc: 0.8398\n",
            "Epoch 97/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3591 - acc: 0.8503 - val_loss: 0.3662 - val_acc: 0.8402\n",
            "Epoch 98/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3565 - acc: 0.8491 - val_loss: 0.3630 - val_acc: 0.8426\n",
            "Epoch 99/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.3568 - acc: 0.8502 - val_loss: 0.3651 - val_acc: 0.8465\n",
            "Epoch 100/500\n",
            "20109/20109 [==============================] - 7s 342us/sample - loss: 0.3548 - acc: 0.8509 - val_loss: 0.3627 - val_acc: 0.8438\n",
            "Epoch 101/500\n",
            "20109/20109 [==============================] - 7s 350us/sample - loss: 0.3503 - acc: 0.8550 - val_loss: 0.3634 - val_acc: 0.8455\n",
            "Epoch 102/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.3529 - acc: 0.8526 - val_loss: 0.3597 - val_acc: 0.8455\n",
            "Epoch 103/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.3475 - acc: 0.8564 - val_loss: 0.3601 - val_acc: 0.8467\n",
            "Epoch 104/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.3473 - acc: 0.8565 - val_loss: 0.3583 - val_acc: 0.8461\n",
            "Epoch 105/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3442 - acc: 0.8563 - val_loss: 0.3565 - val_acc: 0.8495\n",
            "Epoch 106/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3442 - acc: 0.8609 - val_loss: 0.3559 - val_acc: 0.8479\n",
            "Epoch 107/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.3401 - acc: 0.8597 - val_loss: 0.3547 - val_acc: 0.8493\n",
            "Epoch 108/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3388 - acc: 0.8591 - val_loss: 0.3537 - val_acc: 0.8501\n",
            "Epoch 109/500\n",
            "20109/20109 [==============================] - 7s 346us/sample - loss: 0.3396 - acc: 0.8590 - val_loss: 0.3555 - val_acc: 0.8477\n",
            "Epoch 110/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.3384 - acc: 0.8606 - val_loss: 0.3547 - val_acc: 0.8485\n",
            "Epoch 111/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.3349 - acc: 0.8628 - val_loss: 0.3538 - val_acc: 0.8499\n",
            "Epoch 112/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.3353 - acc: 0.8625 - val_loss: 0.3533 - val_acc: 0.8493\n",
            "Epoch 113/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.3325 - acc: 0.8635 - val_loss: 0.3515 - val_acc: 0.8505\n",
            "Epoch 114/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.3322 - acc: 0.8651 - val_loss: 0.3512 - val_acc: 0.8505\n",
            "Epoch 115/500\n",
            "20109/20109 [==============================] - 7s 344us/sample - loss: 0.3318 - acc: 0.8636 - val_loss: 0.3509 - val_acc: 0.8499\n",
            "Epoch 116/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3308 - acc: 0.8622 - val_loss: 0.3514 - val_acc: 0.8485\n",
            "Epoch 117/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.3266 - acc: 0.8660 - val_loss: 0.3496 - val_acc: 0.8499\n",
            "Epoch 118/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.3280 - acc: 0.8671 - val_loss: 0.3495 - val_acc: 0.8505\n",
            "Epoch 119/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.3282 - acc: 0.8648 - val_loss: 0.3493 - val_acc: 0.8497\n",
            "Epoch 120/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.3248 - acc: 0.8657 - val_loss: 0.3482 - val_acc: 0.8509\n",
            "Epoch 121/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.3192 - acc: 0.8699 - val_loss: 0.3476 - val_acc: 0.8501\n",
            "Epoch 122/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3240 - acc: 0.8660 - val_loss: 0.3483 - val_acc: 0.8505\n",
            "Epoch 123/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.3205 - acc: 0.8700 - val_loss: 0.3479 - val_acc: 0.8507\n",
            "Epoch 124/500\n",
            "20109/20109 [==============================] - 7s 344us/sample - loss: 0.3189 - acc: 0.8693 - val_loss: 0.3473 - val_acc: 0.8519\n",
            "Epoch 125/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3187 - acc: 0.8694 - val_loss: 0.3462 - val_acc: 0.8537\n",
            "Epoch 126/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.3187 - acc: 0.8711 - val_loss: 0.3462 - val_acc: 0.8517\n",
            "Epoch 127/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.3184 - acc: 0.8691 - val_loss: 0.3456 - val_acc: 0.8531\n",
            "Epoch 128/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3155 - acc: 0.8712 - val_loss: 0.3451 - val_acc: 0.8527\n",
            "Epoch 129/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3167 - acc: 0.8701 - val_loss: 0.3452 - val_acc: 0.8545\n",
            "Epoch 130/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.3138 - acc: 0.8724 - val_loss: 0.3447 - val_acc: 0.8539\n",
            "Epoch 131/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.3118 - acc: 0.8729 - val_loss: 0.3453 - val_acc: 0.8531\n",
            "Epoch 132/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.3124 - acc: 0.8714 - val_loss: 0.3444 - val_acc: 0.8555\n",
            "Epoch 133/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.3103 - acc: 0.8731 - val_loss: 0.3440 - val_acc: 0.8557\n",
            "Epoch 134/500\n",
            "20109/20109 [==============================] - 6s 320us/sample - loss: 0.3109 - acc: 0.8743 - val_loss: 0.3441 - val_acc: 0.8553\n",
            "Epoch 135/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.3087 - acc: 0.8736 - val_loss: 0.3437 - val_acc: 0.8547\n",
            "Epoch 136/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.3094 - acc: 0.8743 - val_loss: 0.3433 - val_acc: 0.8549\n",
            "Epoch 137/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.3062 - acc: 0.8746 - val_loss: 0.3426 - val_acc: 0.8543\n",
            "Epoch 138/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.3062 - acc: 0.8768 - val_loss: 0.3425 - val_acc: 0.8551\n",
            "Epoch 139/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.3067 - acc: 0.8765 - val_loss: 0.3423 - val_acc: 0.8561\n",
            "Epoch 140/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.3044 - acc: 0.8768 - val_loss: 0.3424 - val_acc: 0.8561\n",
            "Epoch 141/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3037 - acc: 0.8772 - val_loss: 0.3423 - val_acc: 0.8559\n",
            "Epoch 142/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.3037 - acc: 0.8769 - val_loss: 0.3417 - val_acc: 0.8559\n",
            "Epoch 143/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.3023 - acc: 0.8781 - val_loss: 0.3420 - val_acc: 0.8563\n",
            "Epoch 144/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3021 - acc: 0.8778 - val_loss: 0.3418 - val_acc: 0.8567\n",
            "Epoch 145/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3026 - acc: 0.8773 - val_loss: 0.3420 - val_acc: 0.8575\n",
            "Epoch 146/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.2988 - acc: 0.8800 - val_loss: 0.3415 - val_acc: 0.8565\n",
            "Epoch 147/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.3000 - acc: 0.8786 - val_loss: 0.3418 - val_acc: 0.8571\n",
            "Epoch 148/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.3005 - acc: 0.8781 - val_loss: 0.3416 - val_acc: 0.8573\n",
            "Epoch 149/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.2984 - acc: 0.8795 - val_loss: 0.3410 - val_acc: 0.8569\n",
            "Epoch 150/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.2949 - acc: 0.8811 - val_loss: 0.3410 - val_acc: 0.8569\n",
            "Epoch 151/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.2951 - acc: 0.8810 - val_loss: 0.3412 - val_acc: 0.8581\n",
            "Epoch 152/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.2973 - acc: 0.8826 - val_loss: 0.3410 - val_acc: 0.8577\n",
            "Epoch 153/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.2942 - acc: 0.8820 - val_loss: 0.3413 - val_acc: 0.8573\n",
            "Epoch 154/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.2950 - acc: 0.8795 - val_loss: 0.3409 - val_acc: 0.8573\n",
            "Epoch 155/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.2921 - acc: 0.8837 - val_loss: 0.3413 - val_acc: 0.8581\n",
            "Epoch 156/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.2912 - acc: 0.8832 - val_loss: 0.3412 - val_acc: 0.8583\n",
            "Epoch 157/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.2938 - acc: 0.8842 - val_loss: 0.3410 - val_acc: 0.8579\n",
            "Epoch 158/500\n",
            "20109/20109 [==============================] - 7s 350us/sample - loss: 0.2909 - acc: 0.8825 - val_loss: 0.3411 - val_acc: 0.8585\n",
            "Epoch 159/500\n",
            "20109/20109 [==============================] - 7s 342us/sample - loss: 0.2914 - acc: 0.8842 - val_loss: 0.3410 - val_acc: 0.8585\n",
            "Epoch 160/500\n",
            "20109/20109 [==============================] - 7s 344us/sample - loss: 0.2918 - acc: 0.8832 - val_loss: 0.3409 - val_acc: 0.8569\n",
            "Epoch 161/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.2913 - acc: 0.8859 - val_loss: 0.3409 - val_acc: 0.8567\n",
            "Epoch 162/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.2890 - acc: 0.8846 - val_loss: 0.3408 - val_acc: 0.8575\n",
            "Epoch 163/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.2865 - acc: 0.8841 - val_loss: 0.3405 - val_acc: 0.8571\n",
            "Epoch 164/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.2876 - acc: 0.8867 - val_loss: 0.3408 - val_acc: 0.8565\n",
            "Epoch 165/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.2866 - acc: 0.8856 - val_loss: 0.3408 - val_acc: 0.8569\n",
            "Epoch 166/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.2861 - acc: 0.8865 - val_loss: 0.3407 - val_acc: 0.8567\n",
            "Epoch 167/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.2869 - acc: 0.8854 - val_loss: 0.3399 - val_acc: 0.8571\n",
            "Epoch 168/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.2839 - acc: 0.8871 - val_loss: 0.3402 - val_acc: 0.8569\n",
            "Epoch 169/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.2854 - acc: 0.8863 - val_loss: 0.3402 - val_acc: 0.8563\n",
            "Epoch 170/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.2846 - acc: 0.8865 - val_loss: 0.3399 - val_acc: 0.8579\n",
            "Epoch 171/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.2836 - acc: 0.8832 - val_loss: 0.3398 - val_acc: 0.8577\n",
            "Epoch 172/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.2851 - acc: 0.8855 - val_loss: 0.3400 - val_acc: 0.8573\n",
            "Epoch 173/500\n",
            "20109/20109 [==============================] - 7s 351us/sample - loss: 0.2830 - acc: 0.8881 - val_loss: 0.3403 - val_acc: 0.8563\n",
            "Epoch 174/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.2806 - acc: 0.8886 - val_loss: 0.3402 - val_acc: 0.8571\n",
            "Epoch 175/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.2813 - acc: 0.8881 - val_loss: 0.3402 - val_acc: 0.8571\n",
            "Epoch 176/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.2821 - acc: 0.8885 - val_loss: 0.3402 - val_acc: 0.8573\n",
            "Epoch 177/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.2794 - acc: 0.8892 - val_loss: 0.3408 - val_acc: 0.8567\n",
            "Epoch 178/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.2819 - acc: 0.8862 - val_loss: 0.3407 - val_acc: 0.8565\n",
            "Epoch 00178: early stopping\n",
            "--- 1257.4818613529205 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20109/20109 [==============================] - 76s 4ms/sample - loss: 0.2612 - acc: 0.8965\n",
            "5027/5027 [==============================] - 18s 4ms/sample - loss: 0.3407 - acc: 0.8562\n",
            "Train Accuracy: 0.8965\n",
            "Train Loss: 0.2612\n",
            "Validation Accuracy:  0.8562\n",
            "Validation Loss: 0.3407\n",
            "5027/5027 [==============================] - 7s 1ms/sample\n",
            "[[1078  422]\n",
            " [ 301 3226]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.72      0.75      1500\n",
            "         1.0       0.88      0.91      0.90      3527\n",
            "\n",
            "    accuracy                           0.86      5027\n",
            "   macro avg       0.83      0.82      0.82      5027\n",
            "weighted avg       0.85      0.86      0.85      5027\n",
            "\n",
            "RMSE: 0.3792\n",
            "--- Fold 5 ---\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.76.36.210:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1277633526131077744)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6387720827749024011)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10496896205887535107)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6969718561809677301)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12961930590832392281)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 103926089697679672)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8238627829294133264)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2041167189725682759)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6959296792125428166)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4386377055783322133)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 302002105411306474)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20109 samples, validate on 5027 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 7.329408884048462 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20109 [==========================>...] - ETA: 2s - loss: 0.6925 - acc: 0.5257INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 9.090409994125366 secs\n",
            "19456/20109 [============================>.] - ETA: 1s - loss: 0.6923 - acc: 0.5337INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 8.33126974105835 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 10.020947456359863 secs\n",
            "20109/20109 [==============================] - 70s 3ms/sample - loss: 0.6922 - acc: 0.5387 - val_loss: 0.6880 - val_acc: 0.7008\n",
            "Epoch 2/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.6841 - acc: 0.6986 - val_loss: 0.6799 - val_acc: 0.7018\n",
            "Epoch 3/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.6759 - acc: 0.7018 - val_loss: 0.6714 - val_acc: 0.7018\n",
            "Epoch 4/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.6671 - acc: 0.7017 - val_loss: 0.6622 - val_acc: 0.7018\n",
            "Epoch 5/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.6576 - acc: 0.7016 - val_loss: 0.6520 - val_acc: 0.7018\n",
            "Epoch 6/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.6467 - acc: 0.7017 - val_loss: 0.6403 - val_acc: 0.7018\n",
            "Epoch 7/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.6349 - acc: 0.7016 - val_loss: 0.6273 - val_acc: 0.7018\n",
            "Epoch 8/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.6217 - acc: 0.7017 - val_loss: 0.6154 - val_acc: 0.7018\n",
            "Epoch 9/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.6143 - acc: 0.7017 - val_loss: 0.6109 - val_acc: 0.7018\n",
            "Epoch 10/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.6124 - acc: 0.7016 - val_loss: 0.6103 - val_acc: 0.7018\n",
            "Epoch 11/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.6120 - acc: 0.7018 - val_loss: 0.6098 - val_acc: 0.7018\n",
            "Epoch 12/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.6110 - acc: 0.7017 - val_loss: 0.6096 - val_acc: 0.7018\n",
            "Epoch 13/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.6112 - acc: 0.7016 - val_loss: 0.6093 - val_acc: 0.7018\n",
            "Epoch 14/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.6108 - acc: 0.7018 - val_loss: 0.6090 - val_acc: 0.7018\n",
            "Epoch 15/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.6107 - acc: 0.7017 - val_loss: 0.6088 - val_acc: 0.7018\n",
            "Epoch 16/500\n",
            "20109/20109 [==============================] - 7s 349us/sample - loss: 0.6101 - acc: 0.7017 - val_loss: 0.6085 - val_acc: 0.7018\n",
            "Epoch 17/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.6097 - acc: 0.7018 - val_loss: 0.6083 - val_acc: 0.7018\n",
            "Epoch 18/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.6097 - acc: 0.7018 - val_loss: 0.6081 - val_acc: 0.7018\n",
            "Epoch 19/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.6099 - acc: 0.7017 - val_loss: 0.6079 - val_acc: 0.7018\n",
            "Epoch 20/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.6091 - acc: 0.7018 - val_loss: 0.6076 - val_acc: 0.7018\n",
            "Epoch 21/500\n",
            "20109/20109 [==============================] - 7s 344us/sample - loss: 0.6092 - acc: 0.7016 - val_loss: 0.6073 - val_acc: 0.7018\n",
            "Epoch 22/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.6083 - acc: 0.7017 - val_loss: 0.6070 - val_acc: 0.7018\n",
            "Epoch 23/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.6080 - acc: 0.7018 - val_loss: 0.6068 - val_acc: 0.7018\n",
            "Epoch 24/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.6080 - acc: 0.7017 - val_loss: 0.6064 - val_acc: 0.7018\n",
            "Epoch 25/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.6074 - acc: 0.7018 - val_loss: 0.6061 - val_acc: 0.7018\n",
            "Epoch 26/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.6072 - acc: 0.7017 - val_loss: 0.6057 - val_acc: 0.7018\n",
            "Epoch 27/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.6065 - acc: 0.7018 - val_loss: 0.6053 - val_acc: 0.7018\n",
            "Epoch 28/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.6063 - acc: 0.7018 - val_loss: 0.6049 - val_acc: 0.7018\n",
            "Epoch 29/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.6052 - acc: 0.7018 - val_loss: 0.6043 - val_acc: 0.7018\n",
            "Epoch 30/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.6048 - acc: 0.7018 - val_loss: 0.6038 - val_acc: 0.7018\n",
            "Epoch 31/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.6044 - acc: 0.7017 - val_loss: 0.6032 - val_acc: 0.7018\n",
            "Epoch 32/500\n",
            "20109/20109 [==============================] - 7s 354us/sample - loss: 0.6037 - acc: 0.7018 - val_loss: 0.6025 - val_acc: 0.7018\n",
            "Epoch 33/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.6027 - acc: 0.7017 - val_loss: 0.6018 - val_acc: 0.7018\n",
            "Epoch 34/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.6022 - acc: 0.7017 - val_loss: 0.6011 - val_acc: 0.7018\n",
            "Epoch 35/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.6012 - acc: 0.7017 - val_loss: 0.6003 - val_acc: 0.7018\n",
            "Epoch 36/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.6001 - acc: 0.7017 - val_loss: 0.5993 - val_acc: 0.7018\n",
            "Epoch 37/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.5991 - acc: 0.7017 - val_loss: 0.5982 - val_acc: 0.7018\n",
            "Epoch 38/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.5980 - acc: 0.7016 - val_loss: 0.5970 - val_acc: 0.7018\n",
            "Epoch 39/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.5973 - acc: 0.7017 - val_loss: 0.5957 - val_acc: 0.7018\n",
            "Epoch 40/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.5955 - acc: 0.7017 - val_loss: 0.5942 - val_acc: 0.7018\n",
            "Epoch 41/500\n",
            "20109/20109 [==============================] - 7s 346us/sample - loss: 0.5939 - acc: 0.7018 - val_loss: 0.5927 - val_acc: 0.7018\n",
            "Epoch 42/500\n",
            "20109/20109 [==============================] - 7s 356us/sample - loss: 0.5921 - acc: 0.7017 - val_loss: 0.5907 - val_acc: 0.7018\n",
            "Epoch 43/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.5894 - acc: 0.7018 - val_loss: 0.5885 - val_acc: 0.7018\n",
            "Epoch 44/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.5876 - acc: 0.7018 - val_loss: 0.5862 - val_acc: 0.7018\n",
            "Epoch 45/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.5848 - acc: 0.7017 - val_loss: 0.5834 - val_acc: 0.7018\n",
            "Epoch 46/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.5822 - acc: 0.7017 - val_loss: 0.5804 - val_acc: 0.7018\n",
            "Epoch 47/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.5787 - acc: 0.7017 - val_loss: 0.5769 - val_acc: 0.7018\n",
            "Epoch 48/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.5754 - acc: 0.7017 - val_loss: 0.5735 - val_acc: 0.7018\n",
            "Epoch 49/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.5721 - acc: 0.7018 - val_loss: 0.5692 - val_acc: 0.7018\n",
            "Epoch 50/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.5679 - acc: 0.7018 - val_loss: 0.5646 - val_acc: 0.7020\n",
            "Epoch 51/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.5627 - acc: 0.7019 - val_loss: 0.5603 - val_acc: 0.7020\n",
            "Epoch 52/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.5593 - acc: 0.7019 - val_loss: 0.5541 - val_acc: 0.7020\n",
            "Epoch 53/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.5528 - acc: 0.7025 - val_loss: 0.5480 - val_acc: 0.7024\n",
            "Epoch 54/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.5479 - acc: 0.7036 - val_loss: 0.5411 - val_acc: 0.7036\n",
            "Epoch 55/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.5424 - acc: 0.7056 - val_loss: 0.5335 - val_acc: 0.7060\n",
            "Epoch 56/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.5363 - acc: 0.7081 - val_loss: 0.5286 - val_acc: 0.7086\n",
            "Epoch 57/500\n",
            "20109/20109 [==============================] - 7s 323us/sample - loss: 0.5284 - acc: 0.7131 - val_loss: 0.5201 - val_acc: 0.7120\n",
            "Epoch 58/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.5215 - acc: 0.7204 - val_loss: 0.5090 - val_acc: 0.7188\n",
            "Epoch 59/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.5158 - acc: 0.7297 - val_loss: 0.5023 - val_acc: 0.7285\n",
            "Epoch 60/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.5072 - acc: 0.7390 - val_loss: 0.4955 - val_acc: 0.7426\n",
            "Epoch 61/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.4997 - acc: 0.7506 - val_loss: 0.4865 - val_acc: 0.7532\n",
            "Epoch 62/500\n",
            "20109/20109 [==============================] - 7s 342us/sample - loss: 0.4936 - acc: 0.7622 - val_loss: 0.4796 - val_acc: 0.7607\n",
            "Epoch 63/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.4885 - acc: 0.7694 - val_loss: 0.4766 - val_acc: 0.7705\n",
            "Epoch 64/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.4816 - acc: 0.7761 - val_loss: 0.4709 - val_acc: 0.7775\n",
            "Epoch 65/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.4774 - acc: 0.7837 - val_loss: 0.4596 - val_acc: 0.7832\n",
            "Epoch 66/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.4697 - acc: 0.7922 - val_loss: 0.4556 - val_acc: 0.7924\n",
            "Epoch 67/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.4646 - acc: 0.7947 - val_loss: 0.4489 - val_acc: 0.7938\n",
            "Epoch 68/500\n",
            "20109/20109 [==============================] - 7s 346us/sample - loss: 0.4576 - acc: 0.7996 - val_loss: 0.4487 - val_acc: 0.8004\n",
            "Epoch 69/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.4554 - acc: 0.8023 - val_loss: 0.4453 - val_acc: 0.8035\n",
            "Epoch 70/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.4523 - acc: 0.8056 - val_loss: 0.4369 - val_acc: 0.8067\n",
            "Epoch 71/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.4444 - acc: 0.8071 - val_loss: 0.4313 - val_acc: 0.8103\n",
            "Epoch 72/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.4431 - acc: 0.8079 - val_loss: 0.4294 - val_acc: 0.8115\n",
            "Epoch 73/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.4364 - acc: 0.8133 - val_loss: 0.4242 - val_acc: 0.8111\n",
            "Epoch 74/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.4346 - acc: 0.8135 - val_loss: 0.4244 - val_acc: 0.8117\n",
            "Epoch 75/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.4292 - acc: 0.8164 - val_loss: 0.4184 - val_acc: 0.8149\n",
            "Epoch 76/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.4232 - acc: 0.8162 - val_loss: 0.4167 - val_acc: 0.8209\n",
            "Epoch 77/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.4232 - acc: 0.8195 - val_loss: 0.4142 - val_acc: 0.8195\n",
            "Epoch 78/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.4178 - acc: 0.8208 - val_loss: 0.4093 - val_acc: 0.8201\n",
            "Epoch 79/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.4169 - acc: 0.8245 - val_loss: 0.4064 - val_acc: 0.8209\n",
            "Epoch 80/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.4106 - acc: 0.8247 - val_loss: 0.4033 - val_acc: 0.8213\n",
            "Epoch 81/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.4075 - acc: 0.8269 - val_loss: 0.3990 - val_acc: 0.8230\n",
            "Epoch 82/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.4040 - acc: 0.8302 - val_loss: 0.3972 - val_acc: 0.8223\n",
            "Epoch 83/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.4023 - acc: 0.8291 - val_loss: 0.3959 - val_acc: 0.8238\n",
            "Epoch 84/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.3993 - acc: 0.8304 - val_loss: 0.3917 - val_acc: 0.8240\n",
            "Epoch 85/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.3997 - acc: 0.8312 - val_loss: 0.3939 - val_acc: 0.8258\n",
            "Epoch 86/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3920 - acc: 0.8349 - val_loss: 0.3860 - val_acc: 0.8268\n",
            "Epoch 87/500\n",
            "20109/20109 [==============================] - 6s 320us/sample - loss: 0.3945 - acc: 0.8352 - val_loss: 0.3869 - val_acc: 0.8276\n",
            "Epoch 88/500\n",
            "20109/20109 [==============================] - 6s 320us/sample - loss: 0.3879 - acc: 0.8379 - val_loss: 0.3828 - val_acc: 0.8306\n",
            "Epoch 89/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.3859 - acc: 0.8383 - val_loss: 0.3845 - val_acc: 0.8284\n",
            "Epoch 90/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3799 - acc: 0.8395 - val_loss: 0.3787 - val_acc: 0.8314\n",
            "Epoch 91/500\n",
            "20109/20109 [==============================] - 7s 345us/sample - loss: 0.3808 - acc: 0.8415 - val_loss: 0.3772 - val_acc: 0.8336\n",
            "Epoch 92/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3793 - acc: 0.8415 - val_loss: 0.3781 - val_acc: 0.8322\n",
            "Epoch 93/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.3765 - acc: 0.8409 - val_loss: 0.3725 - val_acc: 0.8364\n",
            "Epoch 94/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.3743 - acc: 0.8445 - val_loss: 0.3757 - val_acc: 0.8334\n",
            "Epoch 95/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.3719 - acc: 0.8454 - val_loss: 0.3717 - val_acc: 0.8366\n",
            "Epoch 96/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.3721 - acc: 0.8452 - val_loss: 0.3728 - val_acc: 0.8344\n",
            "Epoch 97/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3662 - acc: 0.8476 - val_loss: 0.3702 - val_acc: 0.8368\n",
            "Epoch 98/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3650 - acc: 0.8503 - val_loss: 0.3682 - val_acc: 0.8384\n",
            "Epoch 99/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.3650 - acc: 0.8482 - val_loss: 0.3663 - val_acc: 0.8398\n",
            "Epoch 100/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3645 - acc: 0.8498 - val_loss: 0.3648 - val_acc: 0.8394\n",
            "Epoch 101/500\n",
            "20109/20109 [==============================] - 7s 348us/sample - loss: 0.3611 - acc: 0.8497 - val_loss: 0.3640 - val_acc: 0.8398\n",
            "Epoch 102/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.3598 - acc: 0.8531 - val_loss: 0.3636 - val_acc: 0.8404\n",
            "Epoch 103/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.3570 - acc: 0.8535 - val_loss: 0.3599 - val_acc: 0.8430\n",
            "Epoch 104/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.3542 - acc: 0.8525 - val_loss: 0.3612 - val_acc: 0.8408\n",
            "Epoch 105/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.3547 - acc: 0.8537 - val_loss: 0.3612 - val_acc: 0.8410\n",
            "Epoch 106/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.3530 - acc: 0.8535 - val_loss: 0.3600 - val_acc: 0.8410\n",
            "Epoch 107/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.3484 - acc: 0.8565 - val_loss: 0.3553 - val_acc: 0.8447\n",
            "Epoch 108/500\n",
            "20109/20109 [==============================] - 7s 344us/sample - loss: 0.3485 - acc: 0.8563 - val_loss: 0.3559 - val_acc: 0.8434\n",
            "Epoch 109/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3471 - acc: 0.8563 - val_loss: 0.3569 - val_acc: 0.8438\n",
            "Epoch 110/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.3459 - acc: 0.8572 - val_loss: 0.3555 - val_acc: 0.8443\n",
            "Epoch 111/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.3451 - acc: 0.8572 - val_loss: 0.3555 - val_acc: 0.8461\n",
            "Epoch 112/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.3414 - acc: 0.8609 - val_loss: 0.3532 - val_acc: 0.8455\n",
            "Epoch 113/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.3445 - acc: 0.8567 - val_loss: 0.3545 - val_acc: 0.8441\n",
            "Epoch 114/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.3430 - acc: 0.8599 - val_loss: 0.3528 - val_acc: 0.8463\n",
            "Epoch 115/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.3409 - acc: 0.8594 - val_loss: 0.3495 - val_acc: 0.8501\n",
            "Epoch 116/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3381 - acc: 0.8596 - val_loss: 0.3490 - val_acc: 0.8489\n",
            "Epoch 117/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.3363 - acc: 0.8626 - val_loss: 0.3486 - val_acc: 0.8483\n",
            "Epoch 118/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3344 - acc: 0.8606 - val_loss: 0.3468 - val_acc: 0.8495\n",
            "Epoch 119/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3350 - acc: 0.8648 - val_loss: 0.3472 - val_acc: 0.8503\n",
            "Epoch 120/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.3319 - acc: 0.8633 - val_loss: 0.3459 - val_acc: 0.8513\n",
            "Epoch 121/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.3312 - acc: 0.8645 - val_loss: 0.3461 - val_acc: 0.8507\n",
            "Epoch 122/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.3270 - acc: 0.8666 - val_loss: 0.3447 - val_acc: 0.8511\n",
            "Epoch 123/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.3300 - acc: 0.8645 - val_loss: 0.3435 - val_acc: 0.8513\n",
            "Epoch 124/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.3299 - acc: 0.8650 - val_loss: 0.3440 - val_acc: 0.8511\n",
            "Epoch 125/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.3261 - acc: 0.8659 - val_loss: 0.3428 - val_acc: 0.8519\n",
            "Epoch 126/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3263 - acc: 0.8662 - val_loss: 0.3441 - val_acc: 0.8501\n",
            "Epoch 127/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3214 - acc: 0.8683 - val_loss: 0.3422 - val_acc: 0.8521\n",
            "Epoch 128/500\n",
            "20109/20109 [==============================] - 7s 351us/sample - loss: 0.3218 - acc: 0.8685 - val_loss: 0.3431 - val_acc: 0.8503\n",
            "Epoch 129/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.3228 - acc: 0.8673 - val_loss: 0.3413 - val_acc: 0.8523\n",
            "Epoch 130/500\n",
            "20109/20109 [==============================] - 7s 346us/sample - loss: 0.3233 - acc: 0.8672 - val_loss: 0.3420 - val_acc: 0.8513\n",
            "Epoch 131/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3221 - acc: 0.8667 - val_loss: 0.3418 - val_acc: 0.8507\n",
            "Epoch 132/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.3188 - acc: 0.8708 - val_loss: 0.3400 - val_acc: 0.8539\n",
            "Epoch 133/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3193 - acc: 0.8699 - val_loss: 0.3404 - val_acc: 0.8517\n",
            "Epoch 134/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.3169 - acc: 0.8724 - val_loss: 0.3411 - val_acc: 0.8511\n",
            "Epoch 135/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.3173 - acc: 0.8720 - val_loss: 0.3401 - val_acc: 0.8539\n",
            "Epoch 136/500\n",
            "20109/20109 [==============================] - 7s 343us/sample - loss: 0.3168 - acc: 0.8710 - val_loss: 0.3395 - val_acc: 0.8543\n",
            "Epoch 137/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.3143 - acc: 0.8742 - val_loss: 0.3377 - val_acc: 0.8541\n",
            "Epoch 138/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.3129 - acc: 0.8726 - val_loss: 0.3382 - val_acc: 0.8533\n",
            "Epoch 139/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.3126 - acc: 0.8729 - val_loss: 0.3376 - val_acc: 0.8531\n",
            "Epoch 140/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3119 - acc: 0.8732 - val_loss: 0.3382 - val_acc: 0.8545\n",
            "Epoch 141/500\n",
            "20109/20109 [==============================] - 7s 344us/sample - loss: 0.3091 - acc: 0.8751 - val_loss: 0.3363 - val_acc: 0.8565\n",
            "Epoch 142/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.3085 - acc: 0.8746 - val_loss: 0.3369 - val_acc: 0.8557\n",
            "Epoch 143/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.3092 - acc: 0.8750 - val_loss: 0.3368 - val_acc: 0.8543\n",
            "Epoch 144/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.3075 - acc: 0.8751 - val_loss: 0.3365 - val_acc: 0.8559\n",
            "Epoch 145/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.3096 - acc: 0.8751 - val_loss: 0.3366 - val_acc: 0.8547\n",
            "Epoch 146/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.3039 - acc: 0.8753 - val_loss: 0.3352 - val_acc: 0.8583\n",
            "Epoch 147/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.3046 - acc: 0.8759 - val_loss: 0.3353 - val_acc: 0.8571\n",
            "Epoch 148/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.3066 - acc: 0.8776 - val_loss: 0.3355 - val_acc: 0.8567\n",
            "Epoch 149/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.3049 - acc: 0.8745 - val_loss: 0.3351 - val_acc: 0.8569\n",
            "Epoch 150/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3031 - acc: 0.8780 - val_loss: 0.3344 - val_acc: 0.8575\n",
            "Epoch 151/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3040 - acc: 0.8774 - val_loss: 0.3350 - val_acc: 0.8573\n",
            "Epoch 152/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.3014 - acc: 0.8780 - val_loss: 0.3346 - val_acc: 0.8567\n",
            "Epoch 153/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.2974 - acc: 0.8806 - val_loss: 0.3345 - val_acc: 0.8559\n",
            "Epoch 154/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.2999 - acc: 0.8790 - val_loss: 0.3337 - val_acc: 0.8573\n",
            "Epoch 155/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.2979 - acc: 0.8787 - val_loss: 0.3335 - val_acc: 0.8571\n",
            "Epoch 156/500\n",
            "20109/20109 [==============================] - 7s 323us/sample - loss: 0.2979 - acc: 0.8812 - val_loss: 0.3341 - val_acc: 0.8567\n",
            "Epoch 157/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.2989 - acc: 0.8802 - val_loss: 0.3337 - val_acc: 0.8569\n",
            "Epoch 158/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.2975 - acc: 0.8792 - val_loss: 0.3345 - val_acc: 0.8573\n",
            "Epoch 159/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.2962 - acc: 0.8811 - val_loss: 0.3341 - val_acc: 0.8567\n",
            "Epoch 160/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.2969 - acc: 0.8816 - val_loss: 0.3337 - val_acc: 0.8569\n",
            "Epoch 161/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.2967 - acc: 0.8800 - val_loss: 0.3334 - val_acc: 0.8569\n",
            "Epoch 162/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.2925 - acc: 0.8811 - val_loss: 0.3335 - val_acc: 0.8571\n",
            "Epoch 163/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.2911 - acc: 0.8813 - val_loss: 0.3326 - val_acc: 0.8589\n",
            "Epoch 164/500\n",
            "20109/20109 [==============================] - 7s 339us/sample - loss: 0.2937 - acc: 0.8816 - val_loss: 0.3330 - val_acc: 0.8581\n",
            "Epoch 165/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.2930 - acc: 0.8833 - val_loss: 0.3335 - val_acc: 0.8581\n",
            "Epoch 166/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.2890 - acc: 0.8830 - val_loss: 0.3323 - val_acc: 0.8595\n",
            "Epoch 167/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.2911 - acc: 0.8827 - val_loss: 0.3333 - val_acc: 0.8575\n",
            "Epoch 168/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.2889 - acc: 0.8847 - val_loss: 0.3327 - val_acc: 0.8579\n",
            "Epoch 169/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.2907 - acc: 0.8835 - val_loss: 0.3327 - val_acc: 0.8587\n",
            "Epoch 170/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.2886 - acc: 0.8836 - val_loss: 0.3320 - val_acc: 0.8589\n",
            "Epoch 171/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.2865 - acc: 0.8845 - val_loss: 0.3316 - val_acc: 0.8601\n",
            "Epoch 172/500\n",
            "20109/20109 [==============================] - 7s 346us/sample - loss: 0.2883 - acc: 0.8864 - val_loss: 0.3322 - val_acc: 0.8585\n",
            "Epoch 173/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.2884 - acc: 0.8846 - val_loss: 0.3325 - val_acc: 0.8589\n",
            "Epoch 174/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.2853 - acc: 0.8841 - val_loss: 0.3317 - val_acc: 0.8605\n",
            "Epoch 175/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.2857 - acc: 0.8872 - val_loss: 0.3325 - val_acc: 0.8585\n",
            "Epoch 176/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.2851 - acc: 0.8851 - val_loss: 0.3322 - val_acc: 0.8593\n",
            "Epoch 177/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.2848 - acc: 0.8856 - val_loss: 0.3320 - val_acc: 0.8595\n",
            "Epoch 178/500\n",
            "20109/20109 [==============================] - 7s 333us/sample - loss: 0.2857 - acc: 0.8849 - val_loss: 0.3321 - val_acc: 0.8597\n",
            "Epoch 00178: early stopping\n",
            "--- 1254.6242294311523 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20109/20109 [==============================] - 78s 4ms/sample - loss: 0.2669 - acc: 0.8930\n",
            "5027/5027 [==============================] - 19s 4ms/sample - loss: 0.3320 - acc: 0.8596\n",
            "Train Accuracy: 0.8930\n",
            "Train Loss: 0.2669\n",
            "Validation Accuracy:  0.8596\n",
            "Validation Loss: 0.3320\n",
            "5027/5027 [==============================] - 7s 1ms/sample\n",
            "[[1081  419]\n",
            " [ 287 3240]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.72      0.75      1500\n",
            "         1.0       0.89      0.92      0.90      3527\n",
            "\n",
            "    accuracy                           0.86      5027\n",
            "   macro avg       0.84      0.82      0.83      5027\n",
            "weighted avg       0.86      0.86      0.86      5027\n",
            "\n",
            "RMSE: 0.3748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhZW6ObDXI8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a1d1ccdb-23ca-4f9c-841e-3e91d67e4b44"
      },
      "source": [
        "print(\"Average Train Accuracy: %.4f%% (+/- %.4f%%)\" % (np.mean(tr_acc_array), np.std(tr_acc_array)))\n",
        "print(\"Average Train Loss: %.4f%% (+/- %.4f%%)\" % (np.mean(tr_loss_array), np.std(tr_loss_array)))\n",
        "print(\"Average Validation Accuracy: %.4f%% (+/- %.4f%%)\" % (np.mean(te_acc_array), np.std(te_acc_array)))\n",
        "print(\"Average Validation Loss: %.4f%% (+/- %.4f%%)\" % (np.mean(te_loss_array), np.std(te_loss_array)))\n",
        "print(\"Average Time: %.4f%% (+/- %.4f%%)\" % (np.mean(time_array), np.std(time_array)))\n",
        "print(\"Average RMSE: %.4f%% (+/- %.4f%%)\" % (np.mean(rmse_array), np.std(rmse_array)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Train Accuracy: 0.8940% (+/- 0.0030%)\n",
            "Average Train Loss: 0.2661% (+/- 0.0077%)\n",
            "Average Validation Accuracy: 0.8532% (+/- 0.0042%)\n",
            "Average Validation Loss: 0.3420% (+/- 0.0070%)\n",
            "Average Time: 1164.1215% (+/- 86.6439%)\n",
            "Average RMSE: 0.3831% (+/- 0.0054%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaXCaRhZ7Otf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.DataFrame({'Train Acc': tr_acc_array, 'Train Loss': tr_loss_array, 'Validation Acc': te_acc_array, 'Validation Loss': te_loss_array, 'Time': time_array, 'RMSE': rmse_array})\n",
        "# dataset.to_csv('data_blstm.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp4OdfgSLhSP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3250e814-a933-47ca-d082-218fc3ee46f4"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train Acc</th>\n",
              "      <th>Train Loss</th>\n",
              "      <th>Validation Acc</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Time</th>\n",
              "      <th>RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.888801</td>\n",
              "      <td>0.280142</td>\n",
              "      <td>0.851432</td>\n",
              "      <td>0.341470</td>\n",
              "      <td>1024.128362</td>\n",
              "      <td>0.385445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.894376</td>\n",
              "      <td>0.264465</td>\n",
              "      <td>0.851005</td>\n",
              "      <td>0.342011</td>\n",
              "      <td>1148.033411</td>\n",
              "      <td>0.385999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.897061</td>\n",
              "      <td>0.257576</td>\n",
              "      <td>0.847822</td>\n",
              "      <td>0.353929</td>\n",
              "      <td>1136.338805</td>\n",
              "      <td>0.390100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.896514</td>\n",
              "      <td>0.261225</td>\n",
              "      <td>0.856177</td>\n",
              "      <td>0.340726</td>\n",
              "      <td>1257.482391</td>\n",
              "      <td>0.379240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.893033</td>\n",
              "      <td>0.266852</td>\n",
              "      <td>0.859558</td>\n",
              "      <td>0.331989</td>\n",
              "      <td>1254.624696</td>\n",
              "      <td>0.374755</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Train Acc  Train Loss  ...         Time      RMSE\n",
              "0   0.888801    0.280142  ...  1024.128362  0.385445\n",
              "1   0.894376    0.264465  ...  1148.033411  0.385999\n",
              "2   0.897061    0.257576  ...  1136.338805  0.390100\n",
              "3   0.896514    0.261225  ...  1257.482391  0.379240\n",
              "4   0.893033    0.266852  ...  1254.624696  0.374755\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3MQQcmILVxv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import time\n",
        "# from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "# start_time = time.time()\n",
        "\n",
        "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\n",
        "\n",
        "  \n",
        "# history = tpu_model.fit(x_train, y_train\n",
        "#                     ,epochs=epochs, verbose=1 \n",
        "#                     ,validation_split=0.2\n",
        "#                     ,batch_size=128 * 8\n",
        "#                     ,validation_data=(x_test, y_test)\n",
        "#                     ,callbacks=[es]\n",
        "#                    )\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# # tpu_model.save_weights('./tpu_model.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB09B2pkVxv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss, accuracy = tpu_model.evaluate(x_train, y_train, verbose=1)\n",
        "# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "# loss, accuracy = tpu_model.evaluate(x_test, y_test, verbose=1)\n",
        "# print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSoKam1eVxwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "e1da2a58-a385-470e-d79a-53d7b5f6cff7"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAFACAYAAAD9D55TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuBI2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKMA0luELvtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_df = pd.DataFrame(list(zip(df['review'].values, df['cleaned'].values, df['sentiment'].values, y_pred)), columns = ['review','cleaned','sentiment','predict'])\n",
        "# df['predict'] = df['predict'].astype(int)1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHmVU73xL8xE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "ee0288de-3f55-4797-8eab-928e3bb99f98"
      },
      "source": [
        "check_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>cleaned</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This movie is the beginning of the culmination...</td>\n",
              "      <td>beginning culmination masterfully woven cinema...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Over the past decade, Marvel has earned itself...</td>\n",
              "      <td>past decade earned benefit doubt consistently ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This film is way better than endgame!\\nThe act...</td>\n",
              "      <td>way better action better writing better dialog...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Summer movies often hype themselves as spectac...</td>\n",
              "      <td>summer often hype spectacular event missed ad ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I was amazed to see so many negative reviews; ...</td>\n",
              "      <td>amazed negative impossible please hour long co...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5022</th>\n",
              "      <td>Admittingly, I was not a fan of the original. ...</td>\n",
              "      <td>admittingly fan original found first eye candi...</td>\n",
              "      <td>0</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5023</th>\n",
              "      <td>Like the first round, it appears it is always ...</td>\n",
              "      <td>like first round appears always smart keep ori...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5024</th>\n",
              "      <td>With the exception of 'Captain America: The Wi...</td>\n",
              "      <td>exception america winter soldier cinematic uni...</td>\n",
              "      <td>1</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5025</th>\n",
              "      <td>I loved it! It was funny and witty and had all...</td>\n",
              "      <td>loved funny witty ingredient make best film fa...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5026</th>\n",
              "      <td>There are a few minor spoilers. I will try to ...</td>\n",
              "      <td>minor spoiler try got first weekend got fun sp...</td>\n",
              "      <td>1</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5027 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 review  ...  predict\n",
              "0     This movie is the beginning of the culmination...  ...   [True]\n",
              "1     Over the past decade, Marvel has earned itself...  ...   [True]\n",
              "2     This film is way better than endgame!\\nThe act...  ...   [True]\n",
              "3     Summer movies often hype themselves as spectac...  ...   [True]\n",
              "4     I was amazed to see so many negative reviews; ...  ...   [True]\n",
              "...                                                 ...  ...      ...\n",
              "5022  Admittingly, I was not a fan of the original. ...  ...  [False]\n",
              "5023  Like the first round, it appears it is always ...  ...   [True]\n",
              "5024  With the exception of 'Captain America: The Wi...  ...  [False]\n",
              "5025  I loved it! It was funny and witty and had all...  ...   [True]\n",
              "5026  There are a few minor spoilers. I will try to ...  ...  [False]\n",
              "\n",
              "[5027 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvFNRqJf2_ph",
        "colab_type": "text"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE6l-_ye928X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nan = pd.read_csv(\"nan.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46sUeQBIGzCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "176a89b3-0e3f-4ae7-cb62-9f986dc54f1c"
      },
      "source": [
        "nan.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>rating</th>\n",
              "      <th>title</th>\n",
              "      <th>review</th>\n",
              "      <th>number</th>\n",
              "      <th>cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>69</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A tedious and dependent film</td>\n",
              "      <td>SPOILER: The plot is simple that the supervill...</td>\n",
              "      <td>0</td>\n",
              "      <td>spoiler simple supervillian succeeds russo bro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>126</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Thanos the Emo Crybaby...Thanos Mickey Mouse S...</td>\n",
              "      <td>12 jokes within 3 minutes in conversation betw...</td>\n",
              "      <td>0</td>\n",
              "      <td>joke within minute conversation doctor strange...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>139</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A snore -- literally</td>\n",
              "      <td>This film was complete and utter tosh. Judging...</td>\n",
              "      <td>0</td>\n",
              "      <td>complete utter tosh judging way positively rev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>146</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Thanos and Bubbles, Kiddie Mickey Mouse Style</td>\n",
              "      <td>I don't recognize the THANOS The MAD TITAN fro...</td>\n",
              "      <td>0</td>\n",
              "      <td>recognize thanos mad titan comic book thanos w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>176</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Good movie, but completely ruined</td>\n",
              "      <td>Spoilers!!!Spoilers: the movie was pretty good...</td>\n",
              "      <td>0</td>\n",
              "      <td>spoiler spoiler pretty good bit quippy good en...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                            cleaned\n",
              "0          69  ...  spoiler simple supervillian succeeds russo bro...\n",
              "1         126  ...  joke within minute conversation doctor strange...\n",
              "2         139  ...  complete utter tosh judging way positively rev...\n",
              "3         146  ...  recognize thanos mad titan comic book thanos w...\n",
              "4         176  ...  spoiler spoiler pretty good bit quippy good en...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqUZKx2pHjb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08aa425d-380e-4a8b-ec12-e9cf982bd49c"
      },
      "source": [
        "nan.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(533, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZmDfVllGvgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sent in nan:\n",
        "    review = nan['review'].values\n",
        "    cleaned = nan['cleaned'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQDuXnbdG_bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_str = cleaned.astype(str)\n",
        "\n",
        "tokenizer = Tokenizer(char_level=False)\n",
        "tokenizer.fit_on_texts(cleaned_str)\n",
        "cleaned_bow = tokenizer.texts_to_sequences(cleaned_str) #text_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5sbZADIHAYc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "583f2510-62fc-4f55-af25-1188226b0b71"
      },
      "source": [
        "x_pred = pad_sequences(cleaned_bow, maxlen=max_len)\n",
        "print('Shape of data tensor:', x_pred.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (533, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tBYTrU1Z3Gee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d62e9f7-a6ba-46b2-c1e7-afd630d984f1"
      },
      "source": [
        "pred = model.predict(x_pred, batch_size=100, verbose = 1)\n",
        "y_pred = (pred > 0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "533/533 [==============================] - 1s 2ms/sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4C7LWibHHRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df = pd.DataFrame(list(zip(review, y_pred)), columns = ['review','predict'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prz4vzbfHxgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "6e4ffd06-0200-4025-8c8c-83f1b0c2365a"
      },
      "source": [
        "pred_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SPOILER: The plot is simple that the supervill...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12 jokes within 3 minutes in conversation betw...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This film was complete and utter tosh. Judging...</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I don't recognize the THANOS The MAD TITAN fro...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spoilers!!!Spoilers: the movie was pretty good...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>528</th>\n",
              "      <td>Just finished watching this movie. My husband ...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>529</th>\n",
              "      <td>Ok at best. Big script let down from what's su...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530</th>\n",
              "      <td>Had this movie come out in 2009 close to \"Iron...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>531</th>\n",
              "      <td>I'm a middle-aged white male. Saw Captain Marv...</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>PROS:Brie Larson's Is Great As Captain MarvelS...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>533 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                review  predict\n",
              "0    SPOILER: The plot is simple that the supervill...   [True]\n",
              "1    12 jokes within 3 minutes in conversation betw...   [True]\n",
              "2    This film was complete and utter tosh. Judging...  [False]\n",
              "3    I don't recognize the THANOS The MAD TITAN fro...   [True]\n",
              "4    Spoilers!!!Spoilers: the movie was pretty good...   [True]\n",
              "..                                                 ...      ...\n",
              "528  Just finished watching this movie. My husband ...   [True]\n",
              "529  Ok at best. Big script let down from what's su...   [True]\n",
              "530  Had this movie come out in 2009 close to \"Iron...   [True]\n",
              "531  I'm a middle-aged white male. Saw Captain Marv...  [False]\n",
              "532  PROS:Brie Larson's Is Great As Captain MarvelS...   [True]\n",
              "\n",
              "[533 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzteJqDNHyPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df.to_csv('pred_blstm.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3IJpIgSvefv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ce9b5c32-b9fa-4b45-c600-5a6114c07356"
      },
      "source": [
        "model.save(\"blstm.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMw2C2U3voMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
