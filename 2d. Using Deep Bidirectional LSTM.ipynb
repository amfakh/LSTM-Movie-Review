{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "04. DBLSTM v4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amfakh/LSTM-Movie-Review/blob/master/2d.%20Using%20Deep%20Bidirectional%20LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0RAuwjiBIAv",
        "colab_type": "text"
      },
      "source": [
        "## Force Tensorflow version 1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcQfl2Xw3wj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "603dc8ae-6b13-4fd8-aa41-420831ecf665"
      },
      "source": [
        "!pip install tensorflow==1.13.1\n",
        "!pip install -qq -U cufflinks\n",
        "!pip install matplotlib==3.1.0\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.17.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (42.0.2)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (3.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: matplotlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (2.4.6)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (1.17.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib==3.1.0) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.1.0) (42.0.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUdDGKLNhH7m",
        "colab_type": "text"
      },
      "source": [
        "## Access Google Drive files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "danJ2_x2WArZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "bc3f9c9a-8d59-4e98-c833-df17bdab6200"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FebLd3UKY6bp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ac70434-d505-4a08-d0fa-d51722f2088d"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2TTdQzHVxtY",
        "colab_type": "text"
      },
      "source": [
        "# Read all the Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8R-ttCYh25P",
        "colab_type": "text"
      },
      "source": [
        "## Read the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B5dzs6P8BFTi",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKGJfAyLkaH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"df.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_SWTGHmjLA9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "030dd5c3-f068-44fc-cc52-53cf9688b3c9"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>rating</th>\n",
              "      <th>title</th>\n",
              "      <th>review</th>\n",
              "      <th>number</th>\n",
              "      <th>cleaned</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>10/10</td>\n",
              "      <td>Unlike anything ever done in the history of ci...</td>\n",
              "      <td>This movie is the beginning of the culmination...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>beginning culmination masterfully woven cinema...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10/10</td>\n",
              "      <td>This movie will blow your mind and break your ...</td>\n",
              "      <td>Over the past decade, Marvel has earned itself...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>past decade earned benefit doubt consistently ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10/10</td>\n",
              "      <td>Way better than endgame</td>\n",
              "      <td>This film is way better than endgame!\\nThe act...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>way better action better writing better dialog...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>10/10</td>\n",
              "      <td>A Summer Film That IS Even Better Than The Hype</td>\n",
              "      <td>Summer movies often hype themselves as spectac...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>summer often hype spectacular event missed ad ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9/10</td>\n",
              "      <td>Excellent Film</td>\n",
              "      <td>I was amazed to see so many negative reviews; ...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>amazed negative impossible please hour long co...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... sentiment\n",
              "0           0  ...         1\n",
              "1           1  ...         1\n",
              "2           2  ...         1\n",
              "3           3  ...         1\n",
              "4           4  ...         1\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfVnOiW7Vxuw",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDWlhG7riIej",
        "colab_type": "text"
      },
      "source": [
        "## Split dataset into datatrain and datatest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKZ_qtURVxuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# for sent in df:\n",
        "train_review = df['cleaned'].values\n",
        "y_train = df['sentiment'].values\n",
        "\n",
        "    # train_review, test_review, y_train, y_test = train_test_split(\n",
        "    #     review, y, test_size=0.3, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_V1qN1DlR6C",
        "colab_type": "text"
      },
      "source": [
        "## Convert text to Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnxXFsJOj75b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fce6e0e9-3f0e-4183-cbd9-fade29102164"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Make sure the data type is str\n",
        "review_train = train_review.astype(str)\n",
        "# review_test = test_review.astype(str)\n",
        "\n",
        "tokenizer = Tokenizer(char_level=False)\n",
        "tokenizer.fit_on_texts(review_train)\n",
        "text_train = tokenizer.texts_to_sequences(review_train)\n",
        "# text_test = tokenizer.texts_to_sequences(review_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RKaFdrumRU8",
        "colab_type": "text"
      },
      "source": [
        "### (Additional) Declare the vocab size to maximum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeDzKUe2mPnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3008fbb-afae-44d0-a3b6-179942840e7c"
      },
      "source": [
        "max_words = len(tokenizer.word_index) + 1  \n",
        "# Adding 1 because of reserved 0 index\n",
        "print('%s unique words.' % max_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37657 unique words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqSYqlz9ma2J",
        "colab_type": "text"
      },
      "source": [
        "### (Additional) Declare maximum length to maximum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdm00yw7Vxvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5cb67e6f-aec4-49f4-847e-f025fa6b3810"
      },
      "source": [
        "text_len = [len(r) for r in text_train]\n",
        "print(\"average length: %0.1f\" % np.mean(text_len))\n",
        "print(\"max length: %d\" % max(text_len))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average length: 75.4\n",
            "max length: 899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRnRzsuJVxvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.hist(text_len, bins=500);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7ud35jqtATi",
        "colab_type": "text"
      },
      "source": [
        "## Declare max_words and max_len"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUIFIA_KVxvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of words to consider as features\n",
        "# max_words = 50000\n",
        "# Cut texts after this number of words (among top max_features most common words)\n",
        "max_len = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhpgG68KmjPc",
        "colab_type": "text"
      },
      "source": [
        "## Pad Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QuVFfpS8TCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4exEzKqvVxvq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cf1477a-e30f-4aff-eb3f-e635356dad2f"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# max_length = max(text_len)\n",
        "\n",
        "# pad sequences with 0s\n",
        "x = pad_sequences(text_train, maxlen=max_len)\n",
        "# x_test = pad_sequences(text_test, maxlen=max_len)\n",
        "print('Shape of data tensor:', x.shape)\n",
        "# print('Shape of data test tensor:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (25136, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAAG8Xuint1U",
        "colab_type": "text"
      },
      "source": [
        "### Check the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gTZu3xAVxvs",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# print(review_train[4])\n",
        "# print(x_train[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avB7woI-nYFq",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# print(y_train[0])\n",
        "# print(y_test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJzDy6xEn-FK",
        "colab_type": "text"
      },
      "source": [
        "Convert label to float"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN4W32bErzB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
        "# y_test = np.asarray(y_test).astype('float32').reshape((-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aYup7IgoCah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8ccdfcb-6763-434d-f1e0-4be81ff4d545"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYYmmalwOsVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras import utils\n",
        "# num_classes = 2\n",
        "\n",
        "# y_train_binary = utils.to_categorical(y_train, num_classes)\n",
        "# y_test_binary = utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nR_WOPVPB72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(y_train_binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g70IGUjUBxxb",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAOsnygio29e",
        "colab_type": "text"
      },
      "source": [
        "## Import stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "906eUoh_o1zm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ed37143f-7523-4ea8-c24d-7a4abed916c5"
      },
      "source": [
        "from tensorflow.python.keras.layers import Dropout, Dense, Embedding\n",
        "from tensorflow.python.keras.layers import SpatialDropout1D, GlobalMaxPool1D\n",
        "from tensorflow.python.keras.layers import LSTM, Input, Bidirectional, GRU\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n",
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF9_O4lIVxvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = x.shape[1]  # Number of features\n",
        "embedding_dim = 128\n",
        "learning_rate = 2e-5\n",
        "epochs = 500\n",
        "decay_rate = 1e-3 # learning_rate / epochs\n",
        "\n",
        "def make_model(batch_size=None):\n",
        "  source = Input(shape=(max_len,), name='Input')\n",
        "  embedding = Embedding(input_dim=max_words,\n",
        "                        output_dim=embedding_dim,\n",
        "                        input_length=max_len)(source)\n",
        "  # spatial = SpatialDropout1D(0.4)(embedding)\n",
        "  lstm = Bidirectional(LSTM(128, \n",
        "              # activation='softsign',\n",
        "              dropout=0.3,\n",
        "              recurrent_dropout=0.3,\n",
        "              # use_bias=True\n",
        "              return_sequences=True\n",
        "              ))(embedding)\n",
        "  lstm = Bidirectional(LSTM(128, \n",
        "              # activation='softsign',\n",
        "              dropout=0.3,\n",
        "              recurrent_dropout=0.3,\n",
        "              # use_bias=True\n",
        "              return_sequences=True\n",
        "              ))(lstm)\n",
        "  lstm = Bidirectional(LSTM(128, \n",
        "              # activation='softsign',\n",
        "              dropout=0.3,\n",
        "              recurrent_dropout=0.3,\n",
        "              # use_bias=True\n",
        "              # ,return_sequences=True\n",
        "              ))(lstm)\n",
        "  # pool = GlobalMaxPool1D()(lstm)\n",
        "  # dense = Dense(24)(lstm)\n",
        "  # drop = Dropout(0.4)(dense)\n",
        "\n",
        "  # lstm = Bidirectional(LSTM(196, activation='softsign',\n",
        "  #           dropout=0.7,\n",
        "  #           recurrent_dropout=0.7,\n",
        "  #           use_bias=True\n",
        "  #           # ,return_sequences=True\n",
        "  #           ))(drop)\n",
        "  # dense = Dense(50)(lstm)\n",
        "  # drop = Dropout(0.7)(dense)\n",
        "\n",
        "  predict = Dense(1, activation='sigmoid')(lstm)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[source], outputs=[predict])\n",
        "\n",
        "  rmsprop = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=decay_rate)\n",
        "  model.compile(\n",
        "      optimizer=rmsprop,\n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EenkZdsjvHsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "# tpu_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rnwt-lEyKTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1ChsH4VVxv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Train acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Train and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Train loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Train and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUDhBL7kv_9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve,roc_auc_score\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "# sns.set()\n",
        "# sns.heatmap(confusion.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "#             xticklabels=np.unique(y_pred),\n",
        "#             yticklabels=np.unique(y_pred))\n",
        "# plt.xlabel('true label')\n",
        "# plt.ylabel('predicted label')\n",
        "# plt.title('Confusion Matrix Prediction')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0h77kZoUcym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ec617c7-d2fb-49e3-a321-68b5589b05b5"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import time, math\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\n",
        "\n",
        "tr_acc_array = []\n",
        "tr_loss_array = []\n",
        "te_acc_array = []\n",
        "te_loss_array = []\n",
        "time_array = []\n",
        "rmse_array = []\n",
        "\n",
        "n=0\n",
        "\n",
        "\n",
        "for train, test in kfold.split(x, y):\n",
        "  n+=1\n",
        "  print(\"--- Fold %d ---\" % (n))\n",
        "  tf.keras.backend.clear_session()\n",
        "  training_model = make_model(batch_size = 128)\n",
        "  # training_model.summary()\n",
        "\n",
        "  tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    training_model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "  start_time = time.time()\n",
        "  history = tpu_model.fit(x[train], y[train]\n",
        "                    ,epochs=epochs, verbose=1 \n",
        "                    ,validation_split=0.2\n",
        "                    ,batch_size=128 * 8\n",
        "                    ,validation_data=(x[test], y[test])\n",
        "                    ,callbacks=[es]\n",
        "                   )\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  time_array.append(time.time() - start_time)\n",
        "\n",
        "  model = tpu_model.sync_to_cpu()\n",
        "\n",
        "  tr_loss, tr_accuracy = model.evaluate(x[train], y[train], verbose=1)\n",
        "  te_loss, te_accuracy = model.evaluate(x[test], y[test], verbose=1)\n",
        "  print(\"Train Accuracy: {:.4f}\".format(tr_accuracy))\n",
        "  tr_acc_array.append(tr_accuracy)\n",
        "  print(\"Train Loss: {:.4f}\".format(tr_loss))\n",
        "  tr_loss_array.append(tr_loss)\n",
        "  print(\"Validation Accuracy:  {:.4f}\".format(te_accuracy))\n",
        "  te_acc_array.append(te_accuracy)\n",
        "  print(\"Validation Loss: {:.4f}\".format(te_loss))\n",
        "  te_loss_array.append(te_loss)\n",
        "\n",
        "  pred = model.predict(x[test], batch_size=1000, verbose = 1)\n",
        "  y_pred = y_pred = (pred > 0.5)\n",
        "\n",
        "  print(confusion_matrix(y[test], y_pred))\n",
        "  # sns.set()\n",
        "  # sns.heatmap(confusion.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "  #             xticklabels=np.unique(y_pred),\n",
        "  #             yticklabels=np.unique(y_pred))\n",
        "  # plt.xlabel('true label')\n",
        "  # plt.ylabel('predicted label')\n",
        "  # plt.title('Confusion Matrix Prediction')\n",
        "\n",
        "\n",
        "  print(classification_report(y[test], y_pred))\n",
        "  \n",
        "  # calculate root mean squared error\n",
        "  rmse = math.sqrt(mean_squared_error(y[test], y_pred))\n",
        "  print('RMSE: %.4f' % (rmse))\n",
        "  rmse_array.append(rmse)\n",
        "  # testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "  # print('Test Score: %.2f RMSE' % (testScore))\n",
        "\n",
        "  # auc_score=roc_auc_score(y[test], y_pred)  \n",
        "  # print('AUC: %.2f' % (auc_score))\n",
        "\n",
        "  # cvscores.append(scores[1] * 100)\n",
        "\n",
        "  # plot_history(history)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Fold 1 ---\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.80.200.18:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 10941942503576000892)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14433294298198489494)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11267183220444994076)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6375897316121174547)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10811646719542814541)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 13074149627867749380)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6491719624428219428)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6773084868664945562)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 14971483707359683640)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6484311248139258291)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 8976632525022643677)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20108 samples, validate on 5028 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 12.017135858535767 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20108 [==========================>...] - ETA: 4s - loss: 0.6891 - acc: 0.6357INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 15.999502897262573 secs\n",
            "19456/20108 [============================>.] - ETA: 2s - loss: 0.6888 - acc: 0.6391INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 14.666980743408203 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 17.648857355117798 secs\n",
            "20108/20108 [==============================] - 130s 6ms/sample - loss: 0.6886 - acc: 0.6407 - val_loss: 0.6831 - val_acc: 0.7014\n",
            "Epoch 2/500\n",
            "20108/20108 [==============================] - 16s 784us/sample - loss: 0.6758 - acc: 0.7016 - val_loss: 0.6687 - val_acc: 0.7014\n",
            "Epoch 3/500\n",
            "20108/20108 [==============================] - 16s 782us/sample - loss: 0.6558 - acc: 0.7016 - val_loss: 0.6478 - val_acc: 0.7014\n",
            "Epoch 4/500\n",
            "20108/20108 [==============================] - 16s 783us/sample - loss: 0.6298 - acc: 0.7017 - val_loss: 0.6299 - val_acc: 0.7014\n",
            "Epoch 5/500\n",
            "20108/20108 [==============================] - 16s 774us/sample - loss: 0.6158 - acc: 0.7016 - val_loss: 0.6274 - val_acc: 0.7014\n",
            "Epoch 6/500\n",
            "20108/20108 [==============================] - 15s 755us/sample - loss: 0.6137 - acc: 0.7018 - val_loss: 0.6215 - val_acc: 0.7014\n",
            "Epoch 7/500\n",
            "20108/20108 [==============================] - 16s 784us/sample - loss: 0.6126 - acc: 0.7016 - val_loss: 0.6188 - val_acc: 0.7014\n",
            "Epoch 8/500\n",
            "20108/20108 [==============================] - 15s 764us/sample - loss: 0.6117 - acc: 0.7017 - val_loss: 0.6163 - val_acc: 0.7014\n",
            "Epoch 9/500\n",
            "20108/20108 [==============================] - 15s 756us/sample - loss: 0.6115 - acc: 0.7017 - val_loss: 0.6161 - val_acc: 0.7014\n",
            "Epoch 10/500\n",
            "20108/20108 [==============================] - 16s 776us/sample - loss: 0.6105 - acc: 0.7017 - val_loss: 0.6146 - val_acc: 0.7014\n",
            "Epoch 11/500\n",
            "20108/20108 [==============================] - 16s 791us/sample - loss: 0.6106 - acc: 0.7017 - val_loss: 0.6143 - val_acc: 0.7014\n",
            "Epoch 12/500\n",
            "20108/20108 [==============================] - 15s 770us/sample - loss: 0.6105 - acc: 0.7016 - val_loss: 0.6136 - val_acc: 0.7014\n",
            "Epoch 13/500\n",
            "20108/20108 [==============================] - 16s 777us/sample - loss: 0.6093 - acc: 0.7016 - val_loss: 0.6133 - val_acc: 0.7014\n",
            "Epoch 14/500\n",
            "20108/20108 [==============================] - 16s 777us/sample - loss: 0.6092 - acc: 0.7017 - val_loss: 0.6122 - val_acc: 0.7014\n",
            "Epoch 15/500\n",
            "20108/20108 [==============================] - 15s 758us/sample - loss: 0.6091 - acc: 0.7017 - val_loss: 0.6113 - val_acc: 0.7014\n",
            "Epoch 16/500\n",
            "20108/20108 [==============================] - 15s 760us/sample - loss: 0.6085 - acc: 0.7016 - val_loss: 0.6106 - val_acc: 0.7014\n",
            "Epoch 17/500\n",
            "20108/20108 [==============================] - 16s 778us/sample - loss: 0.6075 - acc: 0.7017 - val_loss: 0.6097 - val_acc: 0.7014\n",
            "Epoch 18/500\n",
            "20108/20108 [==============================] - 16s 788us/sample - loss: 0.6073 - acc: 0.7017 - val_loss: 0.6088 - val_acc: 0.7014\n",
            "Epoch 19/500\n",
            "20108/20108 [==============================] - 16s 785us/sample - loss: 0.6060 - acc: 0.7017 - val_loss: 0.6075 - val_acc: 0.7014\n",
            "Epoch 20/500\n",
            "20108/20108 [==============================] - 16s 781us/sample - loss: 0.6052 - acc: 0.7016 - val_loss: 0.6064 - val_acc: 0.7014\n",
            "Epoch 21/500\n",
            "20108/20108 [==============================] - 16s 782us/sample - loss: 0.6040 - acc: 0.7017 - val_loss: 0.6048 - val_acc: 0.7014\n",
            "Epoch 22/500\n",
            "20108/20108 [==============================] - 16s 779us/sample - loss: 0.6017 - acc: 0.7016 - val_loss: 0.6028 - val_acc: 0.7014\n",
            "Epoch 23/500\n",
            "20108/20108 [==============================] - 16s 783us/sample - loss: 0.5999 - acc: 0.7016 - val_loss: 0.6001 - val_acc: 0.7014\n",
            "Epoch 24/500\n",
            "20108/20108 [==============================] - 16s 778us/sample - loss: 0.5970 - acc: 0.7017 - val_loss: 0.5964 - val_acc: 0.7014\n",
            "Epoch 25/500\n",
            "20108/20108 [==============================] - 16s 771us/sample - loss: 0.5924 - acc: 0.7016 - val_loss: 0.5908 - val_acc: 0.7014\n",
            "Epoch 26/500\n",
            "20108/20108 [==============================] - 15s 761us/sample - loss: 0.5870 - acc: 0.7017 - val_loss: 0.5827 - val_acc: 0.7014\n",
            "Epoch 27/500\n",
            "20108/20108 [==============================] - 16s 787us/sample - loss: 0.5780 - acc: 0.7017 - val_loss: 0.5705 - val_acc: 0.7014\n",
            "Epoch 28/500\n",
            "20108/20108 [==============================] - 16s 776us/sample - loss: 0.5639 - acc: 0.7029 - val_loss: 0.5503 - val_acc: 0.7028\n",
            "Epoch 29/500\n",
            "20108/20108 [==============================] - 16s 791us/sample - loss: 0.5432 - acc: 0.7138 - val_loss: 0.5193 - val_acc: 0.7245\n",
            "Epoch 30/500\n",
            "20108/20108 [==============================] - 16s 775us/sample - loss: 0.5185 - acc: 0.7389 - val_loss: 0.4846 - val_acc: 0.7544\n",
            "Epoch 31/500\n",
            "20108/20108 [==============================] - 16s 786us/sample - loss: 0.4943 - acc: 0.7618 - val_loss: 0.4563 - val_acc: 0.7816\n",
            "Epoch 32/500\n",
            "20108/20108 [==============================] - 15s 767us/sample - loss: 0.4767 - acc: 0.7746 - val_loss: 0.4371 - val_acc: 0.8051\n",
            "Epoch 33/500\n",
            "20108/20108 [==============================] - 16s 773us/sample - loss: 0.4566 - acc: 0.7919 - val_loss: 0.4261 - val_acc: 0.8081\n",
            "Epoch 34/500\n",
            "20108/20108 [==============================] - 16s 778us/sample - loss: 0.4479 - acc: 0.7963 - val_loss: 0.4171 - val_acc: 0.8123\n",
            "Epoch 35/500\n",
            "20108/20108 [==============================] - 16s 773us/sample - loss: 0.4353 - acc: 0.8016 - val_loss: 0.4108 - val_acc: 0.8133\n",
            "Epoch 36/500\n",
            "20108/20108 [==============================] - 15s 770us/sample - loss: 0.4289 - acc: 0.8070 - val_loss: 0.4067 - val_acc: 0.8167\n",
            "Epoch 37/500\n",
            "20108/20108 [==============================] - 15s 764us/sample - loss: 0.4217 - acc: 0.8089 - val_loss: 0.4025 - val_acc: 0.8187\n",
            "Epoch 38/500\n",
            "20108/20108 [==============================] - 15s 770us/sample - loss: 0.4144 - acc: 0.8166 - val_loss: 0.3977 - val_acc: 0.8211\n",
            "Epoch 39/500\n",
            "20108/20108 [==============================] - 16s 787us/sample - loss: 0.4065 - acc: 0.8194 - val_loss: 0.3942 - val_acc: 0.8236\n",
            "Epoch 40/500\n",
            "20108/20108 [==============================] - 15s 758us/sample - loss: 0.4058 - acc: 0.8223 - val_loss: 0.3922 - val_acc: 0.8248\n",
            "Epoch 41/500\n",
            "20108/20108 [==============================] - 15s 768us/sample - loss: 0.3997 - acc: 0.8232 - val_loss: 0.3907 - val_acc: 0.8262\n",
            "Epoch 42/500\n",
            "20108/20108 [==============================] - 16s 778us/sample - loss: 0.3903 - acc: 0.8294 - val_loss: 0.3865 - val_acc: 0.8256\n",
            "Epoch 43/500\n",
            "20108/20108 [==============================] - 16s 773us/sample - loss: 0.3868 - acc: 0.8312 - val_loss: 0.3846 - val_acc: 0.8266\n",
            "Epoch 44/500\n",
            "20108/20108 [==============================] - 16s 777us/sample - loss: 0.3866 - acc: 0.8318 - val_loss: 0.3832 - val_acc: 0.8280\n",
            "Epoch 45/500\n",
            "20108/20108 [==============================] - 16s 778us/sample - loss: 0.3828 - acc: 0.8334 - val_loss: 0.3821 - val_acc: 0.8276\n",
            "Epoch 46/500\n",
            "20108/20108 [==============================] - 16s 779us/sample - loss: 0.3733 - acc: 0.8394 - val_loss: 0.3793 - val_acc: 0.8304\n",
            "Epoch 47/500\n",
            "20108/20108 [==============================] - 16s 776us/sample - loss: 0.3729 - acc: 0.8396 - val_loss: 0.3773 - val_acc: 0.8316\n",
            "Epoch 48/500\n",
            "20108/20108 [==============================] - 15s 758us/sample - loss: 0.3718 - acc: 0.8413 - val_loss: 0.3760 - val_acc: 0.8322\n",
            "Epoch 49/500\n",
            "20108/20108 [==============================] - 16s 783us/sample - loss: 0.3665 - acc: 0.8429 - val_loss: 0.3757 - val_acc: 0.8332\n",
            "Epoch 50/500\n",
            "20108/20108 [==============================] - 16s 779us/sample - loss: 0.3633 - acc: 0.8445 - val_loss: 0.3754 - val_acc: 0.8348\n",
            "Epoch 51/500\n",
            "20108/20108 [==============================] - 15s 757us/sample - loss: 0.3627 - acc: 0.8464 - val_loss: 0.3724 - val_acc: 0.8366\n",
            "Epoch 52/500\n",
            "20108/20108 [==============================] - 15s 760us/sample - loss: 0.3592 - acc: 0.8468 - val_loss: 0.3721 - val_acc: 0.8364\n",
            "Epoch 53/500\n",
            "20108/20108 [==============================] - 16s 774us/sample - loss: 0.3559 - acc: 0.8479 - val_loss: 0.3704 - val_acc: 0.8382\n",
            "Epoch 54/500\n",
            "20108/20108 [==============================] - 16s 772us/sample - loss: 0.3535 - acc: 0.8520 - val_loss: 0.3692 - val_acc: 0.8376\n",
            "Epoch 55/500\n",
            "20108/20108 [==============================] - 16s 787us/sample - loss: 0.3495 - acc: 0.8522 - val_loss: 0.3686 - val_acc: 0.8398\n",
            "Epoch 56/500\n",
            "20108/20108 [==============================] - 16s 772us/sample - loss: 0.3465 - acc: 0.8540 - val_loss: 0.3686 - val_acc: 0.8386\n",
            "Epoch 57/500\n",
            "20108/20108 [==============================] - 15s 762us/sample - loss: 0.3458 - acc: 0.8530 - val_loss: 0.3678 - val_acc: 0.8400\n",
            "Epoch 58/500\n",
            "20108/20108 [==============================] - 15s 744us/sample - loss: 0.3424 - acc: 0.8564 - val_loss: 0.3673 - val_acc: 0.8390\n",
            "Epoch 59/500\n",
            "20108/20108 [==============================] - 16s 783us/sample - loss: 0.3394 - acc: 0.8572 - val_loss: 0.3649 - val_acc: 0.8424\n",
            "Epoch 60/500\n",
            "20108/20108 [==============================] - 16s 778us/sample - loss: 0.3367 - acc: 0.8605 - val_loss: 0.3662 - val_acc: 0.8416\n",
            "Epoch 61/500\n",
            "20108/20108 [==============================] - 15s 768us/sample - loss: 0.3323 - acc: 0.8627 - val_loss: 0.3650 - val_acc: 0.8436\n",
            "Epoch 62/500\n",
            "20108/20108 [==============================] - 16s 772us/sample - loss: 0.3335 - acc: 0.8607 - val_loss: 0.3637 - val_acc: 0.8441\n",
            "Epoch 63/500\n",
            "20108/20108 [==============================] - 15s 765us/sample - loss: 0.3307 - acc: 0.8629 - val_loss: 0.3642 - val_acc: 0.8439\n",
            "Epoch 64/500\n",
            "20108/20108 [==============================] - 15s 759us/sample - loss: 0.3284 - acc: 0.8668 - val_loss: 0.3630 - val_acc: 0.8443\n",
            "Epoch 65/500\n",
            "20108/20108 [==============================] - 16s 773us/sample - loss: 0.3247 - acc: 0.8644 - val_loss: 0.3658 - val_acc: 0.8428\n",
            "Epoch 66/500\n",
            "20108/20108 [==============================] - 15s 763us/sample - loss: 0.3248 - acc: 0.8646 - val_loss: 0.3624 - val_acc: 0.8445\n",
            "Epoch 67/500\n",
            "20108/20108 [==============================] - 15s 760us/sample - loss: 0.3236 - acc: 0.8666 - val_loss: 0.3638 - val_acc: 0.8426\n",
            "Epoch 68/500\n",
            "20108/20108 [==============================] - 16s 778us/sample - loss: 0.3220 - acc: 0.8683 - val_loss: 0.3627 - val_acc: 0.8441\n",
            "Epoch 69/500\n",
            "20108/20108 [==============================] - 16s 777us/sample - loss: 0.3179 - acc: 0.8691 - val_loss: 0.3624 - val_acc: 0.8463\n",
            "Epoch 70/500\n",
            "20108/20108 [==============================] - 16s 773us/sample - loss: 0.3179 - acc: 0.8693 - val_loss: 0.3622 - val_acc: 0.8465\n",
            "Epoch 71/500\n",
            "20108/20108 [==============================] - 15s 766us/sample - loss: 0.3154 - acc: 0.8716 - val_loss: 0.3634 - val_acc: 0.8457\n",
            "Epoch 72/500\n",
            "20108/20108 [==============================] - 15s 759us/sample - loss: 0.3137 - acc: 0.8724 - val_loss: 0.3599 - val_acc: 0.8479\n",
            "Epoch 73/500\n",
            "20108/20108 [==============================] - 15s 758us/sample - loss: 0.3113 - acc: 0.8729 - val_loss: 0.3612 - val_acc: 0.8481\n",
            "Epoch 74/500\n",
            "20108/20108 [==============================] - 15s 763us/sample - loss: 0.3108 - acc: 0.8720 - val_loss: 0.3612 - val_acc: 0.8483\n",
            "Epoch 75/500\n",
            "20108/20108 [==============================] - 16s 776us/sample - loss: 0.3048 - acc: 0.8742 - val_loss: 0.3610 - val_acc: 0.8495\n",
            "Epoch 76/500\n",
            "20108/20108 [==============================] - 16s 773us/sample - loss: 0.3055 - acc: 0.8757 - val_loss: 0.3608 - val_acc: 0.8487\n",
            "Epoch 77/500\n",
            "20108/20108 [==============================] - 15s 770us/sample - loss: 0.3047 - acc: 0.8762 - val_loss: 0.3589 - val_acc: 0.8501\n",
            "Epoch 78/500\n",
            "20108/20108 [==============================] - 15s 763us/sample - loss: 0.3043 - acc: 0.8764 - val_loss: 0.3586 - val_acc: 0.8511\n",
            "Epoch 79/500\n",
            "20108/20108 [==============================] - 15s 770us/sample - loss: 0.2996 - acc: 0.8808 - val_loss: 0.3603 - val_acc: 0.8497\n",
            "Epoch 80/500\n",
            "20108/20108 [==============================] - 15s 765us/sample - loss: 0.3010 - acc: 0.8785 - val_loss: 0.3600 - val_acc: 0.8511\n",
            "Epoch 81/500\n",
            "20108/20108 [==============================] - 16s 778us/sample - loss: 0.3004 - acc: 0.8788 - val_loss: 0.3604 - val_acc: 0.8515\n",
            "Epoch 82/500\n",
            "20108/20108 [==============================] - 16s 771us/sample - loss: 0.2960 - acc: 0.8847 - val_loss: 0.3618 - val_acc: 0.8501\n",
            "Epoch 83/500\n",
            "20108/20108 [==============================] - 15s 767us/sample - loss: 0.2982 - acc: 0.8797 - val_loss: 0.3608 - val_acc: 0.8517\n",
            "Epoch 84/500\n",
            "20108/20108 [==============================] - 15s 741us/sample - loss: 0.2932 - acc: 0.8834 - val_loss: 0.3624 - val_acc: 0.8511\n",
            "Epoch 85/500\n",
            "20108/20108 [==============================] - 15s 768us/sample - loss: 0.2958 - acc: 0.8830 - val_loss: 0.3596 - val_acc: 0.8501\n",
            "Epoch 00085: early stopping\n",
            "--- 1435.3621604442596 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20108/20108 [==============================] - 255s 13ms/sample - loss: 0.2685 - acc: 0.8949\n",
            "5028/5028 [==============================] - 60s 12ms/sample - loss: 0.3612 - acc: 0.8498\n",
            "Train Accuracy: 0.8949\n",
            "Train Loss: 0.2685\n",
            "Validation Accuracy:  0.8498\n",
            "Validation Loss: 0.3612\n",
            "5028/5028 [==============================] - 19s 4ms/sample\n",
            "[[1110  390]\n",
            " [ 365 3163]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.74      0.75      1500\n",
            "         1.0       0.89      0.90      0.89      3528\n",
            "\n",
            "    accuracy                           0.85      5028\n",
            "   macro avg       0.82      0.82      0.82      5028\n",
            "weighted avg       0.85      0.85      0.85      5028\n",
            "\n",
            "RMSE: 0.3875\n",
            "--- Fold 2 ---\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.80.200.18:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 10941942503576000892)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14433294298198489494)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11267183220444994076)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6375897316121174547)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10811646719542814541)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 13074149627867749380)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6491719624428219428)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6773084868664945562)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 14971483707359683640)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6484311248139258291)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 8976632525022643677)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20109 samples, validate on 5027 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 14.72481369972229 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20109 [==========================>...] - ETA: 4s - loss: 0.6889 - acc: 0.6488INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 17.07391595840454 secs\n",
            "19456/20109 [============================>.] - ETA: 2s - loss: 0.6887 - acc: 0.6510INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 15.962631225585938 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 19.696709632873535 secs\n",
            "20109/20109 [==============================] - 141s 7ms/sample - loss: 0.6886 - acc: 0.6523 - val_loss: 0.6842 - val_acc: 0.7020\n",
            "Epoch 2/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.6790 - acc: 0.7001 - val_loss: 0.6725 - val_acc: 0.7020\n",
            "Epoch 3/500\n",
            "20109/20109 [==============================] - 16s 787us/sample - loss: 0.6642 - acc: 0.7016 - val_loss: 0.6539 - val_acc: 0.7020\n",
            "Epoch 4/500\n",
            "20109/20109 [==============================] - 15s 759us/sample - loss: 0.6415 - acc: 0.7017 - val_loss: 0.6289 - val_acc: 0.7020\n",
            "Epoch 5/500\n",
            "20109/20109 [==============================] - 16s 787us/sample - loss: 0.6194 - acc: 0.7016 - val_loss: 0.6182 - val_acc: 0.7020\n",
            "Epoch 6/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.6149 - acc: 0.7016 - val_loss: 0.6150 - val_acc: 0.7020\n",
            "Epoch 7/500\n",
            "20109/20109 [==============================] - 16s 784us/sample - loss: 0.6137 - acc: 0.7017 - val_loss: 0.6134 - val_acc: 0.7020\n",
            "Epoch 8/500\n",
            "20109/20109 [==============================] - 16s 785us/sample - loss: 0.6127 - acc: 0.7016 - val_loss: 0.6119 - val_acc: 0.7020\n",
            "Epoch 9/500\n",
            "20109/20109 [==============================] - 15s 759us/sample - loss: 0.6118 - acc: 0.7016 - val_loss: 0.6112 - val_acc: 0.7020\n",
            "Epoch 10/500\n",
            "20109/20109 [==============================] - 16s 776us/sample - loss: 0.6111 - acc: 0.7016 - val_loss: 0.6104 - val_acc: 0.7020\n",
            "Epoch 11/500\n",
            "20109/20109 [==============================] - 15s 756us/sample - loss: 0.6105 - acc: 0.7018 - val_loss: 0.6099 - val_acc: 0.7020\n",
            "Epoch 12/500\n",
            "20109/20109 [==============================] - 15s 771us/sample - loss: 0.6105 - acc: 0.7017 - val_loss: 0.6092 - val_acc: 0.7020\n",
            "Epoch 13/500\n",
            "20109/20109 [==============================] - 16s 790us/sample - loss: 0.6103 - acc: 0.7016 - val_loss: 0.6087 - val_acc: 0.7020\n",
            "Epoch 14/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.6097 - acc: 0.7016 - val_loss: 0.6088 - val_acc: 0.7020\n",
            "Epoch 15/500\n",
            "20109/20109 [==============================] - 16s 771us/sample - loss: 0.6093 - acc: 0.7017 - val_loss: 0.6089 - val_acc: 0.7020\n",
            "Epoch 16/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.6093 - acc: 0.7017 - val_loss: 0.6082 - val_acc: 0.7020\n",
            "Epoch 17/500\n",
            "20109/20109 [==============================] - 16s 778us/sample - loss: 0.6089 - acc: 0.7016 - val_loss: 0.6072 - val_acc: 0.7020\n",
            "Epoch 18/500\n",
            "20109/20109 [==============================] - 16s 779us/sample - loss: 0.6081 - acc: 0.7016 - val_loss: 0.6062 - val_acc: 0.7020\n",
            "Epoch 19/500\n",
            "20109/20109 [==============================] - 15s 769us/sample - loss: 0.6072 - acc: 0.7016 - val_loss: 0.6058 - val_acc: 0.7020\n",
            "Epoch 20/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.6069 - acc: 0.7016 - val_loss: 0.6054 - val_acc: 0.7020\n",
            "Epoch 21/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.6057 - acc: 0.7017 - val_loss: 0.6040 - val_acc: 0.7020\n",
            "Epoch 22/500\n",
            "20109/20109 [==============================] - 15s 764us/sample - loss: 0.6048 - acc: 0.7017 - val_loss: 0.6028 - val_acc: 0.7020\n",
            "Epoch 23/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.6031 - acc: 0.7017 - val_loss: 0.6006 - val_acc: 0.7020\n",
            "Epoch 24/500\n",
            "20109/20109 [==============================] - 15s 760us/sample - loss: 0.6011 - acc: 0.7017 - val_loss: 0.5986 - val_acc: 0.7020\n",
            "Epoch 25/500\n",
            "20109/20109 [==============================] - 15s 764us/sample - loss: 0.5990 - acc: 0.7016 - val_loss: 0.5953 - val_acc: 0.7020\n",
            "Epoch 26/500\n",
            "20109/20109 [==============================] - 15s 750us/sample - loss: 0.5953 - acc: 0.7017 - val_loss: 0.5916 - val_acc: 0.7020\n",
            "Epoch 27/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.5905 - acc: 0.7016 - val_loss: 0.5856 - val_acc: 0.7020\n",
            "Epoch 28/500\n",
            "20109/20109 [==============================] - 15s 752us/sample - loss: 0.5849 - acc: 0.7016 - val_loss: 0.5781 - val_acc: 0.7020\n",
            "Epoch 29/500\n",
            "20109/20109 [==============================] - 15s 764us/sample - loss: 0.5744 - acc: 0.7018 - val_loss: 0.5672 - val_acc: 0.7020\n",
            "Epoch 30/500\n",
            "20109/20109 [==============================] - 15s 755us/sample - loss: 0.5613 - acc: 0.7023 - val_loss: 0.5502 - val_acc: 0.7046\n",
            "Epoch 31/500\n",
            "20109/20109 [==============================] - 16s 772us/sample - loss: 0.5391 - acc: 0.7137 - val_loss: 0.5220 - val_acc: 0.7227\n",
            "Epoch 32/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.5075 - acc: 0.7467 - val_loss: 0.4806 - val_acc: 0.7657\n",
            "Epoch 33/500\n",
            "20109/20109 [==============================] - 16s 781us/sample - loss: 0.4748 - acc: 0.7770 - val_loss: 0.4478 - val_acc: 0.7858\n",
            "Epoch 34/500\n",
            "20109/20109 [==============================] - 16s 775us/sample - loss: 0.4541 - acc: 0.7885 - val_loss: 0.4323 - val_acc: 0.8020\n",
            "Epoch 35/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.4426 - acc: 0.7999 - val_loss: 0.4240 - val_acc: 0.8075\n",
            "Epoch 36/500\n",
            "20109/20109 [==============================] - 15s 758us/sample - loss: 0.4346 - acc: 0.8001 - val_loss: 0.4185 - val_acc: 0.8123\n",
            "Epoch 37/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.4247 - acc: 0.8069 - val_loss: 0.4140 - val_acc: 0.8129\n",
            "Epoch 38/500\n",
            "20109/20109 [==============================] - 16s 773us/sample - loss: 0.4193 - acc: 0.8075 - val_loss: 0.4113 - val_acc: 0.8121\n",
            "Epoch 39/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.4137 - acc: 0.8161 - val_loss: 0.4078 - val_acc: 0.8175\n",
            "Epoch 40/500\n",
            "20109/20109 [==============================] - 15s 769us/sample - loss: 0.4070 - acc: 0.8176 - val_loss: 0.4051 - val_acc: 0.8193\n",
            "Epoch 41/500\n",
            "20109/20109 [==============================] - 15s 770us/sample - loss: 0.4024 - acc: 0.8195 - val_loss: 0.4029 - val_acc: 0.8183\n",
            "Epoch 42/500\n",
            "20109/20109 [==============================] - 16s 780us/sample - loss: 0.3962 - acc: 0.8246 - val_loss: 0.4001 - val_acc: 0.8205\n",
            "Epoch 43/500\n",
            "20109/20109 [==============================] - 16s 786us/sample - loss: 0.3930 - acc: 0.8243 - val_loss: 0.3976 - val_acc: 0.8217\n",
            "Epoch 44/500\n",
            "20109/20109 [==============================] - 16s 785us/sample - loss: 0.3911 - acc: 0.8272 - val_loss: 0.3962 - val_acc: 0.8232\n",
            "Epoch 45/500\n",
            "20109/20109 [==============================] - 15s 755us/sample - loss: 0.3817 - acc: 0.8334 - val_loss: 0.3933 - val_acc: 0.8240\n",
            "Epoch 46/500\n",
            "20109/20109 [==============================] - 15s 756us/sample - loss: 0.3818 - acc: 0.8318 - val_loss: 0.3912 - val_acc: 0.8250\n",
            "Epoch 47/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.3788 - acc: 0.8337 - val_loss: 0.3893 - val_acc: 0.8268\n",
            "Epoch 48/500\n",
            "20109/20109 [==============================] - 15s 770us/sample - loss: 0.3741 - acc: 0.8333 - val_loss: 0.3881 - val_acc: 0.8264\n",
            "Epoch 49/500\n",
            "20109/20109 [==============================] - 16s 775us/sample - loss: 0.3709 - acc: 0.8392 - val_loss: 0.3860 - val_acc: 0.8284\n",
            "Epoch 50/500\n",
            "20109/20109 [==============================] - 15s 744us/sample - loss: 0.3695 - acc: 0.8395 - val_loss: 0.3847 - val_acc: 0.8266\n",
            "Epoch 51/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.3617 - acc: 0.8421 - val_loss: 0.3826 - val_acc: 0.8308\n",
            "Epoch 52/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.3620 - acc: 0.8441 - val_loss: 0.3811 - val_acc: 0.8280\n",
            "Epoch 53/500\n",
            "20109/20109 [==============================] - 16s 782us/sample - loss: 0.3594 - acc: 0.8453 - val_loss: 0.3799 - val_acc: 0.8320\n",
            "Epoch 54/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.3562 - acc: 0.8497 - val_loss: 0.3796 - val_acc: 0.8322\n",
            "Epoch 55/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.3538 - acc: 0.8487 - val_loss: 0.3770 - val_acc: 0.8316\n",
            "Epoch 56/500\n",
            "20109/20109 [==============================] - 15s 750us/sample - loss: 0.3463 - acc: 0.8525 - val_loss: 0.3759 - val_acc: 0.8298\n",
            "Epoch 57/500\n",
            "20109/20109 [==============================] - 15s 754us/sample - loss: 0.3463 - acc: 0.8517 - val_loss: 0.3756 - val_acc: 0.8326\n",
            "Epoch 58/500\n",
            "20109/20109 [==============================] - 16s 777us/sample - loss: 0.3457 - acc: 0.8534 - val_loss: 0.3747 - val_acc: 0.8340\n",
            "Epoch 59/500\n",
            "20109/20109 [==============================] - 16s 773us/sample - loss: 0.3451 - acc: 0.8543 - val_loss: 0.3737 - val_acc: 0.8318\n",
            "Epoch 60/500\n",
            "20109/20109 [==============================] - 16s 773us/sample - loss: 0.3439 - acc: 0.8546 - val_loss: 0.3721 - val_acc: 0.8310\n",
            "Epoch 61/500\n",
            "20109/20109 [==============================] - 15s 757us/sample - loss: 0.3372 - acc: 0.8578 - val_loss: 0.3717 - val_acc: 0.8330\n",
            "Epoch 62/500\n",
            "20109/20109 [==============================] - 15s 760us/sample - loss: 0.3365 - acc: 0.8590 - val_loss: 0.3716 - val_acc: 0.8326\n",
            "Epoch 63/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.3347 - acc: 0.8581 - val_loss: 0.3710 - val_acc: 0.8336\n",
            "Epoch 64/500\n",
            "20109/20109 [==============================] - 15s 764us/sample - loss: 0.3313 - acc: 0.8611 - val_loss: 0.3716 - val_acc: 0.8358\n",
            "Epoch 65/500\n",
            "20109/20109 [==============================] - 15s 767us/sample - loss: 0.3298 - acc: 0.8618 - val_loss: 0.3698 - val_acc: 0.8336\n",
            "Epoch 66/500\n",
            "20109/20109 [==============================] - 15s 767us/sample - loss: 0.3268 - acc: 0.8637 - val_loss: 0.3691 - val_acc: 0.8342\n",
            "Epoch 67/500\n",
            "20109/20109 [==============================] - 15s 754us/sample - loss: 0.3263 - acc: 0.8645 - val_loss: 0.3691 - val_acc: 0.8334\n",
            "Epoch 68/500\n",
            "20109/20109 [==============================] - 16s 781us/sample - loss: 0.3255 - acc: 0.8637 - val_loss: 0.3679 - val_acc: 0.8350\n",
            "Epoch 69/500\n",
            "20109/20109 [==============================] - 15s 764us/sample - loss: 0.3224 - acc: 0.8660 - val_loss: 0.3682 - val_acc: 0.8348\n",
            "Epoch 70/500\n",
            "20109/20109 [==============================] - 15s 739us/sample - loss: 0.3189 - acc: 0.8665 - val_loss: 0.3674 - val_acc: 0.8358\n",
            "Epoch 71/500\n",
            "20109/20109 [==============================] - 15s 751us/sample - loss: 0.3169 - acc: 0.8687 - val_loss: 0.3668 - val_acc: 0.8358\n",
            "Epoch 72/500\n",
            "20109/20109 [==============================] - 15s 748us/sample - loss: 0.3163 - acc: 0.8704 - val_loss: 0.3665 - val_acc: 0.8366\n",
            "Epoch 73/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.3161 - acc: 0.8687 - val_loss: 0.3659 - val_acc: 0.8368\n",
            "Epoch 74/500\n",
            "20109/20109 [==============================] - 15s 764us/sample - loss: 0.3121 - acc: 0.8714 - val_loss: 0.3656 - val_acc: 0.8372\n",
            "Epoch 75/500\n",
            "20109/20109 [==============================] - 15s 759us/sample - loss: 0.3114 - acc: 0.8717 - val_loss: 0.3677 - val_acc: 0.8374\n",
            "Epoch 76/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.3132 - acc: 0.8724 - val_loss: 0.3645 - val_acc: 0.8374\n",
            "Epoch 77/500\n",
            "20109/20109 [==============================] - 16s 773us/sample - loss: 0.3086 - acc: 0.8719 - val_loss: 0.3660 - val_acc: 0.8386\n",
            "Epoch 78/500\n",
            "20109/20109 [==============================] - 16s 773us/sample - loss: 0.3101 - acc: 0.8716 - val_loss: 0.3657 - val_acc: 0.8370\n",
            "Epoch 79/500\n",
            "20109/20109 [==============================] - 16s 782us/sample - loss: 0.3068 - acc: 0.8740 - val_loss: 0.3648 - val_acc: 0.8388\n",
            "Epoch 80/500\n",
            "20109/20109 [==============================] - 15s 763us/sample - loss: 0.3050 - acc: 0.8743 - val_loss: 0.3661 - val_acc: 0.8376\n",
            "Epoch 81/500\n",
            "20109/20109 [==============================] - 15s 735us/sample - loss: 0.3049 - acc: 0.8737 - val_loss: 0.3654 - val_acc: 0.8374\n",
            "Epoch 82/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.2971 - acc: 0.8788 - val_loss: 0.3678 - val_acc: 0.8386\n",
            "Epoch 83/500\n",
            "20109/20109 [==============================] - 15s 759us/sample - loss: 0.3021 - acc: 0.8769 - val_loss: 0.3652 - val_acc: 0.8382\n",
            "Epoch 00083: early stopping\n",
            "--- 1406.7409403324127 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20109/20109 [==============================] - 264s 13ms/sample - loss: 0.2754 - acc: 0.8903\n",
            "5027/5027 [==============================] - 62s 12ms/sample - loss: 0.3673 - acc: 0.8403\n",
            "Train Accuracy: 0.8903\n",
            "Train Loss: 0.2754\n",
            "Validation Accuracy:  0.8403\n",
            "Validation Loss: 0.3673\n",
            "5027/5027 [==============================] - 19s 4ms/sample\n",
            "[[1061  438]\n",
            " [ 365 3163]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.71      0.73      1499\n",
            "         1.0       0.88      0.90      0.89      3528\n",
            "\n",
            "    accuracy                           0.84      5027\n",
            "   macro avg       0.81      0.80      0.81      5027\n",
            "weighted avg       0.84      0.84      0.84      5027\n",
            "\n",
            "RMSE: 0.3997\n",
            "--- Fold 3 ---\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.80.200.18:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 10941942503576000892)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14433294298198489494)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11267183220444994076)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6375897316121174547)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10811646719542814541)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 13074149627867749380)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6491719624428219428)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6773084868664945562)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 14971483707359683640)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6484311248139258291)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 8976632525022643677)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20109 samples, validate on 5027 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 15.00313401222229 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20109 [==========================>...] - ETA: 4s - loss: 0.6901 - acc: 0.6086INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 18.422724723815918 secs\n",
            "19456/20109 [============================>.] - ETA: 2s - loss: 0.6899 - acc: 0.6135INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 18.92940068244934 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 21.530871629714966 secs\n",
            "20109/20109 [==============================] - 152s 8ms/sample - loss: 0.6898 - acc: 0.6158 - val_loss: 0.6867 - val_acc: 0.7016\n",
            "Epoch 2/500\n",
            "20109/20109 [==============================] - 15s 754us/sample - loss: 0.6803 - acc: 0.6997 - val_loss: 0.6757 - val_acc: 0.7016\n",
            "Epoch 3/500\n",
            "20109/20109 [==============================] - 16s 773us/sample - loss: 0.6655 - acc: 0.7017 - val_loss: 0.6581 - val_acc: 0.7016\n",
            "Epoch 4/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.6433 - acc: 0.7017 - val_loss: 0.6334 - val_acc: 0.7016\n",
            "Epoch 5/500\n",
            "20109/20109 [==============================] - 16s 782us/sample - loss: 0.6188 - acc: 0.7017 - val_loss: 0.6195 - val_acc: 0.7016\n",
            "Epoch 6/500\n",
            "20109/20109 [==============================] - 16s 784us/sample - loss: 0.6144 - acc: 0.7017 - val_loss: 0.6142 - val_acc: 0.7016\n",
            "Epoch 7/500\n",
            "20109/20109 [==============================] - 15s 770us/sample - loss: 0.6136 - acc: 0.7017 - val_loss: 0.6115 - val_acc: 0.7016\n",
            "Epoch 8/500\n",
            "20109/20109 [==============================] - 16s 778us/sample - loss: 0.6121 - acc: 0.7017 - val_loss: 0.6095 - val_acc: 0.7016\n",
            "Epoch 9/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.6109 - acc: 0.7016 - val_loss: 0.6084 - val_acc: 0.7016\n",
            "Epoch 10/500\n",
            "20109/20109 [==============================] - 16s 777us/sample - loss: 0.6113 - acc: 0.7016 - val_loss: 0.6083 - val_acc: 0.7016\n",
            "Epoch 11/500\n",
            "20109/20109 [==============================] - 16s 780us/sample - loss: 0.6098 - acc: 0.7016 - val_loss: 0.6086 - val_acc: 0.7016\n",
            "Epoch 12/500\n",
            "20109/20109 [==============================] - 16s 780us/sample - loss: 0.6099 - acc: 0.7017 - val_loss: 0.6078 - val_acc: 0.7016\n",
            "Epoch 13/500\n",
            "20109/20109 [==============================] - 15s 753us/sample - loss: 0.6093 - acc: 0.7018 - val_loss: 0.6077 - val_acc: 0.7016\n",
            "Epoch 14/500\n",
            "20109/20109 [==============================] - 15s 763us/sample - loss: 0.6093 - acc: 0.7017 - val_loss: 0.6067 - val_acc: 0.7016\n",
            "Epoch 15/500\n",
            "20109/20109 [==============================] - 16s 775us/sample - loss: 0.6092 - acc: 0.7016 - val_loss: 0.6072 - val_acc: 0.7016\n",
            "Epoch 16/500\n",
            "20109/20109 [==============================] - 16s 776us/sample - loss: 0.6090 - acc: 0.7017 - val_loss: 0.6073 - val_acc: 0.7016\n",
            "Epoch 17/500\n",
            "20109/20109 [==============================] - 15s 753us/sample - loss: 0.6084 - acc: 0.7016 - val_loss: 0.6057 - val_acc: 0.7016\n",
            "Epoch 18/500\n",
            "20109/20109 [==============================] - 16s 774us/sample - loss: 0.6080 - acc: 0.7017 - val_loss: 0.6055 - val_acc: 0.7016\n",
            "Epoch 19/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.6066 - acc: 0.7018 - val_loss: 0.6058 - val_acc: 0.7016\n",
            "Epoch 20/500\n",
            "20109/20109 [==============================] - 16s 775us/sample - loss: 0.6066 - acc: 0.7016 - val_loss: 0.6053 - val_acc: 0.7016\n",
            "Epoch 21/500\n",
            "20109/20109 [==============================] - 16s 791us/sample - loss: 0.6054 - acc: 0.7017 - val_loss: 0.6044 - val_acc: 0.7016\n",
            "Epoch 22/500\n",
            "20109/20109 [==============================] - 16s 789us/sample - loss: 0.6044 - acc: 0.7017 - val_loss: 0.6033 - val_acc: 0.7016\n",
            "Epoch 23/500\n",
            "20109/20109 [==============================] - 16s 784us/sample - loss: 0.6025 - acc: 0.7016 - val_loss: 0.6012 - val_acc: 0.7016\n",
            "Epoch 24/500\n",
            "20109/20109 [==============================] - 15s 748us/sample - loss: 0.6013 - acc: 0.7017 - val_loss: 0.5992 - val_acc: 0.7016\n",
            "Epoch 25/500\n",
            "20109/20109 [==============================] - 15s 758us/sample - loss: 0.5986 - acc: 0.7016 - val_loss: 0.5967 - val_acc: 0.7016\n",
            "Epoch 26/500\n",
            "20109/20109 [==============================] - 16s 782us/sample - loss: 0.5959 - acc: 0.7016 - val_loss: 0.5932 - val_acc: 0.7016\n",
            "Epoch 27/500\n",
            "20109/20109 [==============================] - 15s 756us/sample - loss: 0.5931 - acc: 0.7018 - val_loss: 0.5893 - val_acc: 0.7016\n",
            "Epoch 28/500\n",
            "20109/20109 [==============================] - 16s 785us/sample - loss: 0.5880 - acc: 0.7018 - val_loss: 0.5843 - val_acc: 0.7016\n",
            "Epoch 29/500\n",
            "20109/20109 [==============================] - 16s 781us/sample - loss: 0.5815 - acc: 0.7017 - val_loss: 0.5766 - val_acc: 0.7016\n",
            "Epoch 30/500\n",
            "20109/20109 [==============================] - 15s 770us/sample - loss: 0.5712 - acc: 0.7019 - val_loss: 0.5654 - val_acc: 0.7022\n",
            "Epoch 31/500\n",
            "20109/20109 [==============================] - 15s 769us/sample - loss: 0.5597 - acc: 0.7039 - val_loss: 0.5504 - val_acc: 0.7078\n",
            "Epoch 32/500\n",
            "20109/20109 [==============================] - 15s 761us/sample - loss: 0.5414 - acc: 0.7170 - val_loss: 0.5288 - val_acc: 0.7269\n",
            "Epoch 33/500\n",
            "20109/20109 [==============================] - 15s 771us/sample - loss: 0.5154 - acc: 0.7446 - val_loss: 0.4974 - val_acc: 0.7548\n",
            "Epoch 34/500\n",
            "20109/20109 [==============================] - 16s 776us/sample - loss: 0.4846 - acc: 0.7738 - val_loss: 0.4620 - val_acc: 0.7812\n",
            "Epoch 35/500\n",
            "20109/20109 [==============================] - 15s 756us/sample - loss: 0.4577 - acc: 0.7887 - val_loss: 0.4415 - val_acc: 0.7954\n",
            "Epoch 36/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.4398 - acc: 0.7981 - val_loss: 0.4328 - val_acc: 0.7974\n",
            "Epoch 37/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.4341 - acc: 0.7992 - val_loss: 0.4275 - val_acc: 0.8006\n",
            "Epoch 38/500\n",
            "20109/20109 [==============================] - 16s 776us/sample - loss: 0.4283 - acc: 0.8031 - val_loss: 0.4255 - val_acc: 0.8020\n",
            "Epoch 39/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.4221 - acc: 0.8060 - val_loss: 0.4210 - val_acc: 0.8053\n",
            "Epoch 40/500\n",
            "20109/20109 [==============================] - 16s 772us/sample - loss: 0.4190 - acc: 0.8114 - val_loss: 0.4184 - val_acc: 0.8055\n",
            "Epoch 41/500\n",
            "20109/20109 [==============================] - 16s 772us/sample - loss: 0.4081 - acc: 0.8157 - val_loss: 0.4154 - val_acc: 0.8075\n",
            "Epoch 42/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.4043 - acc: 0.8177 - val_loss: 0.4114 - val_acc: 0.8103\n",
            "Epoch 43/500\n",
            "20109/20109 [==============================] - 15s 757us/sample - loss: 0.4028 - acc: 0.8174 - val_loss: 0.4085 - val_acc: 0.8133\n",
            "Epoch 44/500\n",
            "20109/20109 [==============================] - 16s 772us/sample - loss: 0.3949 - acc: 0.8257 - val_loss: 0.4078 - val_acc: 0.8129\n",
            "Epoch 45/500\n",
            "20109/20109 [==============================] - 16s 779us/sample - loss: 0.3897 - acc: 0.8248 - val_loss: 0.4049 - val_acc: 0.8135\n",
            "Epoch 46/500\n",
            "20109/20109 [==============================] - 15s 747us/sample - loss: 0.3856 - acc: 0.8293 - val_loss: 0.4025 - val_acc: 0.8153\n",
            "Epoch 47/500\n",
            "20109/20109 [==============================] - 15s 757us/sample - loss: 0.3855 - acc: 0.8287 - val_loss: 0.4011 - val_acc: 0.8165\n",
            "Epoch 48/500\n",
            "20109/20109 [==============================] - 16s 776us/sample - loss: 0.3792 - acc: 0.8320 - val_loss: 0.3978 - val_acc: 0.8191\n",
            "Epoch 49/500\n",
            "20109/20109 [==============================] - 16s 772us/sample - loss: 0.3761 - acc: 0.8342 - val_loss: 0.3960 - val_acc: 0.8195\n",
            "Epoch 50/500\n",
            "20109/20109 [==============================] - 15s 763us/sample - loss: 0.3714 - acc: 0.8387 - val_loss: 0.3938 - val_acc: 0.8207\n",
            "Epoch 51/500\n",
            "20109/20109 [==============================] - 16s 777us/sample - loss: 0.3687 - acc: 0.8396 - val_loss: 0.3929 - val_acc: 0.8217\n",
            "Epoch 52/500\n",
            "20109/20109 [==============================] - 15s 755us/sample - loss: 0.3626 - acc: 0.8422 - val_loss: 0.3917 - val_acc: 0.8242\n",
            "Epoch 53/500\n",
            "20109/20109 [==============================] - 16s 773us/sample - loss: 0.3627 - acc: 0.8415 - val_loss: 0.3892 - val_acc: 0.8256\n",
            "Epoch 54/500\n",
            "20109/20109 [==============================] - 16s 773us/sample - loss: 0.3596 - acc: 0.8431 - val_loss: 0.3879 - val_acc: 0.8260\n",
            "Epoch 55/500\n",
            "20109/20109 [==============================] - 16s 772us/sample - loss: 0.3573 - acc: 0.8429 - val_loss: 0.3870 - val_acc: 0.8268\n",
            "Epoch 56/500\n",
            "20109/20109 [==============================] - 16s 787us/sample - loss: 0.3546 - acc: 0.8444 - val_loss: 0.3868 - val_acc: 0.8278\n",
            "Epoch 57/500\n",
            "20109/20109 [==============================] - 16s 774us/sample - loss: 0.3496 - acc: 0.8479 - val_loss: 0.3846 - val_acc: 0.8274\n",
            "Epoch 58/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.3486 - acc: 0.8504 - val_loss: 0.3838 - val_acc: 0.8284\n",
            "Epoch 59/500\n",
            "20109/20109 [==============================] - 15s 767us/sample - loss: 0.3454 - acc: 0.8514 - val_loss: 0.3823 - val_acc: 0.8298\n",
            "Epoch 60/500\n",
            "20109/20109 [==============================] - 16s 772us/sample - loss: 0.3429 - acc: 0.8524 - val_loss: 0.3815 - val_acc: 0.8316\n",
            "Epoch 61/500\n",
            "20109/20109 [==============================] - 16s 772us/sample - loss: 0.3407 - acc: 0.8524 - val_loss: 0.3807 - val_acc: 0.8312\n",
            "Epoch 62/500\n",
            "20109/20109 [==============================] - 15s 769us/sample - loss: 0.3396 - acc: 0.8531 - val_loss: 0.3800 - val_acc: 0.8328\n",
            "Epoch 63/500\n",
            "20109/20109 [==============================] - 15s 767us/sample - loss: 0.3379 - acc: 0.8567 - val_loss: 0.3787 - val_acc: 0.8324\n",
            "Epoch 64/500\n",
            "20109/20109 [==============================] - 15s 756us/sample - loss: 0.3338 - acc: 0.8570 - val_loss: 0.3777 - val_acc: 0.8324\n",
            "Epoch 65/500\n",
            "20109/20109 [==============================] - 15s 760us/sample - loss: 0.3320 - acc: 0.8580 - val_loss: 0.3769 - val_acc: 0.8336\n",
            "Epoch 66/500\n",
            "20109/20109 [==============================] - 15s 764us/sample - loss: 0.3294 - acc: 0.8590 - val_loss: 0.3768 - val_acc: 0.8340\n",
            "Epoch 67/500\n",
            "20109/20109 [==============================] - 16s 772us/sample - loss: 0.3306 - acc: 0.8595 - val_loss: 0.3765 - val_acc: 0.8340\n",
            "Epoch 68/500\n",
            "20109/20109 [==============================] - 15s 767us/sample - loss: 0.3283 - acc: 0.8605 - val_loss: 0.3754 - val_acc: 0.8352\n",
            "Epoch 69/500\n",
            "20109/20109 [==============================] - 16s 783us/sample - loss: 0.3240 - acc: 0.8626 - val_loss: 0.3760 - val_acc: 0.8336\n",
            "Epoch 70/500\n",
            "20109/20109 [==============================] - 16s 772us/sample - loss: 0.3200 - acc: 0.8656 - val_loss: 0.3741 - val_acc: 0.8366\n",
            "Epoch 71/500\n",
            "20109/20109 [==============================] - 15s 767us/sample - loss: 0.3208 - acc: 0.8676 - val_loss: 0.3737 - val_acc: 0.8356\n",
            "Epoch 72/500\n",
            "20109/20109 [==============================] - 15s 763us/sample - loss: 0.3197 - acc: 0.8648 - val_loss: 0.3725 - val_acc: 0.8384\n",
            "Epoch 73/500\n",
            "20109/20109 [==============================] - 15s 757us/sample - loss: 0.3141 - acc: 0.8693 - val_loss: 0.3738 - val_acc: 0.8378\n",
            "Epoch 74/500\n",
            "20109/20109 [==============================] - 16s 781us/sample - loss: 0.3162 - acc: 0.8655 - val_loss: 0.3741 - val_acc: 0.8370\n",
            "Epoch 75/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.3133 - acc: 0.8698 - val_loss: 0.3733 - val_acc: 0.8374\n",
            "Epoch 76/500\n",
            "20109/20109 [==============================] - 16s 776us/sample - loss: 0.3141 - acc: 0.8685 - val_loss: 0.3721 - val_acc: 0.8378\n",
            "Epoch 77/500\n",
            "20109/20109 [==============================] - 15s 769us/sample - loss: 0.3120 - acc: 0.8706 - val_loss: 0.3719 - val_acc: 0.8380\n",
            "Epoch 78/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.3113 - acc: 0.8715 - val_loss: 0.3708 - val_acc: 0.8386\n",
            "Epoch 79/500\n",
            "20109/20109 [==============================] - 16s 789us/sample - loss: 0.3063 - acc: 0.8725 - val_loss: 0.3717 - val_acc: 0.8384\n",
            "Epoch 80/500\n",
            "20109/20109 [==============================] - 16s 777us/sample - loss: 0.3041 - acc: 0.8746 - val_loss: 0.3717 - val_acc: 0.8396\n",
            "Epoch 81/500\n",
            "20109/20109 [==============================] - 15s 763us/sample - loss: 0.3050 - acc: 0.8742 - val_loss: 0.3709 - val_acc: 0.8374\n",
            "Epoch 82/500\n",
            "20109/20109 [==============================] - 16s 776us/sample - loss: 0.3038 - acc: 0.8716 - val_loss: 0.3713 - val_acc: 0.8396\n",
            "Epoch 83/500\n",
            "20109/20109 [==============================] - 16s 796us/sample - loss: 0.2998 - acc: 0.8764 - val_loss: 0.3714 - val_acc: 0.8392\n",
            "Epoch 84/500\n",
            "20109/20109 [==============================] - 16s 775us/sample - loss: 0.3014 - acc: 0.8756 - val_loss: 0.3705 - val_acc: 0.8390\n",
            "Epoch 85/500\n",
            "20109/20109 [==============================] - 16s 783us/sample - loss: 0.3013 - acc: 0.8745 - val_loss: 0.3704 - val_acc: 0.8388\n",
            "Epoch 86/500\n",
            "20109/20109 [==============================] - 16s 771us/sample - loss: 0.2961 - acc: 0.8785 - val_loss: 0.3711 - val_acc: 0.8382\n",
            "Epoch 87/500\n",
            "20109/20109 [==============================] - 16s 774us/sample - loss: 0.2953 - acc: 0.8785 - val_loss: 0.3709 - val_acc: 0.8390\n",
            "Epoch 88/500\n",
            "20109/20109 [==============================] - 16s 778us/sample - loss: 0.2967 - acc: 0.8778 - val_loss: 0.3707 - val_acc: 0.8374\n",
            "Epoch 89/500\n",
            "20109/20109 [==============================] - 16s 781us/sample - loss: 0.2926 - acc: 0.8796 - val_loss: 0.3722 - val_acc: 0.8396\n",
            "Epoch 90/500\n",
            "20109/20109 [==============================] - 15s 760us/sample - loss: 0.2913 - acc: 0.8798 - val_loss: 0.3718 - val_acc: 0.8382\n",
            "Epoch 91/500\n",
            "20109/20109 [==============================] - 16s 787us/sample - loss: 0.2903 - acc: 0.8805 - val_loss: 0.3715 - val_acc: 0.8392\n",
            "Epoch 92/500\n",
            "20109/20109 [==============================] - 16s 788us/sample - loss: 0.2904 - acc: 0.8815 - val_loss: 0.3712 - val_acc: 0.8392\n",
            "Epoch 00092: early stopping\n",
            "--- 1565.518525838852 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20109/20109 [==============================] - 265s 13ms/sample - loss: 0.2646 - acc: 0.8939\n",
            "5027/5027 [==============================] - 64s 13ms/sample - loss: 0.3713 - acc: 0.8383\n",
            "Train Accuracy: 0.8939\n",
            "Train Loss: 0.2646\n",
            "Validation Accuracy:  0.8383\n",
            "Validation Loss: 0.3713\n",
            "5027/5027 [==============================] - 20s 4ms/sample\n",
            "[[1053  447]\n",
            " [ 366 3161]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.70      0.72      1500\n",
            "         1.0       0.88      0.90      0.89      3527\n",
            "\n",
            "    accuracy                           0.84      5027\n",
            "   macro avg       0.81      0.80      0.80      5027\n",
            "weighted avg       0.84      0.84      0.84      5027\n",
            "\n",
            "RMSE: 0.4022\n",
            "--- Fold 4 ---\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.80.200.18:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 10941942503576000892)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14433294298198489494)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11267183220444994076)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6375897316121174547)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10811646719542814541)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 13074149627867749380)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6491719624428219428)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6773084868664945562)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 14971483707359683640)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6484311248139258291)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 8976632525022643677)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20109 samples, validate on 5027 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 15.652349710464478 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20109 [==========================>...] - ETA: 4s - loss: 0.6899 - acc: 0.6305INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 19.429822206497192 secs\n",
            "19456/20109 [============================>.] - ETA: 2s - loss: 0.6896 - acc: 0.6337INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 20.01550531387329 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 23.22312355041504 secs\n",
            "20109/20109 [==============================] - 157s 8ms/sample - loss: 0.6895 - acc: 0.6363 - val_loss: 0.6845 - val_acc: 0.7014\n",
            "Epoch 2/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.6784 - acc: 0.7017 - val_loss: 0.6713 - val_acc: 0.7014\n",
            "Epoch 3/500\n",
            "20109/20109 [==============================] - 16s 771us/sample - loss: 0.6608 - acc: 0.7017 - val_loss: 0.6507 - val_acc: 0.7014\n",
            "Epoch 4/500\n",
            "20109/20109 [==============================] - 15s 769us/sample - loss: 0.6359 - acc: 0.7017 - val_loss: 0.6241 - val_acc: 0.7014\n",
            "Epoch 5/500\n",
            "20109/20109 [==============================] - 16s 778us/sample - loss: 0.6143 - acc: 0.7017 - val_loss: 0.6160 - val_acc: 0.7014\n",
            "Epoch 6/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.6121 - acc: 0.7017 - val_loss: 0.6151 - val_acc: 0.7014\n",
            "Epoch 7/500\n",
            "20109/20109 [==============================] - 16s 793us/sample - loss: 0.6126 - acc: 0.7017 - val_loss: 0.6150 - val_acc: 0.7014\n",
            "Epoch 8/500\n",
            "20109/20109 [==============================] - 16s 778us/sample - loss: 0.6115 - acc: 0.7017 - val_loss: 0.6138 - val_acc: 0.7014\n",
            "Epoch 9/500\n",
            "20109/20109 [==============================] - 16s 783us/sample - loss: 0.6110 - acc: 0.7017 - val_loss: 0.6132 - val_acc: 0.7014\n",
            "Epoch 10/500\n",
            "20109/20109 [==============================] - 15s 767us/sample - loss: 0.6106 - acc: 0.7018 - val_loss: 0.6126 - val_acc: 0.7014\n",
            "Epoch 11/500\n",
            "20109/20109 [==============================] - 16s 778us/sample - loss: 0.6102 - acc: 0.7017 - val_loss: 0.6119 - val_acc: 0.7014\n",
            "Epoch 12/500\n",
            "20109/20109 [==============================] - 15s 753us/sample - loss: 0.6095 - acc: 0.7017 - val_loss: 0.6112 - val_acc: 0.7014\n",
            "Epoch 13/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.6097 - acc: 0.7017 - val_loss: 0.6103 - val_acc: 0.7014\n",
            "Epoch 14/500\n",
            "20109/20109 [==============================] - 16s 780us/sample - loss: 0.6090 - acc: 0.7018 - val_loss: 0.6098 - val_acc: 0.7014\n",
            "Epoch 15/500\n",
            "20109/20109 [==============================] - 16s 782us/sample - loss: 0.6082 - acc: 0.7017 - val_loss: 0.6087 - val_acc: 0.7014\n",
            "Epoch 16/500\n",
            "20109/20109 [==============================] - 15s 764us/sample - loss: 0.6074 - acc: 0.7017 - val_loss: 0.6076 - val_acc: 0.7014\n",
            "Epoch 17/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.6059 - acc: 0.7017 - val_loss: 0.6067 - val_acc: 0.7014\n",
            "Epoch 18/500\n",
            "20109/20109 [==============================] - 16s 777us/sample - loss: 0.6056 - acc: 0.7017 - val_loss: 0.6058 - val_acc: 0.7014\n",
            "Epoch 19/500\n",
            "20109/20109 [==============================] - 16s 782us/sample - loss: 0.6045 - acc: 0.7017 - val_loss: 0.6043 - val_acc: 0.7014\n",
            "Epoch 20/500\n",
            "20109/20109 [==============================] - 16s 783us/sample - loss: 0.6024 - acc: 0.7017 - val_loss: 0.6020 - val_acc: 0.7014\n",
            "Epoch 21/500\n",
            "20109/20109 [==============================] - 16s 773us/sample - loss: 0.6001 - acc: 0.7016 - val_loss: 0.5993 - val_acc: 0.7014\n",
            "Epoch 22/500\n",
            "20109/20109 [==============================] - 16s 782us/sample - loss: 0.5973 - acc: 0.7016 - val_loss: 0.5950 - val_acc: 0.7014\n",
            "Epoch 23/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.5919 - acc: 0.7017 - val_loss: 0.5891 - val_acc: 0.7014\n",
            "Epoch 24/500\n",
            "20109/20109 [==============================] - 15s 758us/sample - loss: 0.5843 - acc: 0.7017 - val_loss: 0.5800 - val_acc: 0.7014\n",
            "Epoch 25/500\n",
            "20109/20109 [==============================] - 16s 775us/sample - loss: 0.5736 - acc: 0.7017 - val_loss: 0.5651 - val_acc: 0.7018\n",
            "Epoch 26/500\n",
            "20109/20109 [==============================] - 16s 785us/sample - loss: 0.5571 - acc: 0.7046 - val_loss: 0.5426 - val_acc: 0.7066\n",
            "Epoch 27/500\n",
            "20109/20109 [==============================] - 16s 774us/sample - loss: 0.5334 - acc: 0.7229 - val_loss: 0.5113 - val_acc: 0.7297\n",
            "Epoch 28/500\n",
            "20109/20109 [==============================] - 15s 770us/sample - loss: 0.5101 - acc: 0.7521 - val_loss: 0.4830 - val_acc: 0.7663\n",
            "Epoch 29/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.4862 - acc: 0.7703 - val_loss: 0.4616 - val_acc: 0.7757\n",
            "Epoch 30/500\n",
            "20109/20109 [==============================] - 16s 774us/sample - loss: 0.4702 - acc: 0.7817 - val_loss: 0.4487 - val_acc: 0.7888\n",
            "Epoch 31/500\n",
            "20109/20109 [==============================] - 15s 769us/sample - loss: 0.4545 - acc: 0.7909 - val_loss: 0.4355 - val_acc: 0.7960\n",
            "Epoch 32/500\n",
            "20109/20109 [==============================] - 15s 747us/sample - loss: 0.4405 - acc: 0.7965 - val_loss: 0.4266 - val_acc: 0.8020\n",
            "Epoch 33/500\n",
            "20109/20109 [==============================] - 16s 772us/sample - loss: 0.4274 - acc: 0.8043 - val_loss: 0.4204 - val_acc: 0.8081\n",
            "Epoch 34/500\n",
            "20109/20109 [==============================] - 15s 767us/sample - loss: 0.4213 - acc: 0.8073 - val_loss: 0.4149 - val_acc: 0.8129\n",
            "Epoch 35/500\n",
            "20109/20109 [==============================] - 15s 767us/sample - loss: 0.4147 - acc: 0.8127 - val_loss: 0.4125 - val_acc: 0.8127\n",
            "Epoch 36/500\n",
            "20109/20109 [==============================] - 16s 774us/sample - loss: 0.4112 - acc: 0.8129 - val_loss: 0.4119 - val_acc: 0.8115\n",
            "Epoch 37/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.4006 - acc: 0.8180 - val_loss: 0.4078 - val_acc: 0.8139\n",
            "Epoch 38/500\n",
            "20109/20109 [==============================] - 15s 759us/sample - loss: 0.3960 - acc: 0.8235 - val_loss: 0.4025 - val_acc: 0.8199\n",
            "Epoch 39/500\n",
            "20109/20109 [==============================] - 15s 771us/sample - loss: 0.3903 - acc: 0.8264 - val_loss: 0.4000 - val_acc: 0.8215\n",
            "Epoch 40/500\n",
            "20109/20109 [==============================] - 15s 760us/sample - loss: 0.3925 - acc: 0.8247 - val_loss: 0.3979 - val_acc: 0.8213\n",
            "Epoch 41/500\n",
            "20109/20109 [==============================] - 16s 791us/sample - loss: 0.3813 - acc: 0.8317 - val_loss: 0.3942 - val_acc: 0.8234\n",
            "Epoch 42/500\n",
            "20109/20109 [==============================] - 15s 770us/sample - loss: 0.3808 - acc: 0.8310 - val_loss: 0.3909 - val_acc: 0.8260\n",
            "Epoch 43/500\n",
            "20109/20109 [==============================] - 16s 781us/sample - loss: 0.3767 - acc: 0.8343 - val_loss: 0.3895 - val_acc: 0.8280\n",
            "Epoch 44/500\n",
            "20109/20109 [==============================] - 15s 746us/sample - loss: 0.3696 - acc: 0.8373 - val_loss: 0.3866 - val_acc: 0.8300\n",
            "Epoch 45/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.3675 - acc: 0.8362 - val_loss: 0.3846 - val_acc: 0.8304\n",
            "Epoch 46/500\n",
            "20109/20109 [==============================] - 15s 757us/sample - loss: 0.3652 - acc: 0.8384 - val_loss: 0.3834 - val_acc: 0.8328\n",
            "Epoch 47/500\n",
            "20109/20109 [==============================] - 16s 785us/sample - loss: 0.3618 - acc: 0.8424 - val_loss: 0.3824 - val_acc: 0.8314\n",
            "Epoch 48/500\n",
            "20109/20109 [==============================] - 16s 787us/sample - loss: 0.3597 - acc: 0.8432 - val_loss: 0.3809 - val_acc: 0.8332\n",
            "Epoch 49/500\n",
            "20109/20109 [==============================] - 16s 784us/sample - loss: 0.3523 - acc: 0.8485 - val_loss: 0.3785 - val_acc: 0.8358\n",
            "Epoch 50/500\n",
            "20109/20109 [==============================] - 16s 786us/sample - loss: 0.3512 - acc: 0.8484 - val_loss: 0.3774 - val_acc: 0.8364\n",
            "Epoch 51/500\n",
            "20109/20109 [==============================] - 15s 754us/sample - loss: 0.3487 - acc: 0.8492 - val_loss: 0.3783 - val_acc: 0.8340\n",
            "Epoch 52/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.3447 - acc: 0.8541 - val_loss: 0.3750 - val_acc: 0.8384\n",
            "Epoch 53/500\n",
            "20109/20109 [==============================] - 16s 774us/sample - loss: 0.3431 - acc: 0.8517 - val_loss: 0.3749 - val_acc: 0.8382\n",
            "Epoch 54/500\n",
            "20109/20109 [==============================] - 16s 778us/sample - loss: 0.3390 - acc: 0.8547 - val_loss: 0.3726 - val_acc: 0.8398\n",
            "Epoch 55/500\n",
            "20109/20109 [==============================] - 16s 786us/sample - loss: 0.3405 - acc: 0.8544 - val_loss: 0.3722 - val_acc: 0.8404\n",
            "Epoch 56/500\n",
            "20109/20109 [==============================] - 15s 769us/sample - loss: 0.3356 - acc: 0.8545 - val_loss: 0.3712 - val_acc: 0.8422\n",
            "Epoch 57/500\n",
            "20109/20109 [==============================] - 16s 778us/sample - loss: 0.3337 - acc: 0.8584 - val_loss: 0.3702 - val_acc: 0.8414\n",
            "Epoch 58/500\n",
            "20109/20109 [==============================] - 16s 776us/sample - loss: 0.3350 - acc: 0.8574 - val_loss: 0.3693 - val_acc: 0.8428\n",
            "Epoch 59/500\n",
            "20109/20109 [==============================] - 15s 764us/sample - loss: 0.3307 - acc: 0.8599 - val_loss: 0.3697 - val_acc: 0.8428\n",
            "Epoch 60/500\n",
            "20109/20109 [==============================] - 16s 778us/sample - loss: 0.3268 - acc: 0.8616 - val_loss: 0.3680 - val_acc: 0.8432\n",
            "Epoch 61/500\n",
            "20109/20109 [==============================] - 16s 778us/sample - loss: 0.3264 - acc: 0.8605 - val_loss: 0.3682 - val_acc: 0.8447\n",
            "Epoch 62/500\n",
            "20109/20109 [==============================] - 15s 758us/sample - loss: 0.3256 - acc: 0.8625 - val_loss: 0.3672 - val_acc: 0.8447\n",
            "Epoch 63/500\n",
            "20109/20109 [==============================] - 15s 759us/sample - loss: 0.3221 - acc: 0.8641 - val_loss: 0.3676 - val_acc: 0.8426\n",
            "Epoch 64/500\n",
            "20109/20109 [==============================] - 15s 763us/sample - loss: 0.3225 - acc: 0.8657 - val_loss: 0.3674 - val_acc: 0.8443\n",
            "Epoch 65/500\n",
            "20109/20109 [==============================] - 15s 748us/sample - loss: 0.3169 - acc: 0.8688 - val_loss: 0.3667 - val_acc: 0.8445\n",
            "Epoch 66/500\n",
            "20109/20109 [==============================] - 15s 767us/sample - loss: 0.3153 - acc: 0.8685 - val_loss: 0.3655 - val_acc: 0.8465\n",
            "Epoch 67/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.3112 - acc: 0.8688 - val_loss: 0.3662 - val_acc: 0.8461\n",
            "Epoch 68/500\n",
            "20109/20109 [==============================] - 16s 781us/sample - loss: 0.3110 - acc: 0.8710 - val_loss: 0.3644 - val_acc: 0.8459\n",
            "Epoch 69/500\n",
            "20109/20109 [==============================] - 16s 784us/sample - loss: 0.3082 - acc: 0.8715 - val_loss: 0.3661 - val_acc: 0.8457\n",
            "Epoch 70/500\n",
            "20109/20109 [==============================] - 16s 781us/sample - loss: 0.3105 - acc: 0.8705 - val_loss: 0.3655 - val_acc: 0.8453\n",
            "Epoch 71/500\n",
            "20109/20109 [==============================] - 15s 770us/sample - loss: 0.3111 - acc: 0.8718 - val_loss: 0.3645 - val_acc: 0.8461\n",
            "Epoch 72/500\n",
            "20109/20109 [==============================] - 16s 772us/sample - loss: 0.3072 - acc: 0.8734 - val_loss: 0.3654 - val_acc: 0.8463\n",
            "Epoch 73/500\n",
            "20109/20109 [==============================] - 16s 786us/sample - loss: 0.3059 - acc: 0.8742 - val_loss: 0.3653 - val_acc: 0.8447\n",
            "Epoch 74/500\n",
            "20109/20109 [==============================] - 16s 772us/sample - loss: 0.3017 - acc: 0.8765 - val_loss: 0.3646 - val_acc: 0.8461\n",
            "Epoch 75/500\n",
            "20109/20109 [==============================] - 16s 774us/sample - loss: 0.2988 - acc: 0.8775 - val_loss: 0.3641 - val_acc: 0.8465\n",
            "Epoch 76/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.2966 - acc: 0.8786 - val_loss: 0.3646 - val_acc: 0.8459\n",
            "Epoch 77/500\n",
            "20109/20109 [==============================] - 16s 783us/sample - loss: 0.2985 - acc: 0.8770 - val_loss: 0.3662 - val_acc: 0.8445\n",
            "Epoch 78/500\n",
            "20109/20109 [==============================] - 16s 783us/sample - loss: 0.2949 - acc: 0.8792 - val_loss: 0.3648 - val_acc: 0.8453\n",
            "Epoch 79/500\n",
            "20109/20109 [==============================] - 15s 750us/sample - loss: 0.2951 - acc: 0.8791 - val_loss: 0.3642 - val_acc: 0.8449\n",
            "Epoch 80/500\n",
            "20109/20109 [==============================] - 15s 770us/sample - loss: 0.2943 - acc: 0.8813 - val_loss: 0.3641 - val_acc: 0.8447\n",
            "Epoch 81/500\n",
            "20109/20109 [==============================] - 15s 764us/sample - loss: 0.2932 - acc: 0.8810 - val_loss: 0.3656 - val_acc: 0.8449\n",
            "Epoch 82/500\n",
            "20109/20109 [==============================] - 16s 777us/sample - loss: 0.2895 - acc: 0.8834 - val_loss: 0.3659 - val_acc: 0.8457\n",
            "Epoch 00082: early stopping\n",
            "--- 1415.7601544857025 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20109/20109 [==============================] - 261s 13ms/sample - loss: 0.2649 - acc: 0.8943\n",
            "5027/5027 [==============================] - 61s 12ms/sample - loss: 0.3645 - acc: 0.8472\n",
            "Train Accuracy: 0.8943\n",
            "Train Loss: 0.2649\n",
            "Validation Accuracy:  0.8472\n",
            "Validation Loss: 0.3645\n",
            "5027/5027 [==============================] - 20s 4ms/sample\n",
            "[[1069  431]\n",
            " [ 337 3190]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.71      0.74      1500\n",
            "         1.0       0.88      0.90      0.89      3527\n",
            "\n",
            "    accuracy                           0.85      5027\n",
            "   macro avg       0.82      0.81      0.81      5027\n",
            "weighted avg       0.84      0.85      0.85      5027\n",
            "\n",
            "RMSE: 0.3909\n",
            "--- Fold 5 ---\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.80.200.18:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 10941942503576000892)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14433294298198489494)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11267183220444994076)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6375897316121174547)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10811646719542814541)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 13074149627867749380)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6491719624428219428)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6773084868664945562)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 14971483707359683640)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6484311248139258291)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 8976632525022643677)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20109 samples, validate on 5027 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 16.11211657524109 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20109 [==========================>...] - ETA: 4s - loss: 0.6915 - acc: 0.5817INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 20.113715648651123 secs\n",
            "19456/20109 [============================>.] - ETA: 3s - loss: 0.6913 - acc: 0.5874INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 19.939454555511475 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 23.07594132423401 secs\n",
            "20109/20109 [==============================] - 161s 8ms/sample - loss: 0.6912 - acc: 0.5910 - val_loss: 0.6872 - val_acc: 0.7018\n",
            "Epoch 2/500\n",
            "20109/20109 [==============================] - 16s 779us/sample - loss: 0.6827 - acc: 0.7004 - val_loss: 0.6774 - val_acc: 0.7018\n",
            "Epoch 3/500\n",
            "20109/20109 [==============================] - 15s 764us/sample - loss: 0.6693 - acc: 0.7018 - val_loss: 0.6607 - val_acc: 0.7018\n",
            "Epoch 4/500\n",
            "20109/20109 [==============================] - 15s 769us/sample - loss: 0.6471 - acc: 0.7018 - val_loss: 0.6348 - val_acc: 0.7018\n",
            "Epoch 5/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.6219 - acc: 0.7017 - val_loss: 0.6190 - val_acc: 0.7018\n",
            "Epoch 6/500\n",
            "20109/20109 [==============================] - 16s 778us/sample - loss: 0.6156 - acc: 0.7017 - val_loss: 0.6163 - val_acc: 0.7018\n",
            "Epoch 7/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.6130 - acc: 0.7018 - val_loss: 0.6144 - val_acc: 0.7018\n",
            "Epoch 8/500\n",
            "20109/20109 [==============================] - 15s 758us/sample - loss: 0.6123 - acc: 0.7016 - val_loss: 0.6132 - val_acc: 0.7018\n",
            "Epoch 9/500\n",
            "20109/20109 [==============================] - 16s 780us/sample - loss: 0.6113 - acc: 0.7017 - val_loss: 0.6122 - val_acc: 0.7018\n",
            "Epoch 10/500\n",
            "20109/20109 [==============================] - 15s 745us/sample - loss: 0.6108 - acc: 0.7017 - val_loss: 0.6117 - val_acc: 0.7018\n",
            "Epoch 11/500\n",
            "20109/20109 [==============================] - 15s 756us/sample - loss: 0.6106 - acc: 0.7017 - val_loss: 0.6109 - val_acc: 0.7018\n",
            "Epoch 12/500\n",
            "20109/20109 [==============================] - 16s 787us/sample - loss: 0.6099 - acc: 0.7017 - val_loss: 0.6107 - val_acc: 0.7018\n",
            "Epoch 13/500\n",
            "20109/20109 [==============================] - 15s 771us/sample - loss: 0.6100 - acc: 0.7017 - val_loss: 0.6103 - val_acc: 0.7018\n",
            "Epoch 14/500\n",
            "20109/20109 [==============================] - 15s 771us/sample - loss: 0.6090 - acc: 0.7017 - val_loss: 0.6100 - val_acc: 0.7018\n",
            "Epoch 15/500\n",
            "20109/20109 [==============================] - 15s 755us/sample - loss: 0.6094 - acc: 0.7017 - val_loss: 0.6092 - val_acc: 0.7018\n",
            "Epoch 16/500\n",
            "20109/20109 [==============================] - 16s 783us/sample - loss: 0.6085 - acc: 0.7017 - val_loss: 0.6085 - val_acc: 0.7018\n",
            "Epoch 17/500\n",
            "20109/20109 [==============================] - 16s 776us/sample - loss: 0.6081 - acc: 0.7017 - val_loss: 0.6089 - val_acc: 0.7018\n",
            "Epoch 18/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.6081 - acc: 0.7016 - val_loss: 0.6074 - val_acc: 0.7018\n",
            "Epoch 19/500\n",
            "20109/20109 [==============================] - 15s 763us/sample - loss: 0.6077 - acc: 0.7017 - val_loss: 0.6069 - val_acc: 0.7018\n",
            "Epoch 20/500\n",
            "20109/20109 [==============================] - 15s 756us/sample - loss: 0.6062 - acc: 0.7017 - val_loss: 0.6064 - val_acc: 0.7018\n",
            "Epoch 21/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.6056 - acc: 0.7017 - val_loss: 0.6048 - val_acc: 0.7018\n",
            "Epoch 22/500\n",
            "20109/20109 [==============================] - 16s 779us/sample - loss: 0.6047 - acc: 0.7017 - val_loss: 0.6036 - val_acc: 0.7018\n",
            "Epoch 23/500\n",
            "20109/20109 [==============================] - 16s 778us/sample - loss: 0.6039 - acc: 0.7017 - val_loss: 0.6019 - val_acc: 0.7018\n",
            "Epoch 24/500\n",
            "20109/20109 [==============================] - 15s 769us/sample - loss: 0.6014 - acc: 0.7016 - val_loss: 0.5996 - val_acc: 0.7018\n",
            "Epoch 25/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.5984 - acc: 0.7017 - val_loss: 0.5961 - val_acc: 0.7018\n",
            "Epoch 26/500\n",
            "20109/20109 [==============================] - 16s 774us/sample - loss: 0.5949 - acc: 0.7018 - val_loss: 0.5923 - val_acc: 0.7018\n",
            "Epoch 27/500\n",
            "20109/20109 [==============================] - 15s 752us/sample - loss: 0.5899 - acc: 0.7017 - val_loss: 0.5860 - val_acc: 0.7018\n",
            "Epoch 28/500\n",
            "20109/20109 [==============================] - 16s 783us/sample - loss: 0.5827 - acc: 0.7016 - val_loss: 0.5776 - val_acc: 0.7018\n",
            "Epoch 29/500\n",
            "20109/20109 [==============================] - 16s 776us/sample - loss: 0.5724 - acc: 0.7019 - val_loss: 0.5648 - val_acc: 0.7020\n",
            "Epoch 30/500\n",
            "20109/20109 [==============================] - 16s 773us/sample - loss: 0.5574 - acc: 0.7058 - val_loss: 0.5439 - val_acc: 0.7092\n",
            "Epoch 31/500\n",
            "20109/20109 [==============================] - 15s 761us/sample - loss: 0.5332 - acc: 0.7212 - val_loss: 0.5124 - val_acc: 0.7317\n",
            "Epoch 32/500\n",
            "20109/20109 [==============================] - 16s 786us/sample - loss: 0.5043 - acc: 0.7529 - val_loss: 0.4761 - val_acc: 0.7655\n",
            "Epoch 33/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.4811 - acc: 0.7731 - val_loss: 0.4489 - val_acc: 0.7926\n",
            "Epoch 34/500\n",
            "20109/20109 [==============================] - 16s 783us/sample - loss: 0.4604 - acc: 0.7869 - val_loss: 0.4364 - val_acc: 0.7960\n",
            "Epoch 35/500\n",
            "20109/20109 [==============================] - 15s 763us/sample - loss: 0.4446 - acc: 0.7956 - val_loss: 0.4253 - val_acc: 0.8035\n",
            "Epoch 36/500\n",
            "20109/20109 [==============================] - 15s 761us/sample - loss: 0.4385 - acc: 0.8007 - val_loss: 0.4165 - val_acc: 0.8115\n",
            "Epoch 37/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.4244 - acc: 0.8083 - val_loss: 0.4094 - val_acc: 0.8145\n",
            "Epoch 38/500\n",
            "20109/20109 [==============================] - 15s 766us/sample - loss: 0.4193 - acc: 0.8116 - val_loss: 0.4063 - val_acc: 0.8165\n",
            "Epoch 39/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.4118 - acc: 0.8160 - val_loss: 0.4021 - val_acc: 0.8187\n",
            "Epoch 40/500\n",
            "20109/20109 [==============================] - 16s 782us/sample - loss: 0.4066 - acc: 0.8191 - val_loss: 0.4002 - val_acc: 0.8175\n",
            "Epoch 41/500\n",
            "20109/20109 [==============================] - 15s 767us/sample - loss: 0.4040 - acc: 0.8216 - val_loss: 0.3965 - val_acc: 0.8221\n",
            "Epoch 42/500\n",
            "20109/20109 [==============================] - 16s 782us/sample - loss: 0.3951 - acc: 0.8251 - val_loss: 0.3933 - val_acc: 0.8211\n",
            "Epoch 43/500\n",
            "20109/20109 [==============================] - 16s 773us/sample - loss: 0.3935 - acc: 0.8269 - val_loss: 0.3909 - val_acc: 0.8229\n",
            "Epoch 44/500\n",
            "20109/20109 [==============================] - 16s 777us/sample - loss: 0.3910 - acc: 0.8264 - val_loss: 0.3882 - val_acc: 0.8270\n",
            "Epoch 45/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.3838 - acc: 0.8297 - val_loss: 0.3858 - val_acc: 0.8270\n",
            "Epoch 46/500\n",
            "20109/20109 [==============================] - 16s 775us/sample - loss: 0.3844 - acc: 0.8295 - val_loss: 0.3851 - val_acc: 0.8264\n",
            "Epoch 47/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.3764 - acc: 0.8373 - val_loss: 0.3826 - val_acc: 0.8276\n",
            "Epoch 48/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.3745 - acc: 0.8373 - val_loss: 0.3805 - val_acc: 0.8296\n",
            "Epoch 49/500\n",
            "20109/20109 [==============================] - 15s 760us/sample - loss: 0.3687 - acc: 0.8413 - val_loss: 0.3779 - val_acc: 0.8328\n",
            "Epoch 50/500\n",
            "20109/20109 [==============================] - 16s 794us/sample - loss: 0.3693 - acc: 0.8392 - val_loss: 0.3767 - val_acc: 0.8344\n",
            "Epoch 51/500\n",
            "20109/20109 [==============================] - 16s 781us/sample - loss: 0.3634 - acc: 0.8424 - val_loss: 0.3745 - val_acc: 0.8346\n",
            "Epoch 52/500\n",
            "20109/20109 [==============================] - 16s 774us/sample - loss: 0.3595 - acc: 0.8479 - val_loss: 0.3738 - val_acc: 0.8332\n",
            "Epoch 53/500\n",
            "20109/20109 [==============================] - 16s 780us/sample - loss: 0.3597 - acc: 0.8492 - val_loss: 0.3718 - val_acc: 0.8364\n",
            "Epoch 54/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.3540 - acc: 0.8480 - val_loss: 0.3721 - val_acc: 0.8358\n",
            "Epoch 55/500\n",
            "20109/20109 [==============================] - 16s 777us/sample - loss: 0.3530 - acc: 0.8492 - val_loss: 0.3685 - val_acc: 0.8394\n",
            "Epoch 56/500\n",
            "20109/20109 [==============================] - 15s 769us/sample - loss: 0.3550 - acc: 0.8501 - val_loss: 0.3683 - val_acc: 0.8396\n",
            "Epoch 57/500\n",
            "20109/20109 [==============================] - 15s 760us/sample - loss: 0.3472 - acc: 0.8496 - val_loss: 0.3680 - val_acc: 0.8370\n",
            "Epoch 58/500\n",
            "20109/20109 [==============================] - 16s 777us/sample - loss: 0.3491 - acc: 0.8504 - val_loss: 0.3668 - val_acc: 0.8406\n",
            "Epoch 59/500\n",
            "20109/20109 [==============================] - 15s 759us/sample - loss: 0.3425 - acc: 0.8551 - val_loss: 0.3656 - val_acc: 0.8390\n",
            "Epoch 60/500\n",
            "20109/20109 [==============================] - 15s 760us/sample - loss: 0.3386 - acc: 0.8563 - val_loss: 0.3647 - val_acc: 0.8386\n",
            "Epoch 61/500\n",
            "20109/20109 [==============================] - 15s 759us/sample - loss: 0.3400 - acc: 0.8552 - val_loss: 0.3635 - val_acc: 0.8396\n",
            "Epoch 62/500\n",
            "20109/20109 [==============================] - 15s 753us/sample - loss: 0.3367 - acc: 0.8567 - val_loss: 0.3628 - val_acc: 0.8416\n",
            "Epoch 63/500\n",
            "20109/20109 [==============================] - 16s 792us/sample - loss: 0.3325 - acc: 0.8600 - val_loss: 0.3619 - val_acc: 0.8424\n",
            "Epoch 64/500\n",
            "20109/20109 [==============================] - 16s 786us/sample - loss: 0.3322 - acc: 0.8614 - val_loss: 0.3614 - val_acc: 0.8428\n",
            "Epoch 65/500\n",
            "20109/20109 [==============================] - 15s 764us/sample - loss: 0.3306 - acc: 0.8602 - val_loss: 0.3604 - val_acc: 0.8402\n",
            "Epoch 66/500\n",
            "20109/20109 [==============================] - 16s 775us/sample - loss: 0.3281 - acc: 0.8626 - val_loss: 0.3603 - val_acc: 0.8408\n",
            "Epoch 67/500\n",
            "20109/20109 [==============================] - 16s 789us/sample - loss: 0.3258 - acc: 0.8662 - val_loss: 0.3595 - val_acc: 0.8426\n",
            "Epoch 68/500\n",
            "20109/20109 [==============================] - 16s 786us/sample - loss: 0.3270 - acc: 0.8628 - val_loss: 0.3593 - val_acc: 0.8434\n",
            "Epoch 69/500\n",
            "20109/20109 [==============================] - 16s 781us/sample - loss: 0.3220 - acc: 0.8670 - val_loss: 0.3586 - val_acc: 0.8432\n",
            "Epoch 70/500\n",
            "20109/20109 [==============================] - 16s 792us/sample - loss: 0.3207 - acc: 0.8663 - val_loss: 0.3588 - val_acc: 0.8443\n",
            "Epoch 71/500\n",
            "20109/20109 [==============================] - 16s 782us/sample - loss: 0.3184 - acc: 0.8695 - val_loss: 0.3585 - val_acc: 0.8469\n",
            "Epoch 72/500\n",
            "20109/20109 [==============================] - 16s 782us/sample - loss: 0.3187 - acc: 0.8671 - val_loss: 0.3586 - val_acc: 0.8443\n",
            "Epoch 73/500\n",
            "20109/20109 [==============================] - 15s 768us/sample - loss: 0.3159 - acc: 0.8692 - val_loss: 0.3571 - val_acc: 0.8445\n",
            "Epoch 74/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.3152 - acc: 0.8699 - val_loss: 0.3578 - val_acc: 0.8447\n",
            "Epoch 75/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.3152 - acc: 0.8688 - val_loss: 0.3575 - val_acc: 0.8453\n",
            "Epoch 76/500\n",
            "20109/20109 [==============================] - 15s 763us/sample - loss: 0.3113 - acc: 0.8720 - val_loss: 0.3574 - val_acc: 0.8465\n",
            "Epoch 77/500\n",
            "20109/20109 [==============================] - 16s 784us/sample - loss: 0.3103 - acc: 0.8726 - val_loss: 0.3557 - val_acc: 0.8424\n",
            "Epoch 78/500\n",
            "20109/20109 [==============================] - 15s 760us/sample - loss: 0.3118 - acc: 0.8724 - val_loss: 0.3554 - val_acc: 0.8445\n",
            "Epoch 79/500\n",
            "20109/20109 [==============================] - 16s 786us/sample - loss: 0.3072 - acc: 0.8754 - val_loss: 0.3566 - val_acc: 0.8449\n",
            "Epoch 80/500\n",
            "20109/20109 [==============================] - 16s 778us/sample - loss: 0.3052 - acc: 0.8775 - val_loss: 0.3555 - val_acc: 0.8451\n",
            "Epoch 81/500\n",
            "20109/20109 [==============================] - 15s 746us/sample - loss: 0.3064 - acc: 0.8741 - val_loss: 0.3568 - val_acc: 0.8449\n",
            "Epoch 82/500\n",
            "20109/20109 [==============================] - 15s 757us/sample - loss: 0.3043 - acc: 0.8764 - val_loss: 0.3553 - val_acc: 0.8459\n",
            "Epoch 83/500\n",
            "20109/20109 [==============================] - 15s 770us/sample - loss: 0.3011 - acc: 0.8765 - val_loss: 0.3565 - val_acc: 0.8451\n",
            "Epoch 84/500\n",
            "20109/20109 [==============================] - 15s 763us/sample - loss: 0.3008 - acc: 0.8787 - val_loss: 0.3560 - val_acc: 0.8447\n",
            "Epoch 85/500\n",
            "20109/20109 [==============================] - 16s 777us/sample - loss: 0.2985 - acc: 0.8808 - val_loss: 0.3553 - val_acc: 0.8459\n",
            "Epoch 86/500\n",
            "20109/20109 [==============================] - 16s 783us/sample - loss: 0.2996 - acc: 0.8774 - val_loss: 0.3551 - val_acc: 0.8465\n",
            "Epoch 87/500\n",
            "20109/20109 [==============================] - 16s 788us/sample - loss: 0.2976 - acc: 0.8767 - val_loss: 0.3558 - val_acc: 0.8463\n",
            "Epoch 88/500\n",
            "20109/20109 [==============================] - 16s 782us/sample - loss: 0.2934 - acc: 0.8822 - val_loss: 0.3551 - val_acc: 0.8441\n",
            "Epoch 89/500\n",
            "20109/20109 [==============================] - 15s 760us/sample - loss: 0.2940 - acc: 0.8788 - val_loss: 0.3564 - val_acc: 0.8455\n",
            "Epoch 90/500\n",
            "20109/20109 [==============================] - 15s 762us/sample - loss: 0.2897 - acc: 0.8831 - val_loss: 0.3576 - val_acc: 0.8469\n",
            "Epoch 91/500\n",
            "20109/20109 [==============================] - 15s 765us/sample - loss: 0.2904 - acc: 0.8807 - val_loss: 0.3564 - val_acc: 0.8463\n",
            "Epoch 92/500\n",
            "20109/20109 [==============================] - 15s 747us/sample - loss: 0.2908 - acc: 0.8824 - val_loss: 0.3567 - val_acc: 0.8461\n",
            "Epoch 93/500\n",
            "20109/20109 [==============================] - 16s 784us/sample - loss: 0.2905 - acc: 0.8828 - val_loss: 0.3553 - val_acc: 0.8445\n",
            "Epoch 94/500\n",
            "20109/20109 [==============================] - 15s 764us/sample - loss: 0.2885 - acc: 0.8831 - val_loss: 0.3570 - val_acc: 0.8475\n",
            "Epoch 95/500\n",
            "20109/20109 [==============================] - 15s 767us/sample - loss: 0.2893 - acc: 0.8810 - val_loss: 0.3558 - val_acc: 0.8467\n",
            "Epoch 00095: early stopping\n",
            "--- 1619.9456524848938 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20109/20109 [==============================] - 262s 13ms/sample - loss: 0.2654 - acc: 0.8959\n",
            "5027/5027 [==============================] - 61s 12ms/sample - loss: 0.3595 - acc: 0.8488\n",
            "Train Accuracy: 0.8959\n",
            "Train Loss: 0.2654\n",
            "Validation Accuracy:  0.8488\n",
            "Validation Loss: 0.3595\n",
            "5027/5027 [==============================] - 20s 4ms/sample\n",
            "[[1065  435]\n",
            " [ 325 3202]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.71      0.74      1500\n",
            "         1.0       0.88      0.91      0.89      3527\n",
            "\n",
            "    accuracy                           0.85      5027\n",
            "   macro avg       0.82      0.81      0.82      5027\n",
            "weighted avg       0.85      0.85      0.85      5027\n",
            "\n",
            "RMSE: 0.3888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhZW6ObDXI8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "04261dbb-757a-444c-b057-c4564122d85e"
      },
      "source": [
        "print(\"Average Train Accuracy: %.4f%% (+/- %.4f%%)\" % (np.mean(tr_acc_array), np.std(tr_acc_array)))\n",
        "print(\"Average Train Loss: %.4f%% (+/- %.4f%%)\" % (np.mean(tr_loss_array), np.std(tr_loss_array)))\n",
        "print(\"Average Validation Accuracy: %.4f%% (+/- %.4f%%)\" % (np.mean(te_acc_array), np.std(te_acc_array)))\n",
        "print(\"Average Validation Loss: %.4f%% (+/- %.4f%%)\" % (np.mean(te_loss_array), np.std(te_loss_array)))\n",
        "print(\"Average Time: %.4f%% (+/- %.4f%%)\" % (np.mean(time_array), np.std(time_array)))\n",
        "print(\"Average RMSE: %.4f%% (+/- %.4f%%)\" % (np.mean(rmse_array), np.std(rmse_array)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Train Accuracy: 0.8939% (+/- 0.0019%)\n",
            "Average Train Loss: 0.2678% (+/- 0.0041%)\n",
            "Average Validation Accuracy: 0.8449% (+/- 0.0047%)\n",
            "Average Validation Loss: 0.3648% (+/- 0.0043%)\n",
            "Average Time: 1488.6657% (+/- 87.1883%)\n",
            "Average RMSE: 0.3938% (+/- 0.0060%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaXCaRhZ7Otf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.DataFrame({'Train Acc': tr_acc_array, 'Train Loss': tr_loss_array, 'Validation Acc': te_acc_array, 'Validation Loss': te_loss_array, 'Time': time_array, 'RMSE': rmse_array})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLPYXJcrSf6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a9076859-a4e1-4d52-861b-a545d8789e32"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train Acc</th>\n",
              "      <th>Train Loss</th>\n",
              "      <th>Validation Acc</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Time</th>\n",
              "      <th>RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.894868</td>\n",
              "      <td>0.268549</td>\n",
              "      <td>0.849841</td>\n",
              "      <td>0.361194</td>\n",
              "      <td>1435.362403</td>\n",
              "      <td>0.387504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.890298</td>\n",
              "      <td>0.275385</td>\n",
              "      <td>0.840263</td>\n",
              "      <td>0.367320</td>\n",
              "      <td>1406.741030</td>\n",
              "      <td>0.399672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.893878</td>\n",
              "      <td>0.264584</td>\n",
              "      <td>0.838273</td>\n",
              "      <td>0.371320</td>\n",
              "      <td>1565.518614</td>\n",
              "      <td>0.402153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.894326</td>\n",
              "      <td>0.264936</td>\n",
              "      <td>0.847225</td>\n",
              "      <td>0.364545</td>\n",
              "      <td>1415.760375</td>\n",
              "      <td>0.390864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.895917</td>\n",
              "      <td>0.265364</td>\n",
              "      <td>0.848816</td>\n",
              "      <td>0.359459</td>\n",
              "      <td>1619.946177</td>\n",
              "      <td>0.388823</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Train Acc  Train Loss  ...         Time      RMSE\n",
              "0   0.894868    0.268549  ...  1435.362403  0.387504\n",
              "1   0.890298    0.275385  ...  1406.741030  0.399672\n",
              "2   0.893878    0.264584  ...  1565.518614  0.402153\n",
              "3   0.894326    0.264936  ...  1415.760375  0.390864\n",
              "4   0.895917    0.265364  ...  1619.946177  0.388823\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3MQQcmILVxv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import time\n",
        "# from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "# start_time = time.time()\n",
        "\n",
        "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\n",
        "\n",
        "  \n",
        "# history = tpu_model.fit(x_train, y_train\n",
        "#                     ,epochs=epochs, verbose=1 \n",
        "#                     ,validation_split=0.2\n",
        "#                     ,batch_size=128 * 8\n",
        "#                     ,validation_data=(x_test, y_test)\n",
        "#                     ,callbacks=[es]\n",
        "#                    )\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# # tpu_model.save_weights('./tpu_model.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB09B2pkVxv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss, accuracy = tpu_model.evaluate(x_train, y_train, verbose=1)\n",
        "# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "# loss, accuracy = tpu_model.evaluate(x_test, y_test, verbose=1)\n",
        "# print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSoKam1eVxwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "ed6a3cb6-1ce0-492e-b9eb-aafec3326c98"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAFACAYAAAD9D55TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERERERixH8B1sSR22WVnTIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKMA0luELvtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_df = pd.DataFrame(list(zip(df['review'].values, df['cleaned'].values, df['sentiment'].values, y_pred)), columns = ['review','cleaned','sentiment','predict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsz-0FbISkWk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "d32179d1-fcfa-42e7-8a45-9cdcfa1d8ec6"
      },
      "source": [
        "check_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>cleaned</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This movie is the beginning of the culmination...</td>\n",
              "      <td>beginning culmination masterfully woven cinema...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Over the past decade, Marvel has earned itself...</td>\n",
              "      <td>past decade earned benefit doubt consistently ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This film is way better than endgame!\\nThe act...</td>\n",
              "      <td>way better action better writing better dialog...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Summer movies often hype themselves as spectac...</td>\n",
              "      <td>summer often hype spectacular event missed ad ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I was amazed to see so many negative reviews; ...</td>\n",
              "      <td>amazed negative impossible please hour long co...</td>\n",
              "      <td>1</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5022</th>\n",
              "      <td>Admittingly, I was not a fan of the original. ...</td>\n",
              "      <td>admittingly fan original found first eye candi...</td>\n",
              "      <td>0</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5023</th>\n",
              "      <td>Like the first round, it appears it is always ...</td>\n",
              "      <td>like first round appears always smart keep ori...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5024</th>\n",
              "      <td>With the exception of 'Captain America: The Wi...</td>\n",
              "      <td>exception america winter soldier cinematic uni...</td>\n",
              "      <td>1</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5025</th>\n",
              "      <td>I loved it! It was funny and witty and had all...</td>\n",
              "      <td>loved funny witty ingredient make best film fa...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5026</th>\n",
              "      <td>There are a few minor spoilers. I will try to ...</td>\n",
              "      <td>minor spoiler try got first weekend got fun sp...</td>\n",
              "      <td>1</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5027 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 review  ...  predict\n",
              "0     This movie is the beginning of the culmination...  ...   [True]\n",
              "1     Over the past decade, Marvel has earned itself...  ...   [True]\n",
              "2     This film is way better than endgame!\\nThe act...  ...   [True]\n",
              "3     Summer movies often hype themselves as spectac...  ...   [True]\n",
              "4     I was amazed to see so many negative reviews; ...  ...  [False]\n",
              "...                                                 ...  ...      ...\n",
              "5022  Admittingly, I was not a fan of the original. ...  ...  [False]\n",
              "5023  Like the first round, it appears it is always ...  ...   [True]\n",
              "5024  With the exception of 'Captain America: The Wi...  ...  [False]\n",
              "5025  I loved it! It was funny and witty and had all...  ...   [True]\n",
              "5026  There are a few minor spoilers. I will try to ...  ...  [False]\n",
              "\n",
              "[5027 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyKv2uqfvrEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_df.to_csv('check_dblstm.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvFNRqJf2_ph",
        "colab_type": "text"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE6l-_ye928X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nan = pd.read_csv(\"nan.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46sUeQBIGzCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "97830011-fd2b-447b-ca06-62bd6d885ec1"
      },
      "source": [
        "nan.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>rating</th>\n",
              "      <th>title</th>\n",
              "      <th>review</th>\n",
              "      <th>number</th>\n",
              "      <th>cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>69</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A tedious and dependent film</td>\n",
              "      <td>SPOILER: The plot is simple that the supervill...</td>\n",
              "      <td>0</td>\n",
              "      <td>spoiler simple supervillian succeeds russo bro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>126</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Thanos the Emo Crybaby...Thanos Mickey Mouse S...</td>\n",
              "      <td>12 jokes within 3 minutes in conversation betw...</td>\n",
              "      <td>0</td>\n",
              "      <td>joke within minute conversation doctor strange...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>139</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A snore -- literally</td>\n",
              "      <td>This film was complete and utter tosh. Judging...</td>\n",
              "      <td>0</td>\n",
              "      <td>complete utter tosh judging way positively rev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>146</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Thanos and Bubbles, Kiddie Mickey Mouse Style</td>\n",
              "      <td>I don't recognize the THANOS The MAD TITAN fro...</td>\n",
              "      <td>0</td>\n",
              "      <td>recognize thanos mad titan comic book thanos w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>176</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Good movie, but completely ruined</td>\n",
              "      <td>Spoilers!!!Spoilers: the movie was pretty good...</td>\n",
              "      <td>0</td>\n",
              "      <td>spoiler spoiler pretty good bit quippy good en...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                            cleaned\n",
              "0          69  ...  spoiler simple supervillian succeeds russo bro...\n",
              "1         126  ...  joke within minute conversation doctor strange...\n",
              "2         139  ...  complete utter tosh judging way positively rev...\n",
              "3         146  ...  recognize thanos mad titan comic book thanos w...\n",
              "4         176  ...  spoiler spoiler pretty good bit quippy good en...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqUZKx2pHjb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f88fd25-87fc-494c-a8f3-7f05ceb0cd47"
      },
      "source": [
        "nan.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(533, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZmDfVllGvgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sent in nan:\n",
        "    review = nan['review'].values\n",
        "    cleaned = nan['cleaned'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQDuXnbdG_bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_str = cleaned.astype(str)\n",
        "\n",
        "tokenizer = Tokenizer(char_level=False)\n",
        "tokenizer.fit_on_texts(cleaned_str)\n",
        "cleaned_bow = tokenizer.texts_to_sequences(cleaned_str) #text_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5sbZADIHAYc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1004c1e7-e182-41e8-cff2-f0bf00b3777d"
      },
      "source": [
        "x_pred = pad_sequences(cleaned_bow, maxlen=max_len)\n",
        "print('Shape of data tensor:', x_pred.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (533, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tBYTrU1Z3Gee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe6e1eef-4379-44a7-ce75-77518518e5f8"
      },
      "source": [
        "pred = model.predict(x_pred, batch_size=100, verbose = 1)\n",
        "y_pred = (pred > 0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "533/533 [==============================] - 3s 6ms/sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4C7LWibHHRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df = pd.DataFrame(list(zip(review, y_pred)), columns = ['review','predict'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prz4vzbfHxgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "b513e2fe-9375-409a-f60e-a333d1326501"
      },
      "source": [
        "pred_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SPOILER: The plot is simple that the supervill...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12 jokes within 3 minutes in conversation betw...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This film was complete and utter tosh. Judging...</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I don't recognize the THANOS The MAD TITAN fro...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spoilers!!!Spoilers: the movie was pretty good...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>528</th>\n",
              "      <td>Just finished watching this movie. My husband ...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>529</th>\n",
              "      <td>Ok at best. Big script let down from what's su...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530</th>\n",
              "      <td>Had this movie come out in 2009 close to \"Iron...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>531</th>\n",
              "      <td>I'm a middle-aged white male. Saw Captain Marv...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>PROS:Brie Larson's Is Great As Captain MarvelS...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>533 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                review  predict\n",
              "0    SPOILER: The plot is simple that the supervill...   [True]\n",
              "1    12 jokes within 3 minutes in conversation betw...   [True]\n",
              "2    This film was complete and utter tosh. Judging...  [False]\n",
              "3    I don't recognize the THANOS The MAD TITAN fro...   [True]\n",
              "4    Spoilers!!!Spoilers: the movie was pretty good...   [True]\n",
              "..                                                 ...      ...\n",
              "528  Just finished watching this movie. My husband ...   [True]\n",
              "529  Ok at best. Big script let down from what's su...   [True]\n",
              "530  Had this movie come out in 2009 close to \"Iron...   [True]\n",
              "531  I'm a middle-aged white male. Saw Captain Marv...   [True]\n",
              "532  PROS:Brie Larson's Is Great As Captain MarvelS...   [True]\n",
              "\n",
              "[533 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzteJqDNHyPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df.to_csv('pred_dblstm.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3IJpIgSvefv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "84d843f6-be49-4c4e-e8a7-3f8518efcdd2"
      },
      "source": [
        "model.save(\"dblstm.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMw2C2U3voMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}