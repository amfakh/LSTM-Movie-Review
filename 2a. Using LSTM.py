{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "01. LSTM v4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amfakh/LSTM-Movie-Review/blob/master/Using%20LSTM\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0RAuwjiBIAv",
        "colab_type": "text"
      },
      "source": [
        "## Force Tensorflow version 1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcQfl2Xw3wj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "89f120e3-f315-4397-ff78-599f3ec09cf4"
      },
      "source": [
        "!pip install tensorflow==1.13.1\n",
        "!pip install -qq -U cufflinks\n",
        "!pip install matplotlib==3.1.0\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.17.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (42.0.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (3.0.5)\n",
            "Requirement already satisfied: matplotlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (2.4.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib==3.1.0) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.1.0) (42.0.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUdDGKLNhH7m",
        "colab_type": "text"
      },
      "source": [
        "## Access Google Drive files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "danJ2_x2WArZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48bb846d-bff9-46d8-bb43-e32b203b28f5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FebLd3UKY6bp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0481b276-d557-48ad-ad16-261194e340b2"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2TTdQzHVxtY",
        "colab_type": "text"
      },
      "source": [
        "# Read all the Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8R-ttCYh25P",
        "colab_type": "text"
      },
      "source": [
        "## Read the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B5dzs6P8BFTi",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKGJfAyLkaH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"df.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_SWTGHmjLA9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2a93e47f-8f52-4d21-fda0-ab10e25d6f82"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>rating</th>\n",
              "      <th>title</th>\n",
              "      <th>review</th>\n",
              "      <th>number</th>\n",
              "      <th>cleaned</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>10/10</td>\n",
              "      <td>Unlike anything ever done in the history of ci...</td>\n",
              "      <td>This movie is the beginning of the culmination...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>beginning culmination masterfully woven cinema...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10/10</td>\n",
              "      <td>This movie will blow your mind and break your ...</td>\n",
              "      <td>Over the past decade, Marvel has earned itself...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>past decade earned benefit doubt consistently ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10/10</td>\n",
              "      <td>Way better than endgame</td>\n",
              "      <td>This film is way better than endgame!\\nThe act...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>way better action better writing better dialog...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>10/10</td>\n",
              "      <td>A Summer Film That IS Even Better Than The Hype</td>\n",
              "      <td>Summer movies often hype themselves as spectac...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>summer often hype spectacular event missed ad ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9/10</td>\n",
              "      <td>Excellent Film</td>\n",
              "      <td>I was amazed to see so many negative reviews; ...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>amazed negative impossible please hour long co...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... sentiment\n",
              "0           0  ...         1\n",
              "1           1  ...         1\n",
              "2           2  ...         1\n",
              "3           3  ...         1\n",
              "4           4  ...         1\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfVnOiW7Vxuw",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDWlhG7riIej",
        "colab_type": "text"
      },
      "source": [
        "## Data management"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKZ_qtURVxuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# for sent in df:\n",
        "train_review = df['cleaned'].values\n",
        "y_train = df['sentiment'].values\n",
        "\n",
        "    # train_review, test_review, y_train, y_test = train_test_split(\n",
        "    #     review, y, test_size=0.3, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_V1qN1DlR6C",
        "colab_type": "text"
      },
      "source": [
        "## Convert text to Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnxXFsJOj75b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a5d8fc7-d7ed-4919-8baa-2a4bcae8a339"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Make sure the data type is str\n",
        "review_train = train_review.astype(str)\n",
        "# review_test = test_review.astype(str)\n",
        "\n",
        "tokenizer = Tokenizer(char_level=False)\n",
        "tokenizer.fit_on_texts(review_train)\n",
        "text_train = tokenizer.texts_to_sequences(review_train)\n",
        "# text_test = tokenizer.texts_to_sequences(review_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RKaFdrumRU8",
        "colab_type": "text"
      },
      "source": [
        "### (Additional) Declare the vocab size to maximum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeDzKUe2mPnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07459fd0-5b60-49e3-f7bc-a106ad0cdea8"
      },
      "source": [
        "max_words = len(tokenizer.word_index) + 1  \n",
        "# Adding 1 because of reserved 0 index\n",
        "print('%s unique words.' % max_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37657 unique words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqSYqlz9ma2J",
        "colab_type": "text"
      },
      "source": [
        "### (Additional) Declare maximum length to maximum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdm00yw7Vxvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "feabd409-5d4b-4b4e-98d1-2d8a303fedc9"
      },
      "source": [
        "text_len = [len(r) for r in text_train]\n",
        "print(\"average length: %0.1f\" % np.mean(text_len))\n",
        "print(\"max length: %d\" % max(text_len))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average length: 75.4\n",
            "max length: 899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7ud35jqtATi",
        "colab_type": "text"
      },
      "source": [
        "## Declare max_len"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUIFIA_KVxvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of words to consider as features\n",
        "# max_words = 50000\n",
        "# Cut texts after this number of words (among top max_features most common words)\n",
        "max_len = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhpgG68KmjPc",
        "colab_type": "text"
      },
      "source": [
        "## Pad Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4exEzKqvVxvq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2bfb07d5-9014-4d32-f3f8-eaf9d2b08f51"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# max_length = max(text_len)\n",
        "\n",
        "# pad sequences with 0s\n",
        "x = pad_sequences(text_train, maxlen=max_len)\n",
        "# x_test = pad_sequences(text_test, maxlen=max_len)\n",
        "print('Shape of data tensor:', x.shape)\n",
        "# print('Shape of data test tensor:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (25136, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAAG8Xuint1U",
        "colab_type": "text"
      },
      "source": [
        "### Check the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gTZu3xAVxvs",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# print(review_train[4])\n",
        "# print(x_train[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avB7woI-nYFq",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# print(y_train[0])\n",
        "# print(y_test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJzDy6xEn-FK",
        "colab_type": "text"
      },
      "source": [
        "Convert label to float"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN4W32bErzB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
        "# y_test = np.asarray(y_test).astype('float32').reshape((-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aYup7IgoCah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d74b6de4-4ce1-4b0f-8e9f-a6c933457afb"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g70IGUjUBxxb",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAOsnygio29e",
        "colab_type": "text"
      },
      "source": [
        "## Import stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "906eUoh_o1zm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "60d04f2c-e6ce-44c2-e1ad-6a36523e57ca"
      },
      "source": [
        "from tensorflow.python.keras.layers import Dropout, Dense, Embedding\n",
        "from tensorflow.python.keras.layers import SpatialDropout1D, GlobalMaxPool1D\n",
        "from tensorflow.python.keras.layers import LSTM, Input, Bidirectional, GRU\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n",
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF9_O4lIVxvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = x.shape[-1]  # Number of features\n",
        "embedding_dim = 128\n",
        "learning_rate = 2e-5\n",
        "epochs = 500\n",
        "decay_rate = 1e-3 # learning_rate / epochs\n",
        "\n",
        "def make_model(batch_size=None):\n",
        "  source = Input(shape=(max_len,), name='Input')\n",
        "  embedding = Embedding(input_dim=max_words,\n",
        "                        output_dim=embedding_dim,\n",
        "                        input_length=max_len)(source)\n",
        "  # spatial = SpatialDropout1D(0.4)(embedding)\n",
        "  lstm = LSTM(128, \n",
        "              # activation='softsign',\n",
        "              dropout=0.3,\n",
        "              recurrent_dropout=0.3\n",
        "              # use_bias=True\n",
        "              # ,return_sequences=True\n",
        "              )(embedding)\n",
        "  # pool = GlobalMaxPool1D()(lstm)\n",
        "  # dense = Dense(24)(lstm)\n",
        "  # drop = Dropout(0.4)(dense)\n",
        "\n",
        "  # lstm = Bidirectional(LSTM(196, activation='softsign',\n",
        "  #           dropout=0.7,\n",
        "  #           recurrent_dropout=0.7,\n",
        "  #           use_bias=True\n",
        "  #           # ,return_sequences=True\n",
        "  #           ))(drop)\n",
        "  # dense = Dense(50)(lstm)\n",
        "  # drop = Dropout(0.7)(dense)\n",
        "\n",
        "  predict = Dense(1, activation='sigmoid')(lstm)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[source], outputs=[predict])\n",
        "\n",
        "  rmsprop = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=decay_rate)\n",
        "  model.compile(\n",
        "      optimizer=rmsprop,\n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EenkZdsjvHsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rnwt-lEyKTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1ChsH4VVxv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Train acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Train and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Train loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Train and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUDhBL7kv_9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve,roc_auc_score\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0h77kZoUcym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "037c0208-8523-4e23-ea29-6ac8feaa02ea"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import time, math\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\n",
        "\n",
        "tr_acc_array = []\n",
        "tr_loss_array = []\n",
        "te_acc_array = []\n",
        "te_loss_array = []\n",
        "time_array = []\n",
        "rmse_array = []\n",
        "\n",
        "n=0\n",
        "\n",
        "\n",
        "for train, test in kfold.split(x, y):\n",
        "  n+=1\n",
        "  print(\"--- Fold %d ---\" % (n))\n",
        "  tf.keras.backend.clear_session()\n",
        "  training_model = make_model(batch_size = 128)\n",
        "  # training_model.summary()\n",
        "\n",
        "  tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    training_model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "  start_time = time.time()\n",
        "  history = tpu_model.fit(x[train], y[train]\n",
        "                    ,epochs=epochs, verbose=1 \n",
        "                    ,validation_split=0.2\n",
        "                    ,batch_size=128 * 8\n",
        "                    ,validation_data=(x[test], y[test])\n",
        "                    ,callbacks=[es]\n",
        "                   )\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  time_array.append(time.time() - start_time)\n",
        "\n",
        "  model = tpu_model.sync_to_cpu()\n",
        "\n",
        "  tr_loss, tr_accuracy = model.evaluate(x[train], y[train], verbose=1)\n",
        "  te_loss, te_accuracy = model.evaluate(x[test], y[test], verbose=1)\n",
        "  print(\"Train Accuracy: {:.4f}\".format(tr_accuracy))\n",
        "  tr_acc_array.append(tr_accuracy)\n",
        "  print(\"Train Loss: {:.4f}\".format(tr_loss))\n",
        "  tr_loss_array.append(tr_loss)\n",
        "  print(\"Validation Accuracy:  {:.4f}\".format(te_accuracy))\n",
        "  te_acc_array.append(te_accuracy)\n",
        "  print(\"Validation Loss: {:.4f}\".format(te_loss))\n",
        "  te_loss_array.append(te_loss)\n",
        "\n",
        "  # model = tpu_model.sync_to_cpu()\n",
        "\n",
        "  pred = model.predict(x[test], batch_size=1000  )\n",
        "  y_pred = y_pred = (pred > 0.5)\n",
        "\n",
        "  print(confusion_matrix(y[test], y_pred))\n",
        "  # sns.set()\n",
        "  # sns.heatmap(confusion.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "  #             xticklabels=np.unique(y_pred),\n",
        "  #             yticklabels=np.unique(y_pred))\n",
        "  # plt.xlabel('true label')\n",
        "  # plt.ylabel('predicted label')\n",
        "  # plt.title('Confusion Matrix Prediction')\n",
        "\n",
        "\n",
        "  print(classification_report(y[test], y_pred))\n",
        "\n",
        "  # calculate root mean squared error\n",
        "  rmse = math.sqrt(mean_squared_error(y[test], y_pred))\n",
        "  print('RMSE: %.4f' % (rmse))\n",
        "  rmse_array.append(rmse)\n",
        "  # testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "  # print('Test Score: %.2f RMSE' % (testScore))\n",
        "\n",
        "  # auc_score=roc_auc_score(y[test], y_pred)  \n",
        "  # print('AUC: %.2f' % (auc_score))\n",
        "\n",
        "  # cvscores.append(scores[1] * 100)\n",
        "\n",
        "  # plot_history(history)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Fold 1 ---\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.76.226.242:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 9991819183416820074)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16423318294591108797)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 372813063997075366)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1911706453291360047)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5192434705240319694)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5125544372300616230)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3311322538967966271)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2284163816072855620)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 16497475372141900428)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8307680946862615395)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14034584378159045674)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20108 samples, validate on 5028 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.12126088142395 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20108 [==========================>...] - ETA: 1s - loss: 0.6931 - acc: 0.5010INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.069234609603882 secs\n",
            "19456/20108 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5039INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 3.439213275909424 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 4.30055570602417 secs\n",
            "20108/20108 [==============================] - 40s 2ms/sample - loss: 0.6930 - acc: 0.5065 - val_loss: 0.6922 - val_acc: 0.5732\n",
            "Epoch 2/500\n",
            "20108/20108 [==============================] - 6s 321us/sample - loss: 0.6913 - acc: 0.6122 - val_loss: 0.6904 - val_acc: 0.6636\n",
            "Epoch 3/500\n",
            "20108/20108 [==============================] - 6s 299us/sample - loss: 0.6894 - acc: 0.6717 - val_loss: 0.6885 - val_acc: 0.6965\n",
            "Epoch 4/500\n",
            "20108/20108 [==============================] - 6s 309us/sample - loss: 0.6875 - acc: 0.6937 - val_loss: 0.6865 - val_acc: 0.6992\n",
            "Epoch 5/500\n",
            "20108/20108 [==============================] - 6s 300us/sample - loss: 0.6853 - acc: 0.7002 - val_loss: 0.6843 - val_acc: 0.7008\n",
            "Epoch 6/500\n",
            "20108/20108 [==============================] - 6s 292us/sample - loss: 0.6830 - acc: 0.7016 - val_loss: 0.6819 - val_acc: 0.7014\n",
            "Epoch 7/500\n",
            "20108/20108 [==============================] - 6s 314us/sample - loss: 0.6803 - acc: 0.7019 - val_loss: 0.6791 - val_acc: 0.7014\n",
            "Epoch 8/500\n",
            "20108/20108 [==============================] - 6s 284us/sample - loss: 0.6772 - acc: 0.7018 - val_loss: 0.6759 - val_acc: 0.7014\n",
            "Epoch 9/500\n",
            "20108/20108 [==============================] - 6s 316us/sample - loss: 0.6738 - acc: 0.7017 - val_loss: 0.6723 - val_acc: 0.7014\n",
            "Epoch 10/500\n",
            "20108/20108 [==============================] - 6s 297us/sample - loss: 0.6698 - acc: 0.7017 - val_loss: 0.6682 - val_acc: 0.7014\n",
            "Epoch 11/500\n",
            "20108/20108 [==============================] - 6s 289us/sample - loss: 0.6653 - acc: 0.7018 - val_loss: 0.6633 - val_acc: 0.7014\n",
            "Epoch 12/500\n",
            "20108/20108 [==============================] - 6s 314us/sample - loss: 0.6597 - acc: 0.7016 - val_loss: 0.6577 - val_acc: 0.7014\n",
            "Epoch 13/500\n",
            "20108/20108 [==============================] - 6s 306us/sample - loss: 0.6535 - acc: 0.7017 - val_loss: 0.6509 - val_acc: 0.7014\n",
            "Epoch 14/500\n",
            "20108/20108 [==============================] - 6s 294us/sample - loss: 0.6456 - acc: 0.7017 - val_loss: 0.6428 - val_acc: 0.7014\n",
            "Epoch 15/500\n",
            "20108/20108 [==============================] - 6s 303us/sample - loss: 0.6368 - acc: 0.7017 - val_loss: 0.6331 - val_acc: 0.7014\n",
            "Epoch 16/500\n",
            "20108/20108 [==============================] - 6s 295us/sample - loss: 0.6265 - acc: 0.7017 - val_loss: 0.6221 - val_acc: 0.7014\n",
            "Epoch 17/500\n",
            "20108/20108 [==============================] - 6s 292us/sample - loss: 0.6165 - acc: 0.7017 - val_loss: 0.6123 - val_acc: 0.7014\n",
            "Epoch 18/500\n",
            "20108/20108 [==============================] - 7s 331us/sample - loss: 0.6137 - acc: 0.7016 - val_loss: 0.6078 - val_acc: 0.7014\n",
            "Epoch 19/500\n",
            "20108/20108 [==============================] - 6s 289us/sample - loss: 0.6113 - acc: 0.7017 - val_loss: 0.6051 - val_acc: 0.7014\n",
            "Epoch 20/500\n",
            "20108/20108 [==============================] - 7s 324us/sample - loss: 0.6059 - acc: 0.7018 - val_loss: 0.6026 - val_acc: 0.7014\n",
            "Epoch 21/500\n",
            "20108/20108 [==============================] - 6s 318us/sample - loss: 0.6050 - acc: 0.7016 - val_loss: 0.6004 - val_acc: 0.7014\n",
            "Epoch 22/500\n",
            "20108/20108 [==============================] - 6s 307us/sample - loss: 0.6024 - acc: 0.7016 - val_loss: 0.5983 - val_acc: 0.7014\n",
            "Epoch 23/500\n",
            "20108/20108 [==============================] - 6s 306us/sample - loss: 0.6001 - acc: 0.7017 - val_loss: 0.5961 - val_acc: 0.7014\n",
            "Epoch 24/500\n",
            "20108/20108 [==============================] - 7s 324us/sample - loss: 0.5991 - acc: 0.7017 - val_loss: 0.5941 - val_acc: 0.7014\n",
            "Epoch 25/500\n",
            "20108/20108 [==============================] - 6s 309us/sample - loss: 0.5955 - acc: 0.7017 - val_loss: 0.5919 - val_acc: 0.7014\n",
            "Epoch 26/500\n",
            "20108/20108 [==============================] - 6s 309us/sample - loss: 0.5948 - acc: 0.7017 - val_loss: 0.5901 - val_acc: 0.7014\n",
            "Epoch 27/500\n",
            "20108/20108 [==============================] - 6s 314us/sample - loss: 0.5916 - acc: 0.7017 - val_loss: 0.5878 - val_acc: 0.7014\n",
            "Epoch 28/500\n",
            "20108/20108 [==============================] - 6s 295us/sample - loss: 0.5895 - acc: 0.7017 - val_loss: 0.5856 - val_acc: 0.7014\n",
            "Epoch 29/500\n",
            "20108/20108 [==============================] - 6s 319us/sample - loss: 0.5875 - acc: 0.7017 - val_loss: 0.5834 - val_acc: 0.7014\n",
            "Epoch 30/500\n",
            "20108/20108 [==============================] - 6s 305us/sample - loss: 0.5836 - acc: 0.7016 - val_loss: 0.5809 - val_acc: 0.7014\n",
            "Epoch 31/500\n",
            "20108/20108 [==============================] - 6s 274us/sample - loss: 0.5812 - acc: 0.7017 - val_loss: 0.5782 - val_acc: 0.7014\n",
            "Epoch 32/500\n",
            "20108/20108 [==============================] - 6s 300us/sample - loss: 0.5793 - acc: 0.7017 - val_loss: 0.5757 - val_acc: 0.7014\n",
            "Epoch 33/500\n",
            "20108/20108 [==============================] - 6s 300us/sample - loss: 0.5740 - acc: 0.7018 - val_loss: 0.5719 - val_acc: 0.7014\n",
            "Epoch 34/500\n",
            "20108/20108 [==============================] - 6s 312us/sample - loss: 0.5720 - acc: 0.7017 - val_loss: 0.5692 - val_acc: 0.7014\n",
            "Epoch 35/500\n",
            "20108/20108 [==============================] - 6s 288us/sample - loss: 0.5681 - acc: 0.7017 - val_loss: 0.5658 - val_acc: 0.7014\n",
            "Epoch 36/500\n",
            "20108/20108 [==============================] - 6s 318us/sample - loss: 0.5648 - acc: 0.7016 - val_loss: 0.5625 - val_acc: 0.7014\n",
            "Epoch 37/500\n",
            "20108/20108 [==============================] - 6s 319us/sample - loss: 0.5621 - acc: 0.7018 - val_loss: 0.5583 - val_acc: 0.7014\n",
            "Epoch 38/500\n",
            "20108/20108 [==============================] - 6s 297us/sample - loss: 0.5567 - acc: 0.7019 - val_loss: 0.5546 - val_acc: 0.7016\n",
            "Epoch 39/500\n",
            "20108/20108 [==============================] - 7s 331us/sample - loss: 0.5531 - acc: 0.7021 - val_loss: 0.5499 - val_acc: 0.7016\n",
            "Epoch 40/500\n",
            "20108/20108 [==============================] - 6s 308us/sample - loss: 0.5479 - acc: 0.7021 - val_loss: 0.5455 - val_acc: 0.7016\n",
            "Epoch 41/500\n",
            "20108/20108 [==============================] - 6s 307us/sample - loss: 0.5441 - acc: 0.7024 - val_loss: 0.5412 - val_acc: 0.7018\n",
            "Epoch 42/500\n",
            "20108/20108 [==============================] - 6s 294us/sample - loss: 0.5394 - acc: 0.7037 - val_loss: 0.5360 - val_acc: 0.7022\n",
            "Epoch 43/500\n",
            "20108/20108 [==============================] - 6s 309us/sample - loss: 0.5339 - acc: 0.7056 - val_loss: 0.5304 - val_acc: 0.7038\n",
            "Epoch 44/500\n",
            "20108/20108 [==============================] - 6s 300us/sample - loss: 0.5291 - acc: 0.7064 - val_loss: 0.5252 - val_acc: 0.7056\n",
            "Epoch 45/500\n",
            "20108/20108 [==============================] - 6s 307us/sample - loss: 0.5233 - acc: 0.7099 - val_loss: 0.5201 - val_acc: 0.7080\n",
            "Epoch 46/500\n",
            "20108/20108 [==============================] - 6s 311us/sample - loss: 0.5200 - acc: 0.7135 - val_loss: 0.5160 - val_acc: 0.7100\n",
            "Epoch 47/500\n",
            "20108/20108 [==============================] - 6s 306us/sample - loss: 0.5139 - acc: 0.7167 - val_loss: 0.5100 - val_acc: 0.7140\n",
            "Epoch 48/500\n",
            "20108/20108 [==============================] - 6s 301us/sample - loss: 0.5094 - acc: 0.7217 - val_loss: 0.5055 - val_acc: 0.7195\n",
            "Epoch 49/500\n",
            "20108/20108 [==============================] - 6s 295us/sample - loss: 0.5041 - acc: 0.7289 - val_loss: 0.4989 - val_acc: 0.7221\n",
            "Epoch 50/500\n",
            "20108/20108 [==============================] - 7s 333us/sample - loss: 0.5007 - acc: 0.7315 - val_loss: 0.4940 - val_acc: 0.7275\n",
            "Epoch 51/500\n",
            "20108/20108 [==============================] - 6s 295us/sample - loss: 0.4944 - acc: 0.7384 - val_loss: 0.4880 - val_acc: 0.7319\n",
            "Epoch 52/500\n",
            "20108/20108 [==============================] - 6s 296us/sample - loss: 0.4905 - acc: 0.7451 - val_loss: 0.4838 - val_acc: 0.7387\n",
            "Epoch 53/500\n",
            "20108/20108 [==============================] - 6s 321us/sample - loss: 0.4833 - acc: 0.7533 - val_loss: 0.4784 - val_acc: 0.7456\n",
            "Epoch 54/500\n",
            "20108/20108 [==============================] - 6s 307us/sample - loss: 0.4794 - acc: 0.7569 - val_loss: 0.4741 - val_acc: 0.7516\n",
            "Epoch 55/500\n",
            "20108/20108 [==============================] - 7s 328us/sample - loss: 0.4712 - acc: 0.7658 - val_loss: 0.4679 - val_acc: 0.7588\n",
            "Epoch 56/500\n",
            "20108/20108 [==============================] - 6s 301us/sample - loss: 0.4674 - acc: 0.7723 - val_loss: 0.4631 - val_acc: 0.7649\n",
            "Epoch 57/500\n",
            "20108/20108 [==============================] - 6s 317us/sample - loss: 0.4629 - acc: 0.7806 - val_loss: 0.4565 - val_acc: 0.7709\n",
            "Epoch 58/500\n",
            "20108/20108 [==============================] - 6s 295us/sample - loss: 0.4574 - acc: 0.7849 - val_loss: 0.4539 - val_acc: 0.7795\n",
            "Epoch 59/500\n",
            "20108/20108 [==============================] - 6s 293us/sample - loss: 0.4538 - acc: 0.7917 - val_loss: 0.4464 - val_acc: 0.7834\n",
            "Epoch 60/500\n",
            "20108/20108 [==============================] - 6s 308us/sample - loss: 0.4500 - acc: 0.7964 - val_loss: 0.4452 - val_acc: 0.7908\n",
            "Epoch 61/500\n",
            "20108/20108 [==============================] - 6s 297us/sample - loss: 0.4451 - acc: 0.8023 - val_loss: 0.4404 - val_acc: 0.7952\n",
            "Epoch 62/500\n",
            "20108/20108 [==============================] - 6s 309us/sample - loss: 0.4458 - acc: 0.8046 - val_loss: 0.4385 - val_acc: 0.7986\n",
            "Epoch 63/500\n",
            "20108/20108 [==============================] - 6s 314us/sample - loss: 0.4380 - acc: 0.8114 - val_loss: 0.4322 - val_acc: 0.8016\n",
            "Epoch 64/500\n",
            "20108/20108 [==============================] - 6s 320us/sample - loss: 0.4320 - acc: 0.8121 - val_loss: 0.4284 - val_acc: 0.8051\n",
            "Epoch 65/500\n",
            "20108/20108 [==============================] - 6s 313us/sample - loss: 0.4300 - acc: 0.8157 - val_loss: 0.4241 - val_acc: 0.8079\n",
            "Epoch 66/500\n",
            "20108/20108 [==============================] - 6s 285us/sample - loss: 0.4256 - acc: 0.8191 - val_loss: 0.4212 - val_acc: 0.8137\n",
            "Epoch 67/500\n",
            "20108/20108 [==============================] - 6s 318us/sample - loss: 0.4199 - acc: 0.8248 - val_loss: 0.4171 - val_acc: 0.8163\n",
            "Epoch 68/500\n",
            "20108/20108 [==============================] - 6s 297us/sample - loss: 0.4173 - acc: 0.8252 - val_loss: 0.4148 - val_acc: 0.8185\n",
            "Epoch 69/500\n",
            "20108/20108 [==============================] - 6s 307us/sample - loss: 0.4154 - acc: 0.8246 - val_loss: 0.4108 - val_acc: 0.8193\n",
            "Epoch 70/500\n",
            "20108/20108 [==============================] - 6s 299us/sample - loss: 0.4098 - acc: 0.8275 - val_loss: 0.4094 - val_acc: 0.8209\n",
            "Epoch 71/500\n",
            "20108/20108 [==============================] - 6s 307us/sample - loss: 0.4086 - acc: 0.8318 - val_loss: 0.4044 - val_acc: 0.8221\n",
            "Epoch 72/500\n",
            "20108/20108 [==============================] - 6s 306us/sample - loss: 0.4040 - acc: 0.8325 - val_loss: 0.4031 - val_acc: 0.8248\n",
            "Epoch 73/500\n",
            "20108/20108 [==============================] - 6s 296us/sample - loss: 0.4024 - acc: 0.8323 - val_loss: 0.4009 - val_acc: 0.8244\n",
            "Epoch 74/500\n",
            "20108/20108 [==============================] - 6s 297us/sample - loss: 0.3989 - acc: 0.8352 - val_loss: 0.3968 - val_acc: 0.8250\n",
            "Epoch 75/500\n",
            "20108/20108 [==============================] - 6s 287us/sample - loss: 0.3987 - acc: 0.8346 - val_loss: 0.3957 - val_acc: 0.8262\n",
            "Epoch 76/500\n",
            "20108/20108 [==============================] - 6s 282us/sample - loss: 0.3949 - acc: 0.8375 - val_loss: 0.3940 - val_acc: 0.8262\n",
            "Epoch 77/500\n",
            "20108/20108 [==============================] - 6s 292us/sample - loss: 0.3923 - acc: 0.8379 - val_loss: 0.3906 - val_acc: 0.8276\n",
            "Epoch 78/500\n",
            "20108/20108 [==============================] - 6s 310us/sample - loss: 0.3864 - acc: 0.8387 - val_loss: 0.3886 - val_acc: 0.8282\n",
            "Epoch 79/500\n",
            "20108/20108 [==============================] - 6s 291us/sample - loss: 0.3850 - acc: 0.8411 - val_loss: 0.3840 - val_acc: 0.8292\n",
            "Epoch 80/500\n",
            "20108/20108 [==============================] - 6s 298us/sample - loss: 0.3834 - acc: 0.8405 - val_loss: 0.3847 - val_acc: 0.8284\n",
            "Epoch 81/500\n",
            "20108/20108 [==============================] - 6s 279us/sample - loss: 0.3800 - acc: 0.8449 - val_loss: 0.3824 - val_acc: 0.8320\n",
            "Epoch 82/500\n",
            "20108/20108 [==============================] - 7s 327us/sample - loss: 0.3785 - acc: 0.8441 - val_loss: 0.3800 - val_acc: 0.8322\n",
            "Epoch 83/500\n",
            "20108/20108 [==============================] - 6s 309us/sample - loss: 0.3728 - acc: 0.8451 - val_loss: 0.3771 - val_acc: 0.8336\n",
            "Epoch 84/500\n",
            "20108/20108 [==============================] - 6s 298us/sample - loss: 0.3764 - acc: 0.8426 - val_loss: 0.3777 - val_acc: 0.8340\n",
            "Epoch 85/500\n",
            "20108/20108 [==============================] - 6s 319us/sample - loss: 0.3714 - acc: 0.8475 - val_loss: 0.3758 - val_acc: 0.8350\n",
            "Epoch 86/500\n",
            "20108/20108 [==============================] - 6s 289us/sample - loss: 0.3671 - acc: 0.8477 - val_loss: 0.3716 - val_acc: 0.8372\n",
            "Epoch 87/500\n",
            "20108/20108 [==============================] - 6s 279us/sample - loss: 0.3660 - acc: 0.8468 - val_loss: 0.3712 - val_acc: 0.8374\n",
            "Epoch 88/500\n",
            "20108/20108 [==============================] - 6s 310us/sample - loss: 0.3639 - acc: 0.8490 - val_loss: 0.3702 - val_acc: 0.8372\n",
            "Epoch 89/500\n",
            "20108/20108 [==============================] - 6s 288us/sample - loss: 0.3628 - acc: 0.8485 - val_loss: 0.3696 - val_acc: 0.8368\n",
            "Epoch 90/500\n",
            "20108/20108 [==============================] - 6s 296us/sample - loss: 0.3621 - acc: 0.8490 - val_loss: 0.3684 - val_acc: 0.8370\n",
            "Epoch 91/500\n",
            "20108/20108 [==============================] - 6s 297us/sample - loss: 0.3601 - acc: 0.8513 - val_loss: 0.3683 - val_acc: 0.8374\n",
            "Epoch 92/500\n",
            "20108/20108 [==============================] - 6s 305us/sample - loss: 0.3584 - acc: 0.8518 - val_loss: 0.3662 - val_acc: 0.8382\n",
            "Epoch 93/500\n",
            "20108/20108 [==============================] - 6s 293us/sample - loss: 0.3552 - acc: 0.8518 - val_loss: 0.3645 - val_acc: 0.8376\n",
            "Epoch 94/500\n",
            "20108/20108 [==============================] - 6s 307us/sample - loss: 0.3548 - acc: 0.8526 - val_loss: 0.3645 - val_acc: 0.8384\n",
            "Epoch 95/500\n",
            "20108/20108 [==============================] - 6s 297us/sample - loss: 0.3499 - acc: 0.8544 - val_loss: 0.3611 - val_acc: 0.8398\n",
            "Epoch 96/500\n",
            "20108/20108 [==============================] - 6s 290us/sample - loss: 0.3514 - acc: 0.8546 - val_loss: 0.3620 - val_acc: 0.8398\n",
            "Epoch 97/500\n",
            "20108/20108 [==============================] - 6s 308us/sample - loss: 0.3493 - acc: 0.8555 - val_loss: 0.3611 - val_acc: 0.8394\n",
            "Epoch 98/500\n",
            "20108/20108 [==============================] - 6s 276us/sample - loss: 0.3451 - acc: 0.8552 - val_loss: 0.3603 - val_acc: 0.8402\n",
            "Epoch 99/500\n",
            "20108/20108 [==============================] - 6s 303us/sample - loss: 0.3442 - acc: 0.8571 - val_loss: 0.3585 - val_acc: 0.8426\n",
            "Epoch 100/500\n",
            "20108/20108 [==============================] - 6s 312us/sample - loss: 0.3437 - acc: 0.8572 - val_loss: 0.3580 - val_acc: 0.8418\n",
            "Epoch 101/500\n",
            "20108/20108 [==============================] - 6s 284us/sample - loss: 0.3413 - acc: 0.8576 - val_loss: 0.3573 - val_acc: 0.8418\n",
            "Epoch 102/500\n",
            "20108/20108 [==============================] - 6s 310us/sample - loss: 0.3405 - acc: 0.8576 - val_loss: 0.3562 - val_acc: 0.8426\n",
            "Epoch 103/500\n",
            "20108/20108 [==============================] - 6s 318us/sample - loss: 0.3385 - acc: 0.8581 - val_loss: 0.3550 - val_acc: 0.8432\n",
            "Epoch 104/500\n",
            "20108/20108 [==============================] - 6s 305us/sample - loss: 0.3378 - acc: 0.8586 - val_loss: 0.3554 - val_acc: 0.8422\n",
            "Epoch 105/500\n",
            "20108/20108 [==============================] - 6s 298us/sample - loss: 0.3342 - acc: 0.8601 - val_loss: 0.3534 - val_acc: 0.8439\n",
            "Epoch 106/500\n",
            "20108/20108 [==============================] - 6s 293us/sample - loss: 0.3361 - acc: 0.8611 - val_loss: 0.3536 - val_acc: 0.8441\n",
            "Epoch 107/500\n",
            "20108/20108 [==============================] - 6s 302us/sample - loss: 0.3333 - acc: 0.8612 - val_loss: 0.3537 - val_acc: 0.8430\n",
            "Epoch 108/500\n",
            "20108/20108 [==============================] - 6s 296us/sample - loss: 0.3312 - acc: 0.8635 - val_loss: 0.3522 - val_acc: 0.8441\n",
            "Epoch 109/500\n",
            "20108/20108 [==============================] - 6s 311us/sample - loss: 0.3329 - acc: 0.8624 - val_loss: 0.3519 - val_acc: 0.8438\n",
            "Epoch 110/500\n",
            "20108/20108 [==============================] - 6s 304us/sample - loss: 0.3268 - acc: 0.8659 - val_loss: 0.3508 - val_acc: 0.8436\n",
            "Epoch 111/500\n",
            "20108/20108 [==============================] - 6s 287us/sample - loss: 0.3315 - acc: 0.8623 - val_loss: 0.3507 - val_acc: 0.8436\n",
            "Epoch 112/500\n",
            "20108/20108 [==============================] - 6s 299us/sample - loss: 0.3286 - acc: 0.8619 - val_loss: 0.3506 - val_acc: 0.8428\n",
            "Epoch 113/500\n",
            "20108/20108 [==============================] - 6s 295us/sample - loss: 0.3258 - acc: 0.8643 - val_loss: 0.3497 - val_acc: 0.8439\n",
            "Epoch 114/500\n",
            "20108/20108 [==============================] - 6s 305us/sample - loss: 0.3242 - acc: 0.8645 - val_loss: 0.3495 - val_acc: 0.8441\n",
            "Epoch 115/500\n",
            "20108/20108 [==============================] - 6s 292us/sample - loss: 0.3242 - acc: 0.8666 - val_loss: 0.3490 - val_acc: 0.8441\n",
            "Epoch 116/500\n",
            "20108/20108 [==============================] - 6s 280us/sample - loss: 0.3227 - acc: 0.8677 - val_loss: 0.3485 - val_acc: 0.8447\n",
            "Epoch 117/500\n",
            "20108/20108 [==============================] - 6s 311us/sample - loss: 0.3222 - acc: 0.8673 - val_loss: 0.3485 - val_acc: 0.8457\n",
            "Epoch 118/500\n",
            "20108/20108 [==============================] - 6s 309us/sample - loss: 0.3207 - acc: 0.8676 - val_loss: 0.3482 - val_acc: 0.8459\n",
            "Epoch 119/500\n",
            "20108/20108 [==============================] - 6s 286us/sample - loss: 0.3207 - acc: 0.8670 - val_loss: 0.3470 - val_acc: 0.8459\n",
            "Epoch 120/500\n",
            "20108/20108 [==============================] - 6s 306us/sample - loss: 0.3165 - acc: 0.8712 - val_loss: 0.3473 - val_acc: 0.8463\n",
            "Epoch 121/500\n",
            "20108/20108 [==============================] - 6s 303us/sample - loss: 0.3179 - acc: 0.8702 - val_loss: 0.3475 - val_acc: 0.8459\n",
            "Epoch 122/500\n",
            "20108/20108 [==============================] - 6s 285us/sample - loss: 0.3162 - acc: 0.8693 - val_loss: 0.3470 - val_acc: 0.8463\n",
            "Epoch 123/500\n",
            "20108/20108 [==============================] - 6s 314us/sample - loss: 0.3158 - acc: 0.8703 - val_loss: 0.3464 - val_acc: 0.8463\n",
            "Epoch 124/500\n",
            "20108/20108 [==============================] - 6s 293us/sample - loss: 0.3161 - acc: 0.8692 - val_loss: 0.3466 - val_acc: 0.8471\n",
            "Epoch 125/500\n",
            "20108/20108 [==============================] - 6s 306us/sample - loss: 0.3135 - acc: 0.8710 - val_loss: 0.3463 - val_acc: 0.8469\n",
            "Epoch 126/500\n",
            "20108/20108 [==============================] - 6s 291us/sample - loss: 0.3127 - acc: 0.8705 - val_loss: 0.3465 - val_acc: 0.8467\n",
            "Epoch 127/500\n",
            "20108/20108 [==============================] - 6s 300us/sample - loss: 0.3119 - acc: 0.8702 - val_loss: 0.3463 - val_acc: 0.8469\n",
            "Epoch 128/500\n",
            "20108/20108 [==============================] - 6s 308us/sample - loss: 0.3093 - acc: 0.8724 - val_loss: 0.3452 - val_acc: 0.8483\n",
            "Epoch 129/500\n",
            "20108/20108 [==============================] - 6s 313us/sample - loss: 0.3120 - acc: 0.8711 - val_loss: 0.3454 - val_acc: 0.8481\n",
            "Epoch 130/500\n",
            "20108/20108 [==============================] - 6s 320us/sample - loss: 0.3089 - acc: 0.8739 - val_loss: 0.3451 - val_acc: 0.8491\n",
            "Epoch 131/500\n",
            "20108/20108 [==============================] - 6s 294us/sample - loss: 0.3083 - acc: 0.8742 - val_loss: 0.3452 - val_acc: 0.8487\n",
            "Epoch 132/500\n",
            "20108/20108 [==============================] - 6s 321us/sample - loss: 0.3067 - acc: 0.8734 - val_loss: 0.3449 - val_acc: 0.8495\n",
            "Epoch 133/500\n",
            "20108/20108 [==============================] - 6s 300us/sample - loss: 0.3077 - acc: 0.8748 - val_loss: 0.3449 - val_acc: 0.8499\n",
            "Epoch 134/500\n",
            "20108/20108 [==============================] - 6s 311us/sample - loss: 0.3065 - acc: 0.8767 - val_loss: 0.3447 - val_acc: 0.8493\n",
            "Epoch 135/500\n",
            "20108/20108 [==============================] - 6s 290us/sample - loss: 0.3056 - acc: 0.8752 - val_loss: 0.3443 - val_acc: 0.8507\n",
            "Epoch 136/500\n",
            "20108/20108 [==============================] - 6s 293us/sample - loss: 0.3036 - acc: 0.8759 - val_loss: 0.3443 - val_acc: 0.8515\n",
            "Epoch 137/500\n",
            "20108/20108 [==============================] - 6s 313us/sample - loss: 0.3021 - acc: 0.8773 - val_loss: 0.3445 - val_acc: 0.8511\n",
            "Epoch 138/500\n",
            "20108/20108 [==============================] - 6s 301us/sample - loss: 0.3010 - acc: 0.8746 - val_loss: 0.3440 - val_acc: 0.8517\n",
            "Epoch 139/500\n",
            "20108/20108 [==============================] - 6s 311us/sample - loss: 0.3024 - acc: 0.8785 - val_loss: 0.3446 - val_acc: 0.8507\n",
            "Epoch 140/500\n",
            "20108/20108 [==============================] - 6s 297us/sample - loss: 0.3028 - acc: 0.8767 - val_loss: 0.3445 - val_acc: 0.8507\n",
            "Epoch 141/500\n",
            "20108/20108 [==============================] - 6s 295us/sample - loss: 0.2998 - acc: 0.8769 - val_loss: 0.3441 - val_acc: 0.8515\n",
            "Epoch 142/500\n",
            "20108/20108 [==============================] - 6s 322us/sample - loss: 0.3033 - acc: 0.8771 - val_loss: 0.3445 - val_acc: 0.8509\n",
            "Epoch 143/500\n",
            "20108/20108 [==============================] - 6s 290us/sample - loss: 0.2989 - acc: 0.8797 - val_loss: 0.3441 - val_acc: 0.8519\n",
            "Epoch 144/500\n",
            "20108/20108 [==============================] - 6s 309us/sample - loss: 0.3005 - acc: 0.8765 - val_loss: 0.3439 - val_acc: 0.8523\n",
            "Epoch 145/500\n",
            "20108/20108 [==============================] - 6s 293us/sample - loss: 0.2979 - acc: 0.8778 - val_loss: 0.3437 - val_acc: 0.8515\n",
            "Epoch 146/500\n",
            "20108/20108 [==============================] - 6s 306us/sample - loss: 0.2993 - acc: 0.8784 - val_loss: 0.3437 - val_acc: 0.8515\n",
            "Epoch 147/500\n",
            "20108/20108 [==============================] - 6s 318us/sample - loss: 0.2984 - acc: 0.8789 - val_loss: 0.3436 - val_acc: 0.8517\n",
            "Epoch 148/500\n",
            "20108/20108 [==============================] - 6s 311us/sample - loss: 0.2980 - acc: 0.8799 - val_loss: 0.3439 - val_acc: 0.8519\n",
            "Epoch 149/500\n",
            "20108/20108 [==============================] - 6s 283us/sample - loss: 0.2948 - acc: 0.8801 - val_loss: 0.3436 - val_acc: 0.8513\n",
            "Epoch 150/500\n",
            "20108/20108 [==============================] - 6s 299us/sample - loss: 0.2937 - acc: 0.8798 - val_loss: 0.3439 - val_acc: 0.8527\n",
            "Epoch 151/500\n",
            "20108/20108 [==============================] - 6s 297us/sample - loss: 0.2964 - acc: 0.8789 - val_loss: 0.3440 - val_acc: 0.8523\n",
            "Epoch 152/500\n",
            "20108/20108 [==============================] - 6s 306us/sample - loss: 0.2936 - acc: 0.8832 - val_loss: 0.3437 - val_acc: 0.8539\n",
            "Epoch 153/500\n",
            "20108/20108 [==============================] - 6s 312us/sample - loss: 0.2947 - acc: 0.8799 - val_loss: 0.3435 - val_acc: 0.8519\n",
            "Epoch 154/500\n",
            "20108/20108 [==============================] - 6s 301us/sample - loss: 0.2911 - acc: 0.8818 - val_loss: 0.3436 - val_acc: 0.8535\n",
            "Epoch 155/500\n",
            "20108/20108 [==============================] - 6s 297us/sample - loss: 0.2909 - acc: 0.8842 - val_loss: 0.3434 - val_acc: 0.8529\n",
            "Epoch 156/500\n",
            "20108/20108 [==============================] - 6s 282us/sample - loss: 0.2939 - acc: 0.8814 - val_loss: 0.3437 - val_acc: 0.8519\n",
            "Epoch 157/500\n",
            "20108/20108 [==============================] - 6s 309us/sample - loss: 0.2905 - acc: 0.8820 - val_loss: 0.3438 - val_acc: 0.8537\n",
            "Epoch 158/500\n",
            "20108/20108 [==============================] - 6s 291us/sample - loss: 0.2910 - acc: 0.8842 - val_loss: 0.3435 - val_acc: 0.8545\n",
            "Epoch 159/500\n",
            "20108/20108 [==============================] - 6s 298us/sample - loss: 0.2905 - acc: 0.8820 - val_loss: 0.3436 - val_acc: 0.8535\n",
            "Epoch 160/500\n",
            "20108/20108 [==============================] - 6s 301us/sample - loss: 0.2890 - acc: 0.8844 - val_loss: 0.3441 - val_acc: 0.8547\n",
            "Epoch 161/500\n",
            "20108/20108 [==============================] - 6s 300us/sample - loss: 0.2924 - acc: 0.8807 - val_loss: 0.3441 - val_acc: 0.8525\n",
            "Epoch 162/500\n",
            "20108/20108 [==============================] - 6s 319us/sample - loss: 0.2874 - acc: 0.8844 - val_loss: 0.3443 - val_acc: 0.8531\n",
            "Epoch 00162: early stopping\n",
            "--- 1021.3748168945312 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20108/20108 [==============================] - 54s 3ms/sample - loss: 0.2701 - acc: 0.8923\n",
            "5028/5028 [==============================] - 14s 3ms/sample - loss: 0.3442 - acc: 0.8530\n",
            "Train Accuracy: 0.8923\n",
            "Train Loss: 0.2701\n",
            "Validation Accuracy:  0.8530\n",
            "Validation Loss: 0.3442\n",
            "5028/5028 [==============================] - 5s 1ms/sample\n",
            "[[1070  430]\n",
            " [ 309 3219]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.71      0.74      1500\n",
            "         1.0       0.88      0.91      0.90      3528\n",
            "\n",
            "    accuracy                           0.85      5028\n",
            "   macro avg       0.83      0.81      0.82      5028\n",
            "weighted avg       0.85      0.85      0.85      5028\n",
            "\n",
            "RMSE: 0.3834\n",
            "--- Fold 2 ---\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.76.226.242:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 9991819183416820074)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16423318294591108797)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 372813063997075366)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1911706453291360047)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5192434705240319694)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5125544372300616230)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3311322538967966271)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2284163816072855620)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 16497475372141900428)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8307680946862615395)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14034584378159045674)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20109 samples, validate on 5027 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.074996709823608 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20109 [==========================>...] - ETA: 1s - loss: 0.6926 - acc: 0.5391INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.290839433670044 secs\n",
            "19456/20109 [============================>.] - ETA: 0s - loss: 0.6926 - acc: 0.5427INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 3.6796767711639404 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 4.300693988800049 secs\n",
            "20109/20109 [==============================] - 41s 2ms/sample - loss: 0.6925 - acc: 0.5447 - val_loss: 0.6916 - val_acc: 0.6184\n",
            "Epoch 2/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.6907 - acc: 0.6398 - val_loss: 0.6899 - val_acc: 0.6819\n",
            "Epoch 3/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.6889 - acc: 0.6870 - val_loss: 0.6880 - val_acc: 0.6984\n",
            "Epoch 4/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.6869 - acc: 0.6994 - val_loss: 0.6859 - val_acc: 0.7028\n",
            "Epoch 5/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.6847 - acc: 0.7019 - val_loss: 0.6836 - val_acc: 0.7020\n",
            "Epoch 6/500\n",
            "20109/20109 [==============================] - 6s 290us/sample - loss: 0.6822 - acc: 0.7019 - val_loss: 0.6810 - val_acc: 0.7020\n",
            "Epoch 7/500\n",
            "20109/20109 [==============================] - 6s 289us/sample - loss: 0.6794 - acc: 0.7019 - val_loss: 0.6781 - val_acc: 0.7020\n",
            "Epoch 8/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.6763 - acc: 0.7017 - val_loss: 0.6748 - val_acc: 0.7020\n",
            "Epoch 9/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.6727 - acc: 0.7016 - val_loss: 0.6710 - val_acc: 0.7020\n",
            "Epoch 10/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.6684 - acc: 0.7017 - val_loss: 0.6666 - val_acc: 0.7020\n",
            "Epoch 11/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.6636 - acc: 0.7016 - val_loss: 0.6615 - val_acc: 0.7020\n",
            "Epoch 12/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.6580 - acc: 0.7016 - val_loss: 0.6554 - val_acc: 0.7020\n",
            "Epoch 13/500\n",
            "20109/20109 [==============================] - 6s 287us/sample - loss: 0.6511 - acc: 0.7017 - val_loss: 0.6482 - val_acc: 0.7020\n",
            "Epoch 14/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.6431 - acc: 0.7016 - val_loss: 0.6396 - val_acc: 0.7020\n",
            "Epoch 15/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.6340 - acc: 0.7017 - val_loss: 0.6296 - val_acc: 0.7020\n",
            "Epoch 16/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.6248 - acc: 0.7017 - val_loss: 0.6192 - val_acc: 0.7020\n",
            "Epoch 17/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.6155 - acc: 0.7016 - val_loss: 0.6115 - val_acc: 0.7020\n",
            "Epoch 18/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.6127 - acc: 0.7017 - val_loss: 0.6078 - val_acc: 0.7020\n",
            "Epoch 19/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.6084 - acc: 0.7017 - val_loss: 0.6044 - val_acc: 0.7020\n",
            "Epoch 20/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.6061 - acc: 0.7016 - val_loss: 0.6022 - val_acc: 0.7020\n",
            "Epoch 21/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.6063 - acc: 0.7016 - val_loss: 0.6002 - val_acc: 0.7020\n",
            "Epoch 22/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.6037 - acc: 0.7016 - val_loss: 0.5979 - val_acc: 0.7020\n",
            "Epoch 23/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.5997 - acc: 0.7017 - val_loss: 0.5958 - val_acc: 0.7020\n",
            "Epoch 24/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.5974 - acc: 0.7016 - val_loss: 0.5940 - val_acc: 0.7020\n",
            "Epoch 25/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.5958 - acc: 0.7016 - val_loss: 0.5916 - val_acc: 0.7020\n",
            "Epoch 26/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.5944 - acc: 0.7016 - val_loss: 0.5895 - val_acc: 0.7020\n",
            "Epoch 27/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.5913 - acc: 0.7017 - val_loss: 0.5878 - val_acc: 0.7020\n",
            "Epoch 28/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.5878 - acc: 0.7016 - val_loss: 0.5853 - val_acc: 0.7020\n",
            "Epoch 29/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.5863 - acc: 0.7016 - val_loss: 0.5830 - val_acc: 0.7020\n",
            "Epoch 30/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.5855 - acc: 0.7017 - val_loss: 0.5809 - val_acc: 0.7020\n",
            "Epoch 31/500\n",
            "20109/20109 [==============================] - 6s 282us/sample - loss: 0.5822 - acc: 0.7017 - val_loss: 0.5787 - val_acc: 0.7020\n",
            "Epoch 32/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.5800 - acc: 0.7016 - val_loss: 0.5761 - val_acc: 0.7020\n",
            "Epoch 33/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.5759 - acc: 0.7016 - val_loss: 0.5733 - val_acc: 0.7020\n",
            "Epoch 34/500\n",
            "20109/20109 [==============================] - 6s 305us/sample - loss: 0.5719 - acc: 0.7017 - val_loss: 0.5699 - val_acc: 0.7020\n",
            "Epoch 35/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.5695 - acc: 0.7017 - val_loss: 0.5668 - val_acc: 0.7020\n",
            "Epoch 36/500\n",
            "20109/20109 [==============================] - 6s 312us/sample - loss: 0.5667 - acc: 0.7018 - val_loss: 0.5640 - val_acc: 0.7020\n",
            "Epoch 37/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.5630 - acc: 0.7018 - val_loss: 0.5605 - val_acc: 0.7020\n",
            "Epoch 38/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.5582 - acc: 0.7016 - val_loss: 0.5567 - val_acc: 0.7020\n",
            "Epoch 39/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.5555 - acc: 0.7019 - val_loss: 0.5528 - val_acc: 0.7022\n",
            "Epoch 40/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.5518 - acc: 0.7019 - val_loss: 0.5485 - val_acc: 0.7022\n",
            "Epoch 41/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.5459 - acc: 0.7026 - val_loss: 0.5438 - val_acc: 0.7024\n",
            "Epoch 42/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.5426 - acc: 0.7033 - val_loss: 0.5394 - val_acc: 0.7026\n",
            "Epoch 43/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.5370 - acc: 0.7041 - val_loss: 0.5351 - val_acc: 0.7040\n",
            "Epoch 44/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.5332 - acc: 0.7053 - val_loss: 0.5296 - val_acc: 0.7060\n",
            "Epoch 45/500\n",
            "20109/20109 [==============================] - 6s 282us/sample - loss: 0.5280 - acc: 0.7078 - val_loss: 0.5242 - val_acc: 0.7078\n",
            "Epoch 46/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.5218 - acc: 0.7105 - val_loss: 0.5190 - val_acc: 0.7104\n",
            "Epoch 47/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.5177 - acc: 0.7145 - val_loss: 0.5146 - val_acc: 0.7138\n",
            "Epoch 48/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.5111 - acc: 0.7181 - val_loss: 0.5085 - val_acc: 0.7162\n",
            "Epoch 49/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.5051 - acc: 0.7232 - val_loss: 0.5020 - val_acc: 0.7203\n",
            "Epoch 50/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.5023 - acc: 0.7298 - val_loss: 0.4994 - val_acc: 0.7297\n",
            "Epoch 51/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.4959 - acc: 0.7359 - val_loss: 0.4922 - val_acc: 0.7343\n",
            "Epoch 52/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.4906 - acc: 0.7431 - val_loss: 0.4884 - val_acc: 0.7406\n",
            "Epoch 53/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.4845 - acc: 0.7501 - val_loss: 0.4824 - val_acc: 0.7470\n",
            "Epoch 54/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.4804 - acc: 0.7584 - val_loss: 0.4792 - val_acc: 0.7562\n",
            "Epoch 55/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.4753 - acc: 0.7660 - val_loss: 0.4738 - val_acc: 0.7631\n",
            "Epoch 56/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.4703 - acc: 0.7719 - val_loss: 0.4694 - val_acc: 0.7729\n",
            "Epoch 57/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.4640 - acc: 0.7817 - val_loss: 0.4638 - val_acc: 0.7807\n",
            "Epoch 58/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.4606 - acc: 0.7855 - val_loss: 0.4582 - val_acc: 0.7838\n",
            "Epoch 59/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.4572 - acc: 0.7911 - val_loss: 0.4548 - val_acc: 0.7890\n",
            "Epoch 60/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.4518 - acc: 0.7990 - val_loss: 0.4510 - val_acc: 0.7914\n",
            "Epoch 61/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.4465 - acc: 0.8021 - val_loss: 0.4482 - val_acc: 0.7970\n",
            "Epoch 62/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.4431 - acc: 0.8068 - val_loss: 0.4452 - val_acc: 0.8031\n",
            "Epoch 63/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.4382 - acc: 0.8110 - val_loss: 0.4384 - val_acc: 0.8055\n",
            "Epoch 64/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.4339 - acc: 0.8150 - val_loss: 0.4363 - val_acc: 0.8085\n",
            "Epoch 65/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.4294 - acc: 0.8194 - val_loss: 0.4307 - val_acc: 0.8119\n",
            "Epoch 66/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.4234 - acc: 0.8183 - val_loss: 0.4274 - val_acc: 0.8151\n",
            "Epoch 67/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.4210 - acc: 0.8202 - val_loss: 0.4265 - val_acc: 0.8175\n",
            "Epoch 68/500\n",
            "20109/20109 [==============================] - 7s 330us/sample - loss: 0.4177 - acc: 0.8278 - val_loss: 0.4228 - val_acc: 0.8185\n",
            "Epoch 69/500\n",
            "20109/20109 [==============================] - 7s 338us/sample - loss: 0.4154 - acc: 0.8268 - val_loss: 0.4198 - val_acc: 0.8205\n",
            "Epoch 70/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.4081 - acc: 0.8298 - val_loss: 0.4156 - val_acc: 0.8221\n",
            "Epoch 71/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.4095 - acc: 0.8260 - val_loss: 0.4148 - val_acc: 0.8215\n",
            "Epoch 72/500\n",
            "20109/20109 [==============================] - 6s 305us/sample - loss: 0.4051 - acc: 0.8305 - val_loss: 0.4112 - val_acc: 0.8230\n",
            "Epoch 73/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.3986 - acc: 0.8357 - val_loss: 0.4067 - val_acc: 0.8232\n",
            "Epoch 74/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.3965 - acc: 0.8352 - val_loss: 0.4032 - val_acc: 0.8240\n",
            "Epoch 75/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.3968 - acc: 0.8368 - val_loss: 0.4023 - val_acc: 0.8250\n",
            "Epoch 76/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.3923 - acc: 0.8385 - val_loss: 0.3985 - val_acc: 0.8260\n",
            "Epoch 77/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.3892 - acc: 0.8384 - val_loss: 0.3970 - val_acc: 0.8268\n",
            "Epoch 78/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.3871 - acc: 0.8394 - val_loss: 0.3951 - val_acc: 0.8254\n",
            "Epoch 79/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.3814 - acc: 0.8419 - val_loss: 0.3918 - val_acc: 0.8266\n",
            "Epoch 80/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.3797 - acc: 0.8426 - val_loss: 0.3910 - val_acc: 0.8278\n",
            "Epoch 81/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.3771 - acc: 0.8442 - val_loss: 0.3889 - val_acc: 0.8300\n",
            "Epoch 82/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.3760 - acc: 0.8443 - val_loss: 0.3880 - val_acc: 0.8310\n",
            "Epoch 83/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3739 - acc: 0.8434 - val_loss: 0.3860 - val_acc: 0.8316\n",
            "Epoch 84/500\n",
            "20109/20109 [==============================] - 6s 320us/sample - loss: 0.3716 - acc: 0.8420 - val_loss: 0.3844 - val_acc: 0.8314\n",
            "Epoch 85/500\n",
            "20109/20109 [==============================] - 6s 320us/sample - loss: 0.3672 - acc: 0.8456 - val_loss: 0.3826 - val_acc: 0.8332\n",
            "Epoch 86/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.3685 - acc: 0.8468 - val_loss: 0.3827 - val_acc: 0.8330\n",
            "Epoch 87/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.3642 - acc: 0.8473 - val_loss: 0.3794 - val_acc: 0.8350\n",
            "Epoch 88/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.3624 - acc: 0.8474 - val_loss: 0.3801 - val_acc: 0.8356\n",
            "Epoch 89/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.3610 - acc: 0.8496 - val_loss: 0.3768 - val_acc: 0.8350\n",
            "Epoch 90/500\n",
            "20109/20109 [==============================] - 6s 320us/sample - loss: 0.3593 - acc: 0.8501 - val_loss: 0.3764 - val_acc: 0.8368\n",
            "Epoch 91/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.3572 - acc: 0.8506 - val_loss: 0.3753 - val_acc: 0.8366\n",
            "Epoch 92/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.3540 - acc: 0.8527 - val_loss: 0.3746 - val_acc: 0.8378\n",
            "Epoch 93/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.3529 - acc: 0.8543 - val_loss: 0.3719 - val_acc: 0.8388\n",
            "Epoch 94/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.3512 - acc: 0.8524 - val_loss: 0.3724 - val_acc: 0.8386\n",
            "Epoch 95/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.3506 - acc: 0.8541 - val_loss: 0.3709 - val_acc: 0.8384\n",
            "Epoch 96/500\n",
            "20109/20109 [==============================] - 6s 290us/sample - loss: 0.3487 - acc: 0.8551 - val_loss: 0.3713 - val_acc: 0.8380\n",
            "Epoch 97/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.3454 - acc: 0.8571 - val_loss: 0.3691 - val_acc: 0.8390\n",
            "Epoch 98/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.3465 - acc: 0.8527 - val_loss: 0.3689 - val_acc: 0.8390\n",
            "Epoch 99/500\n",
            "20109/20109 [==============================] - 7s 337us/sample - loss: 0.3441 - acc: 0.8540 - val_loss: 0.3687 - val_acc: 0.8392\n",
            "Epoch 100/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.3439 - acc: 0.8557 - val_loss: 0.3686 - val_acc: 0.8394\n",
            "Epoch 101/500\n",
            "20109/20109 [==============================] - 6s 320us/sample - loss: 0.3400 - acc: 0.8573 - val_loss: 0.3671 - val_acc: 0.8402\n",
            "Epoch 102/500\n",
            "20109/20109 [==============================] - 6s 305us/sample - loss: 0.3397 - acc: 0.8585 - val_loss: 0.3671 - val_acc: 0.8402\n",
            "Epoch 103/500\n",
            "20109/20109 [==============================] - 6s 287us/sample - loss: 0.3391 - acc: 0.8565 - val_loss: 0.3661 - val_acc: 0.8400\n",
            "Epoch 104/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.3351 - acc: 0.8601 - val_loss: 0.3634 - val_acc: 0.8408\n",
            "Epoch 105/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.3363 - acc: 0.8591 - val_loss: 0.3631 - val_acc: 0.8408\n",
            "Epoch 106/500\n",
            "20109/20109 [==============================] - 6s 289us/sample - loss: 0.3351 - acc: 0.8603 - val_loss: 0.3625 - val_acc: 0.8412\n",
            "Epoch 107/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.3350 - acc: 0.8594 - val_loss: 0.3627 - val_acc: 0.8402\n",
            "Epoch 108/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.3298 - acc: 0.8625 - val_loss: 0.3610 - val_acc: 0.8418\n",
            "Epoch 109/500\n",
            "20109/20109 [==============================] - 7s 341us/sample - loss: 0.3300 - acc: 0.8624 - val_loss: 0.3621 - val_acc: 0.8402\n",
            "Epoch 110/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.3313 - acc: 0.8635 - val_loss: 0.3608 - val_acc: 0.8410\n",
            "Epoch 111/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.3276 - acc: 0.8653 - val_loss: 0.3595 - val_acc: 0.8422\n",
            "Epoch 112/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.3287 - acc: 0.8630 - val_loss: 0.3589 - val_acc: 0.8428\n",
            "Epoch 113/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.3274 - acc: 0.8651 - val_loss: 0.3597 - val_acc: 0.8406\n",
            "Epoch 114/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.3248 - acc: 0.8668 - val_loss: 0.3586 - val_acc: 0.8402\n",
            "Epoch 115/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.3245 - acc: 0.8659 - val_loss: 0.3583 - val_acc: 0.8406\n",
            "Epoch 116/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.3225 - acc: 0.8662 - val_loss: 0.3579 - val_acc: 0.8410\n",
            "Epoch 117/500\n",
            "20109/20109 [==============================] - 6s 314us/sample - loss: 0.3215 - acc: 0.8678 - val_loss: 0.3569 - val_acc: 0.8418\n",
            "Epoch 118/500\n",
            "20109/20109 [==============================] - 6s 289us/sample - loss: 0.3199 - acc: 0.8703 - val_loss: 0.3578 - val_acc: 0.8404\n",
            "Epoch 119/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.3202 - acc: 0.8676 - val_loss: 0.3572 - val_acc: 0.8402\n",
            "Epoch 120/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3195 - acc: 0.8698 - val_loss: 0.3565 - val_acc: 0.8410\n",
            "Epoch 121/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.3196 - acc: 0.8694 - val_loss: 0.3560 - val_acc: 0.8416\n",
            "Epoch 122/500\n",
            "20109/20109 [==============================] - 6s 276us/sample - loss: 0.3171 - acc: 0.8687 - val_loss: 0.3554 - val_acc: 0.8414\n",
            "Epoch 123/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.3177 - acc: 0.8695 - val_loss: 0.3556 - val_acc: 0.8416\n",
            "Epoch 124/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.3157 - acc: 0.8711 - val_loss: 0.3551 - val_acc: 0.8422\n",
            "Epoch 125/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.3151 - acc: 0.8698 - val_loss: 0.3544 - val_acc: 0.8420\n",
            "Epoch 126/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3129 - acc: 0.8717 - val_loss: 0.3533 - val_acc: 0.8436\n",
            "Epoch 127/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.3142 - acc: 0.8722 - val_loss: 0.3543 - val_acc: 0.8426\n",
            "Epoch 128/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.3125 - acc: 0.8727 - val_loss: 0.3539 - val_acc: 0.8430\n",
            "Epoch 129/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.3110 - acc: 0.8750 - val_loss: 0.3535 - val_acc: 0.8430\n",
            "Epoch 130/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.3083 - acc: 0.8746 - val_loss: 0.3524 - val_acc: 0.8434\n",
            "Epoch 131/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.3112 - acc: 0.8749 - val_loss: 0.3530 - val_acc: 0.8432\n",
            "Epoch 132/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3107 - acc: 0.8737 - val_loss: 0.3534 - val_acc: 0.8430\n",
            "Epoch 133/500\n",
            "20109/20109 [==============================] - 7s 336us/sample - loss: 0.3058 - acc: 0.8745 - val_loss: 0.3513 - val_acc: 0.8445\n",
            "Epoch 134/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.3085 - acc: 0.8750 - val_loss: 0.3522 - val_acc: 0.8443\n",
            "Epoch 135/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.3072 - acc: 0.8744 - val_loss: 0.3520 - val_acc: 0.8443\n",
            "Epoch 136/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.3064 - acc: 0.8755 - val_loss: 0.3516 - val_acc: 0.8443\n",
            "Epoch 137/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.3055 - acc: 0.8742 - val_loss: 0.3514 - val_acc: 0.8443\n",
            "Epoch 138/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.3055 - acc: 0.8758 - val_loss: 0.3522 - val_acc: 0.8439\n",
            "Epoch 139/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.3047 - acc: 0.8720 - val_loss: 0.3522 - val_acc: 0.8443\n",
            "Epoch 140/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.3017 - acc: 0.8780 - val_loss: 0.3512 - val_acc: 0.8443\n",
            "Epoch 141/500\n",
            "20109/20109 [==============================] - 6s 314us/sample - loss: 0.3013 - acc: 0.8767 - val_loss: 0.3512 - val_acc: 0.8447\n",
            "Epoch 142/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.3036 - acc: 0.8775 - val_loss: 0.3512 - val_acc: 0.8455\n",
            "Epoch 143/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.3048 - acc: 0.8747 - val_loss: 0.3512 - val_acc: 0.8451\n",
            "Epoch 144/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.3015 - acc: 0.8777 - val_loss: 0.3504 - val_acc: 0.8453\n",
            "Epoch 145/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3009 - acc: 0.8794 - val_loss: 0.3501 - val_acc: 0.8461\n",
            "Epoch 146/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.3014 - acc: 0.8796 - val_loss: 0.3502 - val_acc: 0.8467\n",
            "Epoch 147/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.3000 - acc: 0.8773 - val_loss: 0.3503 - val_acc: 0.8469\n",
            "Epoch 148/500\n",
            "20109/20109 [==============================] - 7s 327us/sample - loss: 0.3002 - acc: 0.8793 - val_loss: 0.3501 - val_acc: 0.8459\n",
            "Epoch 149/500\n",
            "20109/20109 [==============================] - 6s 314us/sample - loss: 0.2942 - acc: 0.8827 - val_loss: 0.3489 - val_acc: 0.8477\n",
            "Epoch 150/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.2952 - acc: 0.8804 - val_loss: 0.3486 - val_acc: 0.8475\n",
            "Epoch 151/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.2962 - acc: 0.8786 - val_loss: 0.3491 - val_acc: 0.8467\n",
            "Epoch 152/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.2963 - acc: 0.8798 - val_loss: 0.3494 - val_acc: 0.8469\n",
            "Epoch 153/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.2946 - acc: 0.8809 - val_loss: 0.3487 - val_acc: 0.8477\n",
            "Epoch 154/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.2916 - acc: 0.8818 - val_loss: 0.3492 - val_acc: 0.8477\n",
            "Epoch 155/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.2945 - acc: 0.8797 - val_loss: 0.3488 - val_acc: 0.8473\n",
            "Epoch 156/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.2946 - acc: 0.8805 - val_loss: 0.3497 - val_acc: 0.8477\n",
            "Epoch 157/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.2908 - acc: 0.8836 - val_loss: 0.3489 - val_acc: 0.8463\n",
            "Epoch 00157: early stopping\n",
            "--- 1004.6905977725983 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20109/20109 [==============================] - 53s 3ms/sample - loss: 0.2750 - acc: 0.8886\n",
            "5027/5027 [==============================] - 13s 3ms/sample - loss: 0.3487 - acc: 0.8466\n",
            "Train Accuracy: 0.8886\n",
            "Train Loss: 0.2750\n",
            "Validation Accuracy:  0.8466\n",
            "Validation Loss: 0.3487\n",
            "5027/5027 [==============================] - 5s 973us/sample\n",
            "[[1039  460]\n",
            " [ 311 3217]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.69      0.73      1499\n",
            "         1.0       0.87      0.91      0.89      3528\n",
            "\n",
            "    accuracy                           0.85      5027\n",
            "   macro avg       0.82      0.80      0.81      5027\n",
            "weighted avg       0.84      0.85      0.84      5027\n",
            "\n",
            "RMSE: 0.3916\n",
            "--- Fold 3 ---\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.76.226.242:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 9991819183416820074)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16423318294591108797)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 372813063997075366)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1911706453291360047)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5192434705240319694)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5125544372300616230)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3311322538967966271)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2284163816072855620)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 16497475372141900428)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8307680946862615395)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14034584378159045674)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20109 samples, validate on 5027 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.294733047485352 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20109 [==========================>...] - ETA: 1s - loss: 0.6922 - acc: 0.5589INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.6965858936309814 secs\n",
            "19456/20109 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.5613INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 4.057218074798584 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 4.76181697845459 secs\n",
            "20109/20109 [==============================] - 43s 2ms/sample - loss: 0.6922 - acc: 0.5626 - val_loss: 0.6910 - val_acc: 0.6541\n",
            "Epoch 2/500\n",
            "20109/20109 [==============================] - 6s 314us/sample - loss: 0.6902 - acc: 0.6513 - val_loss: 0.6890 - val_acc: 0.6913\n",
            "Epoch 3/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.6881 - acc: 0.6892 - val_loss: 0.6869 - val_acc: 0.7008\n",
            "Epoch 4/500\n",
            "20109/20109 [==============================] - 6s 289us/sample - loss: 0.6858 - acc: 0.7007 - val_loss: 0.6845 - val_acc: 0.7008\n",
            "Epoch 5/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.6833 - acc: 0.7019 - val_loss: 0.6819 - val_acc: 0.7014\n",
            "Epoch 6/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.6805 - acc: 0.7020 - val_loss: 0.6789 - val_acc: 0.7016\n",
            "Epoch 7/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.6772 - acc: 0.7019 - val_loss: 0.6754 - val_acc: 0.7016\n",
            "Epoch 8/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.6735 - acc: 0.7017 - val_loss: 0.6714 - val_acc: 0.7016\n",
            "Epoch 9/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.6692 - acc: 0.7017 - val_loss: 0.6668 - val_acc: 0.7016\n",
            "Epoch 10/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.6636 - acc: 0.7018 - val_loss: 0.6612 - val_acc: 0.7016\n",
            "Epoch 11/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.6573 - acc: 0.7017 - val_loss: 0.6545 - val_acc: 0.7016\n",
            "Epoch 12/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.6498 - acc: 0.7018 - val_loss: 0.6464 - val_acc: 0.7016\n",
            "Epoch 13/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.6412 - acc: 0.7017 - val_loss: 0.6368 - val_acc: 0.7016\n",
            "Epoch 14/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.6312 - acc: 0.7017 - val_loss: 0.6259 - val_acc: 0.7016\n",
            "Epoch 15/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.6229 - acc: 0.7017 - val_loss: 0.6161 - val_acc: 0.7016\n",
            "Epoch 16/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.6164 - acc: 0.7017 - val_loss: 0.6104 - val_acc: 0.7016\n",
            "Epoch 17/500\n",
            "20109/20109 [==============================] - 6s 289us/sample - loss: 0.6138 - acc: 0.7018 - val_loss: 0.6074 - val_acc: 0.7016\n",
            "Epoch 18/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.6124 - acc: 0.7016 - val_loss: 0.6051 - val_acc: 0.7016\n",
            "Epoch 19/500\n",
            "20109/20109 [==============================] - 6s 289us/sample - loss: 0.6082 - acc: 0.7018 - val_loss: 0.6024 - val_acc: 0.7016\n",
            "Epoch 20/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.6073 - acc: 0.7017 - val_loss: 0.6004 - val_acc: 0.7016\n",
            "Epoch 21/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.6052 - acc: 0.7018 - val_loss: 0.5986 - val_acc: 0.7016\n",
            "Epoch 22/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.6022 - acc: 0.7016 - val_loss: 0.5967 - val_acc: 0.7016\n",
            "Epoch 23/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.6003 - acc: 0.7018 - val_loss: 0.5947 - val_acc: 0.7016\n",
            "Epoch 24/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.5977 - acc: 0.7017 - val_loss: 0.5927 - val_acc: 0.7016\n",
            "Epoch 25/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.5954 - acc: 0.7016 - val_loss: 0.5908 - val_acc: 0.7016\n",
            "Epoch 26/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.5932 - acc: 0.7017 - val_loss: 0.5888 - val_acc: 0.7016\n",
            "Epoch 27/500\n",
            "20109/20109 [==============================] - 6s 284us/sample - loss: 0.5917 - acc: 0.7018 - val_loss: 0.5868 - val_acc: 0.7016\n",
            "Epoch 28/500\n",
            "20109/20109 [==============================] - 6s 314us/sample - loss: 0.5893 - acc: 0.7016 - val_loss: 0.5846 - val_acc: 0.7016\n",
            "Epoch 29/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.5863 - acc: 0.7016 - val_loss: 0.5824 - val_acc: 0.7016\n",
            "Epoch 30/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.5848 - acc: 0.7016 - val_loss: 0.5802 - val_acc: 0.7016\n",
            "Epoch 31/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.5823 - acc: 0.7018 - val_loss: 0.5780 - val_acc: 0.7016\n",
            "Epoch 32/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.5792 - acc: 0.7017 - val_loss: 0.5754 - val_acc: 0.7016\n",
            "Epoch 33/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.5767 - acc: 0.7017 - val_loss: 0.5726 - val_acc: 0.7016\n",
            "Epoch 34/500\n",
            "20109/20109 [==============================] - 6s 305us/sample - loss: 0.5735 - acc: 0.7017 - val_loss: 0.5698 - val_acc: 0.7016\n",
            "Epoch 35/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.5721 - acc: 0.7016 - val_loss: 0.5670 - val_acc: 0.7016\n",
            "Epoch 36/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.5678 - acc: 0.7017 - val_loss: 0.5638 - val_acc: 0.7016\n",
            "Epoch 37/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.5641 - acc: 0.7018 - val_loss: 0.5608 - val_acc: 0.7016\n",
            "Epoch 38/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.5604 - acc: 0.7019 - val_loss: 0.5568 - val_acc: 0.7016\n",
            "Epoch 39/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.5563 - acc: 0.7019 - val_loss: 0.5529 - val_acc: 0.7016\n",
            "Epoch 40/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.5516 - acc: 0.7023 - val_loss: 0.5488 - val_acc: 0.7016\n",
            "Epoch 41/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.5492 - acc: 0.7023 - val_loss: 0.5447 - val_acc: 0.7020\n",
            "Epoch 42/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.5439 - acc: 0.7031 - val_loss: 0.5400 - val_acc: 0.7026\n",
            "Epoch 43/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.5390 - acc: 0.7036 - val_loss: 0.5353 - val_acc: 0.7032\n",
            "Epoch 44/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.5334 - acc: 0.7053 - val_loss: 0.5297 - val_acc: 0.7048\n",
            "Epoch 45/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.5290 - acc: 0.7073 - val_loss: 0.5259 - val_acc: 0.7066\n",
            "Epoch 46/500\n",
            "20109/20109 [==============================] - 7s 340us/sample - loss: 0.5234 - acc: 0.7095 - val_loss: 0.5205 - val_acc: 0.7094\n",
            "Epoch 47/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.5185 - acc: 0.7128 - val_loss: 0.5145 - val_acc: 0.7128\n",
            "Epoch 48/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.5124 - acc: 0.7183 - val_loss: 0.5095 - val_acc: 0.7172\n",
            "Epoch 49/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.5067 - acc: 0.7228 - val_loss: 0.5046 - val_acc: 0.7247\n",
            "Epoch 50/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.5039 - acc: 0.7287 - val_loss: 0.4998 - val_acc: 0.7305\n",
            "Epoch 51/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.4975 - acc: 0.7344 - val_loss: 0.4954 - val_acc: 0.7367\n",
            "Epoch 52/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.4923 - acc: 0.7422 - val_loss: 0.4887 - val_acc: 0.7410\n",
            "Epoch 53/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.4889 - acc: 0.7478 - val_loss: 0.4869 - val_acc: 0.7460\n",
            "Epoch 54/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.4831 - acc: 0.7552 - val_loss: 0.4801 - val_acc: 0.7520\n",
            "Epoch 55/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.4760 - acc: 0.7619 - val_loss: 0.4760 - val_acc: 0.7590\n",
            "Epoch 56/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.4722 - acc: 0.7701 - val_loss: 0.4711 - val_acc: 0.7645\n",
            "Epoch 57/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.4677 - acc: 0.7760 - val_loss: 0.4667 - val_acc: 0.7739\n",
            "Epoch 58/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.4621 - acc: 0.7818 - val_loss: 0.4615 - val_acc: 0.7795\n",
            "Epoch 59/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.4575 - acc: 0.7906 - val_loss: 0.4572 - val_acc: 0.7840\n",
            "Epoch 60/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.4526 - acc: 0.7949 - val_loss: 0.4525 - val_acc: 0.7894\n",
            "Epoch 61/500\n",
            "20109/20109 [==============================] - 6s 287us/sample - loss: 0.4509 - acc: 0.7988 - val_loss: 0.4494 - val_acc: 0.7940\n",
            "Epoch 62/500\n",
            "20109/20109 [==============================] - 6s 280us/sample - loss: 0.4468 - acc: 0.8020 - val_loss: 0.4455 - val_acc: 0.7982\n",
            "Epoch 63/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.4407 - acc: 0.8046 - val_loss: 0.4431 - val_acc: 0.8018\n",
            "Epoch 64/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.4342 - acc: 0.8122 - val_loss: 0.4390 - val_acc: 0.8063\n",
            "Epoch 65/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.4319 - acc: 0.8158 - val_loss: 0.4346 - val_acc: 0.8095\n",
            "Epoch 66/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.4287 - acc: 0.8177 - val_loss: 0.4317 - val_acc: 0.8129\n",
            "Epoch 67/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.4241 - acc: 0.8206 - val_loss: 0.4265 - val_acc: 0.8171\n",
            "Epoch 68/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.4158 - acc: 0.8242 - val_loss: 0.4231 - val_acc: 0.8187\n",
            "Epoch 69/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.4131 - acc: 0.8259 - val_loss: 0.4212 - val_acc: 0.8213\n",
            "Epoch 70/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.4124 - acc: 0.8300 - val_loss: 0.4164 - val_acc: 0.8215\n",
            "Epoch 71/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.4081 - acc: 0.8311 - val_loss: 0.4158 - val_acc: 0.8246\n",
            "Epoch 72/500\n",
            "20109/20109 [==============================] - 6s 320us/sample - loss: 0.4042 - acc: 0.8327 - val_loss: 0.4133 - val_acc: 0.8250\n",
            "Epoch 73/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.4004 - acc: 0.8339 - val_loss: 0.4104 - val_acc: 0.8274\n",
            "Epoch 74/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.3971 - acc: 0.8378 - val_loss: 0.4079 - val_acc: 0.8288\n",
            "Epoch 75/500\n",
            "20109/20109 [==============================] - 7s 342us/sample - loss: 0.3946 - acc: 0.8379 - val_loss: 0.4042 - val_acc: 0.8310\n",
            "Epoch 76/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.3920 - acc: 0.8384 - val_loss: 0.4032 - val_acc: 0.8318\n",
            "Epoch 77/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.3900 - acc: 0.8377 - val_loss: 0.3988 - val_acc: 0.8324\n",
            "Epoch 78/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.3861 - acc: 0.8433 - val_loss: 0.3988 - val_acc: 0.8326\n",
            "Epoch 79/500\n",
            "20109/20109 [==============================] - 6s 287us/sample - loss: 0.3830 - acc: 0.8410 - val_loss: 0.3947 - val_acc: 0.8340\n",
            "Epoch 80/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.3799 - acc: 0.8420 - val_loss: 0.3929 - val_acc: 0.8338\n",
            "Epoch 81/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.3778 - acc: 0.8454 - val_loss: 0.3920 - val_acc: 0.8344\n",
            "Epoch 82/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.3753 - acc: 0.8449 - val_loss: 0.3899 - val_acc: 0.8352\n",
            "Epoch 83/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.3723 - acc: 0.8443 - val_loss: 0.3894 - val_acc: 0.8360\n",
            "Epoch 84/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3717 - acc: 0.8452 - val_loss: 0.3867 - val_acc: 0.8354\n",
            "Epoch 85/500\n",
            "20109/20109 [==============================] - 5s 270us/sample - loss: 0.3675 - acc: 0.8478 - val_loss: 0.3853 - val_acc: 0.8372\n",
            "Epoch 86/500\n",
            "20109/20109 [==============================] - 6s 290us/sample - loss: 0.3665 - acc: 0.8477 - val_loss: 0.3842 - val_acc: 0.8372\n",
            "Epoch 87/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.3627 - acc: 0.8501 - val_loss: 0.3806 - val_acc: 0.8368\n",
            "Epoch 88/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.3591 - acc: 0.8497 - val_loss: 0.3799 - val_acc: 0.8378\n",
            "Epoch 89/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.3584 - acc: 0.8492 - val_loss: 0.3795 - val_acc: 0.8388\n",
            "Epoch 90/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.3571 - acc: 0.8506 - val_loss: 0.3804 - val_acc: 0.8374\n",
            "Epoch 91/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.3543 - acc: 0.8536 - val_loss: 0.3761 - val_acc: 0.8388\n",
            "Epoch 92/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.3536 - acc: 0.8538 - val_loss: 0.3760 - val_acc: 0.8388\n",
            "Epoch 93/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.3522 - acc: 0.8535 - val_loss: 0.3764 - val_acc: 0.8384\n",
            "Epoch 94/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.3505 - acc: 0.8539 - val_loss: 0.3755 - val_acc: 0.8378\n",
            "Epoch 95/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.3463 - acc: 0.8540 - val_loss: 0.3735 - val_acc: 0.8382\n",
            "Epoch 96/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3452 - acc: 0.8569 - val_loss: 0.3732 - val_acc: 0.8386\n",
            "Epoch 97/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.3450 - acc: 0.8584 - val_loss: 0.3720 - val_acc: 0.8386\n",
            "Epoch 98/500\n",
            "20109/20109 [==============================] - 6s 312us/sample - loss: 0.3430 - acc: 0.8579 - val_loss: 0.3717 - val_acc: 0.8388\n",
            "Epoch 99/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.3421 - acc: 0.8591 - val_loss: 0.3691 - val_acc: 0.8412\n",
            "Epoch 100/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.3385 - acc: 0.8575 - val_loss: 0.3696 - val_acc: 0.8406\n",
            "Epoch 101/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.3369 - acc: 0.8602 - val_loss: 0.3698 - val_acc: 0.8396\n",
            "Epoch 102/500\n",
            "20109/20109 [==============================] - 6s 285us/sample - loss: 0.3364 - acc: 0.8593 - val_loss: 0.3692 - val_acc: 0.8396\n",
            "Epoch 103/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.3351 - acc: 0.8617 - val_loss: 0.3684 - val_acc: 0.8402\n",
            "Epoch 104/500\n",
            "20109/20109 [==============================] - 7s 332us/sample - loss: 0.3345 - acc: 0.8631 - val_loss: 0.3672 - val_acc: 0.8416\n",
            "Epoch 105/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.3334 - acc: 0.8630 - val_loss: 0.3657 - val_acc: 0.8412\n",
            "Epoch 106/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.3323 - acc: 0.8623 - val_loss: 0.3661 - val_acc: 0.8420\n",
            "Epoch 107/500\n",
            "20109/20109 [==============================] - 6s 289us/sample - loss: 0.3332 - acc: 0.8633 - val_loss: 0.3656 - val_acc: 0.8416\n",
            "Epoch 108/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.3295 - acc: 0.8646 - val_loss: 0.3645 - val_acc: 0.8414\n",
            "Epoch 109/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.3277 - acc: 0.8625 - val_loss: 0.3641 - val_acc: 0.8418\n",
            "Epoch 110/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3248 - acc: 0.8641 - val_loss: 0.3630 - val_acc: 0.8418\n",
            "Epoch 111/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.3250 - acc: 0.8637 - val_loss: 0.3626 - val_acc: 0.8422\n",
            "Epoch 112/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.3239 - acc: 0.8671 - val_loss: 0.3623 - val_acc: 0.8418\n",
            "Epoch 113/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.3239 - acc: 0.8651 - val_loss: 0.3625 - val_acc: 0.8430\n",
            "Epoch 114/500\n",
            "20109/20109 [==============================] - 6s 284us/sample - loss: 0.3182 - acc: 0.8683 - val_loss: 0.3615 - val_acc: 0.8422\n",
            "Epoch 115/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.3223 - acc: 0.8664 - val_loss: 0.3623 - val_acc: 0.8428\n",
            "Epoch 116/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.3181 - acc: 0.8676 - val_loss: 0.3612 - val_acc: 0.8430\n",
            "Epoch 117/500\n",
            "20109/20109 [==============================] - 7s 324us/sample - loss: 0.3210 - acc: 0.8680 - val_loss: 0.3614 - val_acc: 0.8424\n",
            "Epoch 118/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.3193 - acc: 0.8678 - val_loss: 0.3604 - val_acc: 0.8434\n",
            "Epoch 119/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.3164 - acc: 0.8696 - val_loss: 0.3605 - val_acc: 0.8434\n",
            "Epoch 120/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.3170 - acc: 0.8692 - val_loss: 0.3600 - val_acc: 0.8441\n",
            "Epoch 121/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.3163 - acc: 0.8714 - val_loss: 0.3598 - val_acc: 0.8443\n",
            "Epoch 122/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.3143 - acc: 0.8732 - val_loss: 0.3594 - val_acc: 0.8451\n",
            "Epoch 123/500\n",
            "20109/20109 [==============================] - 6s 314us/sample - loss: 0.3136 - acc: 0.8719 - val_loss: 0.3593 - val_acc: 0.8443\n",
            "Epoch 124/500\n",
            "20109/20109 [==============================] - 6s 314us/sample - loss: 0.3099 - acc: 0.8723 - val_loss: 0.3589 - val_acc: 0.8461\n",
            "Epoch 125/500\n",
            "20109/20109 [==============================] - 6s 286us/sample - loss: 0.3123 - acc: 0.8725 - val_loss: 0.3588 - val_acc: 0.8453\n",
            "Epoch 126/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.3087 - acc: 0.8748 - val_loss: 0.3577 - val_acc: 0.8461\n",
            "Epoch 127/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.3097 - acc: 0.8742 - val_loss: 0.3580 - val_acc: 0.8457\n",
            "Epoch 128/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.3064 - acc: 0.8742 - val_loss: 0.3580 - val_acc: 0.8477\n",
            "Epoch 129/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.3106 - acc: 0.8722 - val_loss: 0.3576 - val_acc: 0.8471\n",
            "Epoch 130/500\n",
            "20109/20109 [==============================] - 6s 289us/sample - loss: 0.3068 - acc: 0.8764 - val_loss: 0.3573 - val_acc: 0.8483\n",
            "Epoch 131/500\n",
            "20109/20109 [==============================] - 6s 290us/sample - loss: 0.3083 - acc: 0.8744 - val_loss: 0.3568 - val_acc: 0.8477\n",
            "Epoch 132/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3047 - acc: 0.8767 - val_loss: 0.3567 - val_acc: 0.8477\n",
            "Epoch 133/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3052 - acc: 0.8740 - val_loss: 0.3567 - val_acc: 0.8493\n",
            "Epoch 134/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.3044 - acc: 0.8743 - val_loss: 0.3566 - val_acc: 0.8479\n",
            "Epoch 135/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.3035 - acc: 0.8750 - val_loss: 0.3565 - val_acc: 0.8499\n",
            "Epoch 136/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.2995 - acc: 0.8781 - val_loss: 0.3562 - val_acc: 0.8493\n",
            "Epoch 137/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.3019 - acc: 0.8778 - val_loss: 0.3563 - val_acc: 0.8501\n",
            "Epoch 138/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.2991 - acc: 0.8802 - val_loss: 0.3562 - val_acc: 0.8491\n",
            "Epoch 139/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.3004 - acc: 0.8801 - val_loss: 0.3560 - val_acc: 0.8485\n",
            "Epoch 140/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.2952 - acc: 0.8800 - val_loss: 0.3559 - val_acc: 0.8475\n",
            "Epoch 141/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.2965 - acc: 0.8796 - val_loss: 0.3558 - val_acc: 0.8481\n",
            "Epoch 142/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.2978 - acc: 0.8786 - val_loss: 0.3558 - val_acc: 0.8489\n",
            "Epoch 143/500\n",
            "20109/20109 [==============================] - 6s 285us/sample - loss: 0.2954 - acc: 0.8800 - val_loss: 0.3555 - val_acc: 0.8483\n",
            "Epoch 144/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.2972 - acc: 0.8804 - val_loss: 0.3556 - val_acc: 0.8483\n",
            "Epoch 145/500\n",
            "20109/20109 [==============================] - 6s 305us/sample - loss: 0.2950 - acc: 0.8795 - val_loss: 0.3556 - val_acc: 0.8481\n",
            "Epoch 146/500\n",
            "20109/20109 [==============================] - 6s 282us/sample - loss: 0.2963 - acc: 0.8775 - val_loss: 0.3558 - val_acc: 0.8487\n",
            "Epoch 147/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.2961 - acc: 0.8803 - val_loss: 0.3555 - val_acc: 0.8477\n",
            "Epoch 148/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.2937 - acc: 0.8816 - val_loss: 0.3554 - val_acc: 0.8481\n",
            "Epoch 149/500\n",
            "20109/20109 [==============================] - 6s 283us/sample - loss: 0.2945 - acc: 0.8809 - val_loss: 0.3554 - val_acc: 0.8475\n",
            "Epoch 150/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.2912 - acc: 0.8820 - val_loss: 0.3553 - val_acc: 0.8481\n",
            "Epoch 151/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.2919 - acc: 0.8805 - val_loss: 0.3553 - val_acc: 0.8479\n",
            "Epoch 152/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.2919 - acc: 0.8818 - val_loss: 0.3551 - val_acc: 0.8469\n",
            "Epoch 153/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.2923 - acc: 0.8798 - val_loss: 0.3551 - val_acc: 0.8475\n",
            "Epoch 154/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.2914 - acc: 0.8804 - val_loss: 0.3550 - val_acc: 0.8477\n",
            "Epoch 155/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.2904 - acc: 0.8813 - val_loss: 0.3551 - val_acc: 0.8487\n",
            "Epoch 156/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.2880 - acc: 0.8848 - val_loss: 0.3552 - val_acc: 0.8485\n",
            "Epoch 157/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.2883 - acc: 0.8853 - val_loss: 0.3550 - val_acc: 0.8479\n",
            "Epoch 158/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.2874 - acc: 0.8841 - val_loss: 0.3550 - val_acc: 0.8473\n",
            "Epoch 159/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.2879 - acc: 0.8850 - val_loss: 0.3550 - val_acc: 0.8481\n",
            "Epoch 160/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.2862 - acc: 0.8837 - val_loss: 0.3550 - val_acc: 0.8467\n",
            "Epoch 161/500\n",
            "20109/20109 [==============================] - 6s 314us/sample - loss: 0.2854 - acc: 0.8852 - val_loss: 0.3551 - val_acc: 0.8475\n",
            "Epoch 162/500\n",
            "20109/20109 [==============================] - 6s 281us/sample - loss: 0.2846 - acc: 0.8839 - val_loss: 0.3550 - val_acc: 0.8465\n",
            "Epoch 163/500\n",
            "20109/20109 [==============================] - 6s 314us/sample - loss: 0.2862 - acc: 0.8852 - val_loss: 0.3551 - val_acc: 0.8473\n",
            "Epoch 164/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.2852 - acc: 0.8850 - val_loss: 0.3551 - val_acc: 0.8473\n",
            "Epoch 165/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.2825 - acc: 0.8860 - val_loss: 0.3551 - val_acc: 0.8473\n",
            "Epoch 00165: early stopping\n",
            "--- 1045.5729913711548 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20109/20109 [==============================] - 54s 3ms/sample - loss: 0.2665 - acc: 0.8928\n",
            "5027/5027 [==============================] - 13s 3ms/sample - loss: 0.3555 - acc: 0.8468\n",
            "Train Accuracy: 0.8928\n",
            "Train Loss: 0.2665\n",
            "Validation Accuracy:  0.8468\n",
            "Validation Loss: 0.3555\n",
            "5027/5027 [==============================] - 6s 1ms/sample\n",
            "[[1055  445]\n",
            " [ 325 3202]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.70      0.73      1500\n",
            "         1.0       0.88      0.91      0.89      3527\n",
            "\n",
            "    accuracy                           0.85      5027\n",
            "   macro avg       0.82      0.81      0.81      5027\n",
            "weighted avg       0.84      0.85      0.84      5027\n",
            "\n",
            "RMSE: 0.3914\n",
            "--- Fold 4 ---\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.76.226.242:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 9991819183416820074)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16423318294591108797)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 372813063997075366)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1911706453291360047)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5192434705240319694)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5125544372300616230)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3311322538967966271)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2284163816072855620)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 16497475372141900428)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8307680946862615395)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14034584378159045674)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20109 samples, validate on 5027 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.282201528549194 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20109 [==========================>...] - ETA: 1s - loss: 0.6923 - acc: 0.5479INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 6.052133798599243 secs\n",
            "19456/20109 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.5497INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 4.222835063934326 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.0670740604400635 secs\n",
            "20109/20109 [==============================] - 44s 2ms/sample - loss: 0.6922 - acc: 0.5513 - val_loss: 0.6912 - val_acc: 0.6262\n",
            "Epoch 2/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.6903 - acc: 0.6388 - val_loss: 0.6892 - val_acc: 0.6809\n",
            "Epoch 3/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.6882 - acc: 0.6832 - val_loss: 0.6871 - val_acc: 0.6992\n",
            "Epoch 4/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.6859 - acc: 0.6996 - val_loss: 0.6848 - val_acc: 0.7002\n",
            "Epoch 5/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.6835 - acc: 0.7031 - val_loss: 0.6822 - val_acc: 0.7012\n",
            "Epoch 6/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.6807 - acc: 0.7020 - val_loss: 0.6793 - val_acc: 0.7014\n",
            "Epoch 7/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.6776 - acc: 0.7016 - val_loss: 0.6760 - val_acc: 0.7014\n",
            "Epoch 8/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.6740 - acc: 0.7018 - val_loss: 0.6721 - val_acc: 0.7014\n",
            "Epoch 9/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.6695 - acc: 0.7017 - val_loss: 0.6676 - val_acc: 0.7014\n",
            "Epoch 10/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.6647 - acc: 0.7017 - val_loss: 0.6623 - val_acc: 0.7014\n",
            "Epoch 11/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.6586 - acc: 0.7017 - val_loss: 0.6559 - val_acc: 0.7014\n",
            "Epoch 12/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.6517 - acc: 0.7017 - val_loss: 0.6483 - val_acc: 0.7014\n",
            "Epoch 13/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.6431 - acc: 0.7017 - val_loss: 0.6391 - val_acc: 0.7014\n",
            "Epoch 14/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.6332 - acc: 0.7017 - val_loss: 0.6286 - val_acc: 0.7014\n",
            "Epoch 15/500\n",
            "20109/20109 [==============================] - 6s 290us/sample - loss: 0.6239 - acc: 0.7016 - val_loss: 0.6183 - val_acc: 0.7014\n",
            "Epoch 16/500\n",
            "20109/20109 [==============================] - 6s 305us/sample - loss: 0.6177 - acc: 0.7017 - val_loss: 0.6120 - val_acc: 0.7014\n",
            "Epoch 17/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.6146 - acc: 0.7017 - val_loss: 0.6080 - val_acc: 0.7014\n",
            "Epoch 18/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.6115 - acc: 0.7016 - val_loss: 0.6054 - val_acc: 0.7014\n",
            "Epoch 19/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.6083 - acc: 0.7017 - val_loss: 0.6031 - val_acc: 0.7014\n",
            "Epoch 20/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.6053 - acc: 0.7017 - val_loss: 0.6003 - val_acc: 0.7014\n",
            "Epoch 21/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.6037 - acc: 0.7016 - val_loss: 0.5986 - val_acc: 0.7014\n",
            "Epoch 22/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.6015 - acc: 0.7018 - val_loss: 0.5965 - val_acc: 0.7014\n",
            "Epoch 23/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.5971 - acc: 0.7017 - val_loss: 0.5942 - val_acc: 0.7014\n",
            "Epoch 24/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.5967 - acc: 0.7018 - val_loss: 0.5923 - val_acc: 0.7014\n",
            "Epoch 25/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.5921 - acc: 0.7017 - val_loss: 0.5898 - val_acc: 0.7014\n",
            "Epoch 26/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.5912 - acc: 0.7017 - val_loss: 0.5876 - val_acc: 0.7014\n",
            "Epoch 27/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.5888 - acc: 0.7018 - val_loss: 0.5855 - val_acc: 0.7014\n",
            "Epoch 28/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.5870 - acc: 0.7017 - val_loss: 0.5831 - val_acc: 0.7014\n",
            "Epoch 29/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.5844 - acc: 0.7017 - val_loss: 0.5808 - val_acc: 0.7014\n",
            "Epoch 30/500\n",
            "20109/20109 [==============================] - 6s 283us/sample - loss: 0.5819 - acc: 0.7017 - val_loss: 0.5783 - val_acc: 0.7014\n",
            "Epoch 31/500\n",
            "20109/20109 [==============================] - 6s 287us/sample - loss: 0.5792 - acc: 0.7019 - val_loss: 0.5753 - val_acc: 0.7014\n",
            "Epoch 32/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.5747 - acc: 0.7017 - val_loss: 0.5726 - val_acc: 0.7014\n",
            "Epoch 33/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.5728 - acc: 0.7016 - val_loss: 0.5697 - val_acc: 0.7014\n",
            "Epoch 34/500\n",
            "20109/20109 [==============================] - 7s 329us/sample - loss: 0.5703 - acc: 0.7018 - val_loss: 0.5666 - val_acc: 0.7014\n",
            "Epoch 35/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.5656 - acc: 0.7017 - val_loss: 0.5628 - val_acc: 0.7014\n",
            "Epoch 36/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.5619 - acc: 0.7017 - val_loss: 0.5591 - val_acc: 0.7014\n",
            "Epoch 37/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.5588 - acc: 0.7017 - val_loss: 0.5554 - val_acc: 0.7016\n",
            "Epoch 38/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.5552 - acc: 0.7020 - val_loss: 0.5514 - val_acc: 0.7016\n",
            "Epoch 39/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.5509 - acc: 0.7019 - val_loss: 0.5473 - val_acc: 0.7016\n",
            "Epoch 40/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.5462 - acc: 0.7025 - val_loss: 0.5427 - val_acc: 0.7014\n",
            "Epoch 41/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.5411 - acc: 0.7034 - val_loss: 0.5372 - val_acc: 0.7016\n",
            "Epoch 42/500\n",
            "20109/20109 [==============================] - 6s 289us/sample - loss: 0.5366 - acc: 0.7041 - val_loss: 0.5333 - val_acc: 0.7022\n",
            "Epoch 43/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.5323 - acc: 0.7052 - val_loss: 0.5277 - val_acc: 0.7034\n",
            "Epoch 44/500\n",
            "20109/20109 [==============================] - 6s 314us/sample - loss: 0.5259 - acc: 0.7075 - val_loss: 0.5228 - val_acc: 0.7054\n",
            "Epoch 45/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.5231 - acc: 0.7107 - val_loss: 0.5193 - val_acc: 0.7096\n",
            "Epoch 46/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.5163 - acc: 0.7146 - val_loss: 0.5135 - val_acc: 0.7124\n",
            "Epoch 47/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.5118 - acc: 0.7190 - val_loss: 0.5083 - val_acc: 0.7162\n",
            "Epoch 48/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.5076 - acc: 0.7247 - val_loss: 0.5035 - val_acc: 0.7207\n",
            "Epoch 49/500\n",
            "20109/20109 [==============================] - 6s 277us/sample - loss: 0.5013 - acc: 0.7301 - val_loss: 0.4987 - val_acc: 0.7251\n",
            "Epoch 50/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.4964 - acc: 0.7385 - val_loss: 0.4919 - val_acc: 0.7291\n",
            "Epoch 51/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.4936 - acc: 0.7430 - val_loss: 0.4885 - val_acc: 0.7343\n",
            "Epoch 52/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.4864 - acc: 0.7491 - val_loss: 0.4837 - val_acc: 0.7428\n",
            "Epoch 53/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.4810 - acc: 0.7575 - val_loss: 0.4770 - val_acc: 0.7474\n",
            "Epoch 54/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.4761 - acc: 0.7627 - val_loss: 0.4723 - val_acc: 0.7546\n",
            "Epoch 55/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.4708 - acc: 0.7678 - val_loss: 0.4678 - val_acc: 0.7649\n",
            "Epoch 56/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.4672 - acc: 0.7785 - val_loss: 0.4625 - val_acc: 0.7715\n",
            "Epoch 57/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.4618 - acc: 0.7835 - val_loss: 0.4603 - val_acc: 0.7803\n",
            "Epoch 58/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.4583 - acc: 0.7900 - val_loss: 0.4561 - val_acc: 0.7860\n",
            "Epoch 59/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.4533 - acc: 0.7937 - val_loss: 0.4500 - val_acc: 0.7912\n",
            "Epoch 60/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.4479 - acc: 0.7985 - val_loss: 0.4460 - val_acc: 0.7982\n",
            "Epoch 61/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.4445 - acc: 0.8019 - val_loss: 0.4437 - val_acc: 0.8025\n",
            "Epoch 62/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.4413 - acc: 0.8071 - val_loss: 0.4380 - val_acc: 0.8061\n",
            "Epoch 63/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.4385 - acc: 0.8100 - val_loss: 0.4360 - val_acc: 0.8075\n",
            "Epoch 64/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.4298 - acc: 0.8136 - val_loss: 0.4281 - val_acc: 0.8109\n",
            "Epoch 65/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.4290 - acc: 0.8163 - val_loss: 0.4264 - val_acc: 0.8155\n",
            "Epoch 66/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.4245 - acc: 0.8204 - val_loss: 0.4268 - val_acc: 0.8191\n",
            "Epoch 67/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.4184 - acc: 0.8238 - val_loss: 0.4191 - val_acc: 0.8207\n",
            "Epoch 68/500\n",
            "20109/20109 [==============================] - 6s 283us/sample - loss: 0.4175 - acc: 0.8236 - val_loss: 0.4171 - val_acc: 0.8256\n",
            "Epoch 69/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.4114 - acc: 0.8276 - val_loss: 0.4117 - val_acc: 0.8284\n",
            "Epoch 70/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.4094 - acc: 0.8281 - val_loss: 0.4105 - val_acc: 0.8296\n",
            "Epoch 71/500\n",
            "20109/20109 [==============================] - 6s 314us/sample - loss: 0.4056 - acc: 0.8314 - val_loss: 0.4073 - val_acc: 0.8324\n",
            "Epoch 72/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.4050 - acc: 0.8315 - val_loss: 0.4043 - val_acc: 0.8334\n",
            "Epoch 73/500\n",
            "20109/20109 [==============================] - 6s 305us/sample - loss: 0.3996 - acc: 0.8349 - val_loss: 0.4003 - val_acc: 0.8352\n",
            "Epoch 74/500\n",
            "20109/20109 [==============================] - 6s 305us/sample - loss: 0.3949 - acc: 0.8357 - val_loss: 0.3972 - val_acc: 0.8374\n",
            "Epoch 75/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.3937 - acc: 0.8344 - val_loss: 0.3962 - val_acc: 0.8402\n",
            "Epoch 76/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.3909 - acc: 0.8379 - val_loss: 0.3928 - val_acc: 0.8402\n",
            "Epoch 77/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.3888 - acc: 0.8372 - val_loss: 0.3902 - val_acc: 0.8424\n",
            "Epoch 78/500\n",
            "20109/20109 [==============================] - 6s 279us/sample - loss: 0.3856 - acc: 0.8364 - val_loss: 0.3876 - val_acc: 0.8424\n",
            "Epoch 79/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.3838 - acc: 0.8397 - val_loss: 0.3850 - val_acc: 0.8438\n",
            "Epoch 80/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.3807 - acc: 0.8424 - val_loss: 0.3843 - val_acc: 0.8426\n",
            "Epoch 81/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.3773 - acc: 0.8409 - val_loss: 0.3823 - val_acc: 0.8426\n",
            "Epoch 82/500\n",
            "20109/20109 [==============================] - 6s 290us/sample - loss: 0.3752 - acc: 0.8420 - val_loss: 0.3799 - val_acc: 0.8430\n",
            "Epoch 83/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.3744 - acc: 0.8423 - val_loss: 0.3796 - val_acc: 0.8430\n",
            "Epoch 84/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.3701 - acc: 0.8444 - val_loss: 0.3762 - val_acc: 0.8443\n",
            "Epoch 85/500\n",
            "20109/20109 [==============================] - 7s 323us/sample - loss: 0.3684 - acc: 0.8458 - val_loss: 0.3743 - val_acc: 0.8447\n",
            "Epoch 86/500\n",
            "20109/20109 [==============================] - 6s 312us/sample - loss: 0.3628 - acc: 0.8483 - val_loss: 0.3717 - val_acc: 0.8441\n",
            "Epoch 87/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.3634 - acc: 0.8472 - val_loss: 0.3716 - val_acc: 0.8447\n",
            "Epoch 88/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.3605 - acc: 0.8488 - val_loss: 0.3698 - val_acc: 0.8453\n",
            "Epoch 89/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.3588 - acc: 0.8472 - val_loss: 0.3694 - val_acc: 0.8453\n",
            "Epoch 90/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.3600 - acc: 0.8488 - val_loss: 0.3684 - val_acc: 0.8453\n",
            "Epoch 91/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.3567 - acc: 0.8485 - val_loss: 0.3684 - val_acc: 0.8457\n",
            "Epoch 92/500\n",
            "20109/20109 [==============================] - 6s 314us/sample - loss: 0.3545 - acc: 0.8505 - val_loss: 0.3665 - val_acc: 0.8469\n",
            "Epoch 93/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.3533 - acc: 0.8525 - val_loss: 0.3642 - val_acc: 0.8475\n",
            "Epoch 94/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.3514 - acc: 0.8503 - val_loss: 0.3633 - val_acc: 0.8483\n",
            "Epoch 95/500\n",
            "20109/20109 [==============================] - 6s 280us/sample - loss: 0.3490 - acc: 0.8521 - val_loss: 0.3629 - val_acc: 0.8483\n",
            "Epoch 96/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3470 - acc: 0.8536 - val_loss: 0.3622 - val_acc: 0.8495\n",
            "Epoch 97/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.3470 - acc: 0.8562 - val_loss: 0.3601 - val_acc: 0.8493\n",
            "Epoch 98/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.3433 - acc: 0.8558 - val_loss: 0.3591 - val_acc: 0.8507\n",
            "Epoch 99/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.3412 - acc: 0.8582 - val_loss: 0.3584 - val_acc: 0.8509\n",
            "Epoch 100/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.3420 - acc: 0.8563 - val_loss: 0.3581 - val_acc: 0.8519\n",
            "Epoch 101/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.3398 - acc: 0.8568 - val_loss: 0.3565 - val_acc: 0.8515\n",
            "Epoch 102/500\n",
            "20109/20109 [==============================] - 6s 320us/sample - loss: 0.3393 - acc: 0.8573 - val_loss: 0.3562 - val_acc: 0.8515\n",
            "Epoch 103/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3376 - acc: 0.8585 - val_loss: 0.3562 - val_acc: 0.8515\n",
            "Epoch 104/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.3338 - acc: 0.8606 - val_loss: 0.3543 - val_acc: 0.8533\n",
            "Epoch 105/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.3346 - acc: 0.8607 - val_loss: 0.3547 - val_acc: 0.8525\n",
            "Epoch 106/500\n",
            "20109/20109 [==============================] - 6s 305us/sample - loss: 0.3339 - acc: 0.8596 - val_loss: 0.3547 - val_acc: 0.8529\n",
            "Epoch 107/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.3327 - acc: 0.8603 - val_loss: 0.3535 - val_acc: 0.8535\n",
            "Epoch 108/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.3306 - acc: 0.8611 - val_loss: 0.3527 - val_acc: 0.8527\n",
            "Epoch 109/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.3290 - acc: 0.8625 - val_loss: 0.3528 - val_acc: 0.8537\n",
            "Epoch 110/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3271 - acc: 0.8645 - val_loss: 0.3515 - val_acc: 0.8541\n",
            "Epoch 111/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.3290 - acc: 0.8616 - val_loss: 0.3512 - val_acc: 0.8543\n",
            "Epoch 112/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.3252 - acc: 0.8639 - val_loss: 0.3507 - val_acc: 0.8549\n",
            "Epoch 113/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.3264 - acc: 0.8623 - val_loss: 0.3503 - val_acc: 0.8547\n",
            "Epoch 114/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.3234 - acc: 0.8658 - val_loss: 0.3494 - val_acc: 0.8549\n",
            "Epoch 115/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.3221 - acc: 0.8669 - val_loss: 0.3492 - val_acc: 0.8551\n",
            "Epoch 116/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.3222 - acc: 0.8661 - val_loss: 0.3490 - val_acc: 0.8559\n",
            "Epoch 117/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.3211 - acc: 0.8660 - val_loss: 0.3480 - val_acc: 0.8563\n",
            "Epoch 118/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3236 - acc: 0.8661 - val_loss: 0.3488 - val_acc: 0.8553\n",
            "Epoch 119/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.3195 - acc: 0.8681 - val_loss: 0.3484 - val_acc: 0.8553\n",
            "Epoch 120/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.3178 - acc: 0.8694 - val_loss: 0.3471 - val_acc: 0.8557\n",
            "Epoch 121/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.3150 - acc: 0.8679 - val_loss: 0.3470 - val_acc: 0.8547\n",
            "Epoch 122/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.3164 - acc: 0.8687 - val_loss: 0.3467 - val_acc: 0.8543\n",
            "Epoch 123/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.3127 - acc: 0.8706 - val_loss: 0.3464 - val_acc: 0.8555\n",
            "Epoch 124/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.3149 - acc: 0.8709 - val_loss: 0.3464 - val_acc: 0.8553\n",
            "Epoch 125/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.3117 - acc: 0.8705 - val_loss: 0.3458 - val_acc: 0.8549\n",
            "Epoch 126/500\n",
            "20109/20109 [==============================] - 6s 285us/sample - loss: 0.3127 - acc: 0.8707 - val_loss: 0.3455 - val_acc: 0.8551\n",
            "Epoch 127/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.3107 - acc: 0.8727 - val_loss: 0.3451 - val_acc: 0.8555\n",
            "Epoch 128/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.3128 - acc: 0.8732 - val_loss: 0.3453 - val_acc: 0.8555\n",
            "Epoch 129/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.3104 - acc: 0.8721 - val_loss: 0.3445 - val_acc: 0.8571\n",
            "Epoch 130/500\n",
            "20109/20109 [==============================] - 6s 286us/sample - loss: 0.3103 - acc: 0.8710 - val_loss: 0.3445 - val_acc: 0.8569\n",
            "Epoch 131/500\n",
            "20109/20109 [==============================] - 6s 289us/sample - loss: 0.3095 - acc: 0.8722 - val_loss: 0.3449 - val_acc: 0.8557\n",
            "Epoch 132/500\n",
            "20109/20109 [==============================] - 6s 289us/sample - loss: 0.3065 - acc: 0.8727 - val_loss: 0.3436 - val_acc: 0.8577\n",
            "Epoch 133/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.3065 - acc: 0.8751 - val_loss: 0.3438 - val_acc: 0.8565\n",
            "Epoch 134/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.3073 - acc: 0.8737 - val_loss: 0.3438 - val_acc: 0.8579\n",
            "Epoch 135/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3072 - acc: 0.8729 - val_loss: 0.3438 - val_acc: 0.8577\n",
            "Epoch 136/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.3056 - acc: 0.8758 - val_loss: 0.3437 - val_acc: 0.8573\n",
            "Epoch 137/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.3044 - acc: 0.8761 - val_loss: 0.3433 - val_acc: 0.8575\n",
            "Epoch 138/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.3031 - acc: 0.8755 - val_loss: 0.3431 - val_acc: 0.8571\n",
            "Epoch 139/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3020 - acc: 0.8772 - val_loss: 0.3427 - val_acc: 0.8579\n",
            "Epoch 140/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.3013 - acc: 0.8760 - val_loss: 0.3426 - val_acc: 0.8587\n",
            "Epoch 141/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.2975 - acc: 0.8780 - val_loss: 0.3423 - val_acc: 0.8585\n",
            "Epoch 142/500\n",
            "20109/20109 [==============================] - 6s 312us/sample - loss: 0.3005 - acc: 0.8785 - val_loss: 0.3430 - val_acc: 0.8567\n",
            "Epoch 143/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.3015 - acc: 0.8751 - val_loss: 0.3426 - val_acc: 0.8573\n",
            "Epoch 144/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.2991 - acc: 0.8771 - val_loss: 0.3428 - val_acc: 0.8571\n",
            "Epoch 145/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.2960 - acc: 0.8799 - val_loss: 0.3424 - val_acc: 0.8577\n",
            "Epoch 146/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.2984 - acc: 0.8792 - val_loss: 0.3426 - val_acc: 0.8565\n",
            "Epoch 147/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.2993 - acc: 0.8787 - val_loss: 0.3422 - val_acc: 0.8573\n",
            "Epoch 148/500\n",
            "20109/20109 [==============================] - 6s 283us/sample - loss: 0.2942 - acc: 0.8810 - val_loss: 0.3422 - val_acc: 0.8579\n",
            "Epoch 149/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.2947 - acc: 0.8788 - val_loss: 0.3422 - val_acc: 0.8575\n",
            "Epoch 150/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.2938 - acc: 0.8784 - val_loss: 0.3419 - val_acc: 0.8577\n",
            "Epoch 151/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.2941 - acc: 0.8807 - val_loss: 0.3417 - val_acc: 0.8581\n",
            "Epoch 152/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.2935 - acc: 0.8811 - val_loss: 0.3416 - val_acc: 0.8585\n",
            "Epoch 153/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.2923 - acc: 0.8804 - val_loss: 0.3417 - val_acc: 0.8575\n",
            "Epoch 154/500\n",
            "20109/20109 [==============================] - 6s 314us/sample - loss: 0.2897 - acc: 0.8829 - val_loss: 0.3415 - val_acc: 0.8587\n",
            "Epoch 155/500\n",
            "20109/20109 [==============================] - 7s 335us/sample - loss: 0.2927 - acc: 0.8821 - val_loss: 0.3416 - val_acc: 0.8575\n",
            "Epoch 156/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.2920 - acc: 0.8812 - val_loss: 0.3416 - val_acc: 0.8569\n",
            "Epoch 157/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.2893 - acc: 0.8813 - val_loss: 0.3414 - val_acc: 0.8593\n",
            "Epoch 158/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.2925 - acc: 0.8835 - val_loss: 0.3415 - val_acc: 0.8579\n",
            "Epoch 159/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.2911 - acc: 0.8816 - val_loss: 0.3416 - val_acc: 0.8571\n",
            "Epoch 160/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.2921 - acc: 0.8802 - val_loss: 0.3415 - val_acc: 0.8579\n",
            "Epoch 161/500\n",
            "20109/20109 [==============================] - 6s 282us/sample - loss: 0.2884 - acc: 0.8838 - val_loss: 0.3414 - val_acc: 0.8577\n",
            "Epoch 162/500\n",
            "20109/20109 [==============================] - 7s 325us/sample - loss: 0.2887 - acc: 0.8842 - val_loss: 0.3414 - val_acc: 0.8571\n",
            "Epoch 163/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.2870 - acc: 0.8852 - val_loss: 0.3414 - val_acc: 0.8567\n",
            "Epoch 164/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.2879 - acc: 0.8849 - val_loss: 0.3415 - val_acc: 0.8553\n",
            "Epoch 165/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.2883 - acc: 0.8815 - val_loss: 0.3417 - val_acc: 0.8545\n",
            "Epoch 166/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.2833 - acc: 0.8857 - val_loss: 0.3418 - val_acc: 0.8555\n",
            "Epoch 167/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.2840 - acc: 0.8850 - val_loss: 0.3414 - val_acc: 0.8557\n",
            "Epoch 168/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.2837 - acc: 0.8858 - val_loss: 0.3414 - val_acc: 0.8565\n",
            "Epoch 00168: early stopping\n",
            "--- 1067.1315701007843 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20109/20109 [==============================] - 54s 3ms/sample - loss: 0.2665 - acc: 0.8930\n",
            "5027/5027 [==============================] - 13s 3ms/sample - loss: 0.3413 - acc: 0.8566\n",
            "Train Accuracy: 0.8930\n",
            "Train Loss: 0.2665\n",
            "Validation Accuracy:  0.8566\n",
            "Validation Loss: 0.3413\n",
            "5027/5027 [==============================] - 6s 1ms/sample\n",
            "[[1073  427]\n",
            " [ 294 3233]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.72      0.75      1500\n",
            "         1.0       0.88      0.92      0.90      3527\n",
            "\n",
            "    accuracy                           0.86      5027\n",
            "   macro avg       0.83      0.82      0.82      5027\n",
            "weighted avg       0.85      0.86      0.85      5027\n",
            "\n",
            "RMSE: 0.3787\n",
            "--- Fold 5 ---\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.76.226.242:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 9991819183416820074)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16423318294591108797)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 372813063997075366)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1911706453291360047)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5192434705240319694)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5125544372300616230)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3311322538967966271)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2284163816072855620)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 16497475372141900428)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8307680946862615395)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14034584378159045674)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 20109 samples, validate on 5027 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.440202474594116 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "18432/20109 [==========================>...] - ETA: 1s - loss: 0.6923 - acc: 0.5427INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(81,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(81, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(81, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.661697864532471 secs\n",
            "19456/20109 [============================>.] - ETA: 0s - loss: 0.6923 - acc: 0.5444INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 4.229448080062866 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(116,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(116, 500), dtype=tf.float32, name='Input_10'), TensorSpec(shape=(116, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for Input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.021766662597656 secs\n",
            "20109/20109 [==============================] - 43s 2ms/sample - loss: 0.6923 - acc: 0.5461 - val_loss: 0.6912 - val_acc: 0.6182\n",
            "Epoch 2/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.6901 - acc: 0.6414 - val_loss: 0.6891 - val_acc: 0.6793\n",
            "Epoch 3/500\n",
            "20109/20109 [==============================] - 6s 312us/sample - loss: 0.6880 - acc: 0.6833 - val_loss: 0.6868 - val_acc: 0.6980\n",
            "Epoch 4/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.6856 - acc: 0.6974 - val_loss: 0.6843 - val_acc: 0.7008\n",
            "Epoch 5/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.6827 - acc: 0.7017 - val_loss: 0.6814 - val_acc: 0.7016\n",
            "Epoch 6/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.6796 - acc: 0.7018 - val_loss: 0.6781 - val_acc: 0.7018\n",
            "Epoch 7/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.6762 - acc: 0.7017 - val_loss: 0.6744 - val_acc: 0.7018\n",
            "Epoch 8/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.6720 - acc: 0.7017 - val_loss: 0.6700 - val_acc: 0.7018\n",
            "Epoch 9/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.6672 - acc: 0.7016 - val_loss: 0.6649 - val_acc: 0.7018\n",
            "Epoch 10/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.6616 - acc: 0.7018 - val_loss: 0.6589 - val_acc: 0.7018\n",
            "Epoch 11/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.6548 - acc: 0.7018 - val_loss: 0.6518 - val_acc: 0.7018\n",
            "Epoch 12/500\n",
            "20109/20109 [==============================] - 6s 277us/sample - loss: 0.6471 - acc: 0.7017 - val_loss: 0.6434 - val_acc: 0.7018\n",
            "Epoch 13/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.6381 - acc: 0.7017 - val_loss: 0.6337 - val_acc: 0.7018\n",
            "Epoch 14/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.6284 - acc: 0.7017 - val_loss: 0.6233 - val_acc: 0.7018\n",
            "Epoch 15/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.6207 - acc: 0.7017 - val_loss: 0.6156 - val_acc: 0.7018\n",
            "Epoch 16/500\n",
            "20109/20109 [==============================] - 6s 290us/sample - loss: 0.6170 - acc: 0.7017 - val_loss: 0.6115 - val_acc: 0.7018\n",
            "Epoch 17/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.6119 - acc: 0.7017 - val_loss: 0.6080 - val_acc: 0.7018\n",
            "Epoch 18/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.6095 - acc: 0.7018 - val_loss: 0.6057 - val_acc: 0.7018\n",
            "Epoch 19/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.6094 - acc: 0.7017 - val_loss: 0.6035 - val_acc: 0.7018\n",
            "Epoch 20/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.6053 - acc: 0.7016 - val_loss: 0.6014 - val_acc: 0.7018\n",
            "Epoch 21/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.6034 - acc: 0.7017 - val_loss: 0.5994 - val_acc: 0.7018\n",
            "Epoch 22/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.5997 - acc: 0.7017 - val_loss: 0.5972 - val_acc: 0.7018\n",
            "Epoch 23/500\n",
            "20109/20109 [==============================] - 6s 280us/sample - loss: 0.5983 - acc: 0.7018 - val_loss: 0.5952 - val_acc: 0.7018\n",
            "Epoch 24/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.5964 - acc: 0.7017 - val_loss: 0.5933 - val_acc: 0.7018\n",
            "Epoch 25/500\n",
            "20109/20109 [==============================] - 6s 321us/sample - loss: 0.5947 - acc: 0.7017 - val_loss: 0.5917 - val_acc: 0.7018\n",
            "Epoch 26/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.5933 - acc: 0.7017 - val_loss: 0.5896 - val_acc: 0.7018\n",
            "Epoch 27/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.5903 - acc: 0.7018 - val_loss: 0.5874 - val_acc: 0.7018\n",
            "Epoch 28/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.5884 - acc: 0.7017 - val_loss: 0.5856 - val_acc: 0.7018\n",
            "Epoch 29/500\n",
            "20109/20109 [==============================] - 6s 276us/sample - loss: 0.5857 - acc: 0.7018 - val_loss: 0.5832 - val_acc: 0.7018\n",
            "Epoch 30/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.5829 - acc: 0.7016 - val_loss: 0.5807 - val_acc: 0.7018\n",
            "Epoch 31/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.5808 - acc: 0.7017 - val_loss: 0.5783 - val_acc: 0.7018\n",
            "Epoch 32/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.5769 - acc: 0.7017 - val_loss: 0.5757 - val_acc: 0.7018\n",
            "Epoch 33/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.5740 - acc: 0.7017 - val_loss: 0.5726 - val_acc: 0.7018\n",
            "Epoch 34/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.5717 - acc: 0.7017 - val_loss: 0.5699 - val_acc: 0.7018\n",
            "Epoch 35/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.5695 - acc: 0.7017 - val_loss: 0.5667 - val_acc: 0.7018\n",
            "Epoch 36/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.5659 - acc: 0.7017 - val_loss: 0.5634 - val_acc: 0.7018\n",
            "Epoch 37/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.5616 - acc: 0.7016 - val_loss: 0.5594 - val_acc: 0.7018\n",
            "Epoch 38/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.5591 - acc: 0.7017 - val_loss: 0.5557 - val_acc: 0.7020\n",
            "Epoch 39/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.5553 - acc: 0.7016 - val_loss: 0.5519 - val_acc: 0.7020\n",
            "Epoch 40/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.5495 - acc: 0.7020 - val_loss: 0.5476 - val_acc: 0.7022\n",
            "Epoch 41/500\n",
            "20109/20109 [==============================] - 6s 290us/sample - loss: 0.5467 - acc: 0.7025 - val_loss: 0.5431 - val_acc: 0.7022\n",
            "Epoch 42/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.5413 - acc: 0.7029 - val_loss: 0.5386 - val_acc: 0.7028\n",
            "Epoch 43/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.5380 - acc: 0.7037 - val_loss: 0.5331 - val_acc: 0.7038\n",
            "Epoch 44/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.5320 - acc: 0.7047 - val_loss: 0.5280 - val_acc: 0.7042\n",
            "Epoch 45/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.5267 - acc: 0.7073 - val_loss: 0.5230 - val_acc: 0.7066\n",
            "Epoch 46/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.5216 - acc: 0.7103 - val_loss: 0.5173 - val_acc: 0.7084\n",
            "Epoch 47/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.5149 - acc: 0.7135 - val_loss: 0.5118 - val_acc: 0.7120\n",
            "Epoch 48/500\n",
            "20109/20109 [==============================] - 6s 305us/sample - loss: 0.5111 - acc: 0.7188 - val_loss: 0.5060 - val_acc: 0.7144\n",
            "Epoch 49/500\n",
            "20109/20109 [==============================] - 6s 305us/sample - loss: 0.5056 - acc: 0.7232 - val_loss: 0.5009 - val_acc: 0.7193\n",
            "Epoch 50/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.5007 - acc: 0.7288 - val_loss: 0.4957 - val_acc: 0.7255\n",
            "Epoch 51/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.4945 - acc: 0.7381 - val_loss: 0.4883 - val_acc: 0.7335\n",
            "Epoch 52/500\n",
            "20109/20109 [==============================] - 6s 312us/sample - loss: 0.4862 - acc: 0.7438 - val_loss: 0.4826 - val_acc: 0.7404\n",
            "Epoch 53/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.4837 - acc: 0.7503 - val_loss: 0.4787 - val_acc: 0.7480\n",
            "Epoch 54/500\n",
            "20109/20109 [==============================] - 6s 279us/sample - loss: 0.4780 - acc: 0.7580 - val_loss: 0.4714 - val_acc: 0.7544\n",
            "Epoch 55/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.4710 - acc: 0.7647 - val_loss: 0.4664 - val_acc: 0.7645\n",
            "Epoch 56/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.4673 - acc: 0.7717 - val_loss: 0.4635 - val_acc: 0.7743\n",
            "Epoch 57/500\n",
            "20109/20109 [==============================] - 6s 288us/sample - loss: 0.4628 - acc: 0.7789 - val_loss: 0.4568 - val_acc: 0.7814\n",
            "Epoch 58/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.4547 - acc: 0.7872 - val_loss: 0.4520 - val_acc: 0.7924\n",
            "Epoch 59/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.4542 - acc: 0.7957 - val_loss: 0.4481 - val_acc: 0.7972\n",
            "Epoch 60/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.4467 - acc: 0.7996 - val_loss: 0.4433 - val_acc: 0.8012\n",
            "Epoch 61/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.4441 - acc: 0.8014 - val_loss: 0.4399 - val_acc: 0.8043\n",
            "Epoch 62/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.4372 - acc: 0.8101 - val_loss: 0.4349 - val_acc: 0.8069\n",
            "Epoch 63/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.4339 - acc: 0.8110 - val_loss: 0.4312 - val_acc: 0.8083\n",
            "Epoch 64/500\n",
            "20109/20109 [==============================] - 6s 319us/sample - loss: 0.4276 - acc: 0.8152 - val_loss: 0.4238 - val_acc: 0.8109\n",
            "Epoch 65/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.4226 - acc: 0.8194 - val_loss: 0.4198 - val_acc: 0.8137\n",
            "Epoch 66/500\n",
            "20109/20109 [==============================] - 6s 283us/sample - loss: 0.4210 - acc: 0.8212 - val_loss: 0.4187 - val_acc: 0.8175\n",
            "Epoch 67/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.4164 - acc: 0.8241 - val_loss: 0.4128 - val_acc: 0.8187\n",
            "Epoch 68/500\n",
            "20109/20109 [==============================] - 6s 287us/sample - loss: 0.4139 - acc: 0.8251 - val_loss: 0.4122 - val_acc: 0.8217\n",
            "Epoch 69/500\n",
            "20109/20109 [==============================] - 6s 283us/sample - loss: 0.4105 - acc: 0.8318 - val_loss: 0.4074 - val_acc: 0.8223\n",
            "Epoch 70/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.4110 - acc: 0.8300 - val_loss: 0.4043 - val_acc: 0.8232\n",
            "Epoch 71/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.4053 - acc: 0.8329 - val_loss: 0.4037 - val_acc: 0.8264\n",
            "Epoch 72/500\n",
            "20109/20109 [==============================] - 6s 287us/sample - loss: 0.4000 - acc: 0.8339 - val_loss: 0.3985 - val_acc: 0.8256\n",
            "Epoch 73/500\n",
            "20109/20109 [==============================] - 6s 281us/sample - loss: 0.3972 - acc: 0.8356 - val_loss: 0.3983 - val_acc: 0.8260\n",
            "Epoch 74/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3957 - acc: 0.8360 - val_loss: 0.3942 - val_acc: 0.8278\n",
            "Epoch 75/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.3907 - acc: 0.8377 - val_loss: 0.3914 - val_acc: 0.8272\n",
            "Epoch 76/500\n",
            "20109/20109 [==============================] - 6s 286us/sample - loss: 0.3888 - acc: 0.8375 - val_loss: 0.3881 - val_acc: 0.8274\n",
            "Epoch 77/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.3860 - acc: 0.8393 - val_loss: 0.3862 - val_acc: 0.8306\n",
            "Epoch 78/500\n",
            "20109/20109 [==============================] - 6s 287us/sample - loss: 0.3835 - acc: 0.8413 - val_loss: 0.3850 - val_acc: 0.8306\n",
            "Epoch 79/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.3812 - acc: 0.8429 - val_loss: 0.3822 - val_acc: 0.8318\n",
            "Epoch 80/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.3786 - acc: 0.8410 - val_loss: 0.3794 - val_acc: 0.8326\n",
            "Epoch 81/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.3747 - acc: 0.8402 - val_loss: 0.3777 - val_acc: 0.8358\n",
            "Epoch 82/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.3701 - acc: 0.8436 - val_loss: 0.3744 - val_acc: 0.8356\n",
            "Epoch 83/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.3699 - acc: 0.8475 - val_loss: 0.3738 - val_acc: 0.8358\n",
            "Epoch 84/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.3676 - acc: 0.8456 - val_loss: 0.3715 - val_acc: 0.8348\n",
            "Epoch 85/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.3675 - acc: 0.8472 - val_loss: 0.3699 - val_acc: 0.8360\n",
            "Epoch 86/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.3649 - acc: 0.8475 - val_loss: 0.3691 - val_acc: 0.8360\n",
            "Epoch 87/500\n",
            "20109/20109 [==============================] - 6s 287us/sample - loss: 0.3613 - acc: 0.8483 - val_loss: 0.3672 - val_acc: 0.8358\n",
            "Epoch 88/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.3601 - acc: 0.8467 - val_loss: 0.3648 - val_acc: 0.8364\n",
            "Epoch 89/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.3581 - acc: 0.8490 - val_loss: 0.3655 - val_acc: 0.8366\n",
            "Epoch 90/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.3554 - acc: 0.8510 - val_loss: 0.3634 - val_acc: 0.8370\n",
            "Epoch 91/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.3549 - acc: 0.8508 - val_loss: 0.3639 - val_acc: 0.8394\n",
            "Epoch 92/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.3528 - acc: 0.8535 - val_loss: 0.3621 - val_acc: 0.8386\n",
            "Epoch 93/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.3512 - acc: 0.8520 - val_loss: 0.3601 - val_acc: 0.8398\n",
            "Epoch 94/500\n",
            "20109/20109 [==============================] - 6s 294us/sample - loss: 0.3493 - acc: 0.8539 - val_loss: 0.3598 - val_acc: 0.8396\n",
            "Epoch 95/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.3484 - acc: 0.8521 - val_loss: 0.3600 - val_acc: 0.8412\n",
            "Epoch 96/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.3458 - acc: 0.8567 - val_loss: 0.3583 - val_acc: 0.8416\n",
            "Epoch 97/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.3477 - acc: 0.8540 - val_loss: 0.3587 - val_acc: 0.8420\n",
            "Epoch 98/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.3436 - acc: 0.8547 - val_loss: 0.3565 - val_acc: 0.8434\n",
            "Epoch 99/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.3445 - acc: 0.8561 - val_loss: 0.3560 - val_acc: 0.8438\n",
            "Epoch 100/500\n",
            "20109/20109 [==============================] - 6s 286us/sample - loss: 0.3396 - acc: 0.8577 - val_loss: 0.3543 - val_acc: 0.8434\n",
            "Epoch 101/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.3390 - acc: 0.8588 - val_loss: 0.3541 - val_acc: 0.8439\n",
            "Epoch 102/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.3391 - acc: 0.8551 - val_loss: 0.3544 - val_acc: 0.8438\n",
            "Epoch 103/500\n",
            "20109/20109 [==============================] - 7s 331us/sample - loss: 0.3378 - acc: 0.8596 - val_loss: 0.3524 - val_acc: 0.8453\n",
            "Epoch 104/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.3351 - acc: 0.8602 - val_loss: 0.3517 - val_acc: 0.8459\n",
            "Epoch 105/500\n",
            "20109/20109 [==============================] - 6s 305us/sample - loss: 0.3353 - acc: 0.8585 - val_loss: 0.3526 - val_acc: 0.8453\n",
            "Epoch 106/500\n",
            "20109/20109 [==============================] - 6s 284us/sample - loss: 0.3335 - acc: 0.8599 - val_loss: 0.3513 - val_acc: 0.8451\n",
            "Epoch 107/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.3296 - acc: 0.8643 - val_loss: 0.3514 - val_acc: 0.8459\n",
            "Epoch 108/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.3286 - acc: 0.8647 - val_loss: 0.3491 - val_acc: 0.8475\n",
            "Epoch 109/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.3291 - acc: 0.8627 - val_loss: 0.3487 - val_acc: 0.8479\n",
            "Epoch 110/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.3285 - acc: 0.8646 - val_loss: 0.3498 - val_acc: 0.8467\n",
            "Epoch 111/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.3259 - acc: 0.8652 - val_loss: 0.3481 - val_acc: 0.8473\n",
            "Epoch 112/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.3253 - acc: 0.8652 - val_loss: 0.3475 - val_acc: 0.8477\n",
            "Epoch 113/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.3244 - acc: 0.8653 - val_loss: 0.3468 - val_acc: 0.8479\n",
            "Epoch 114/500\n",
            "20109/20109 [==============================] - 6s 308us/sample - loss: 0.3225 - acc: 0.8664 - val_loss: 0.3463 - val_acc: 0.8481\n",
            "Epoch 115/500\n",
            "20109/20109 [==============================] - 7s 334us/sample - loss: 0.3203 - acc: 0.8661 - val_loss: 0.3466 - val_acc: 0.8477\n",
            "Epoch 116/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.3212 - acc: 0.8675 - val_loss: 0.3465 - val_acc: 0.8475\n",
            "Epoch 117/500\n",
            "20109/20109 [==============================] - 6s 315us/sample - loss: 0.3221 - acc: 0.8664 - val_loss: 0.3454 - val_acc: 0.8485\n",
            "Epoch 118/500\n",
            "20109/20109 [==============================] - 6s 311us/sample - loss: 0.3216 - acc: 0.8689 - val_loss: 0.3453 - val_acc: 0.8481\n",
            "Epoch 119/500\n",
            "20109/20109 [==============================] - 6s 307us/sample - loss: 0.3193 - acc: 0.8690 - val_loss: 0.3455 - val_acc: 0.8485\n",
            "Epoch 120/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.3207 - acc: 0.8700 - val_loss: 0.3459 - val_acc: 0.8485\n",
            "Epoch 121/500\n",
            "20109/20109 [==============================] - 6s 295us/sample - loss: 0.3168 - acc: 0.8692 - val_loss: 0.3439 - val_acc: 0.8487\n",
            "Epoch 122/500\n",
            "20109/20109 [==============================] - 7s 326us/sample - loss: 0.3164 - acc: 0.8699 - val_loss: 0.3438 - val_acc: 0.8487\n",
            "Epoch 123/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.3142 - acc: 0.8684 - val_loss: 0.3439 - val_acc: 0.8495\n",
            "Epoch 124/500\n",
            "20109/20109 [==============================] - 6s 312us/sample - loss: 0.3141 - acc: 0.8706 - val_loss: 0.3427 - val_acc: 0.8509\n",
            "Epoch 125/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.3135 - acc: 0.8704 - val_loss: 0.3427 - val_acc: 0.8505\n",
            "Epoch 126/500\n",
            "20109/20109 [==============================] - 6s 279us/sample - loss: 0.3115 - acc: 0.8710 - val_loss: 0.3420 - val_acc: 0.8525\n",
            "Epoch 127/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.3107 - acc: 0.8718 - val_loss: 0.3426 - val_acc: 0.8495\n",
            "Epoch 128/500\n",
            "20109/20109 [==============================] - 6s 289us/sample - loss: 0.3111 - acc: 0.8750 - val_loss: 0.3415 - val_acc: 0.8519\n",
            "Epoch 129/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.3105 - acc: 0.8745 - val_loss: 0.3414 - val_acc: 0.8523\n",
            "Epoch 130/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.3105 - acc: 0.8726 - val_loss: 0.3414 - val_acc: 0.8523\n",
            "Epoch 131/500\n",
            "20109/20109 [==============================] - 7s 328us/sample - loss: 0.3061 - acc: 0.8739 - val_loss: 0.3405 - val_acc: 0.8529\n",
            "Epoch 132/500\n",
            "20109/20109 [==============================] - 6s 316us/sample - loss: 0.3070 - acc: 0.8733 - val_loss: 0.3404 - val_acc: 0.8535\n",
            "Epoch 133/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.3073 - acc: 0.8750 - val_loss: 0.3409 - val_acc: 0.8521\n",
            "Epoch 134/500\n",
            "20109/20109 [==============================] - 6s 283us/sample - loss: 0.3075 - acc: 0.8752 - val_loss: 0.3404 - val_acc: 0.8531\n",
            "Epoch 135/500\n",
            "20109/20109 [==============================] - 6s 309us/sample - loss: 0.3054 - acc: 0.8745 - val_loss: 0.3403 - val_acc: 0.8525\n",
            "Epoch 136/500\n",
            "20109/20109 [==============================] - 6s 275us/sample - loss: 0.3071 - acc: 0.8740 - val_loss: 0.3405 - val_acc: 0.8527\n",
            "Epoch 137/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.3033 - acc: 0.8752 - val_loss: 0.3396 - val_acc: 0.8545\n",
            "Epoch 138/500\n",
            "20109/20109 [==============================] - 6s 285us/sample - loss: 0.3022 - acc: 0.8773 - val_loss: 0.3400 - val_acc: 0.8537\n",
            "Epoch 139/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.3026 - acc: 0.8770 - val_loss: 0.3397 - val_acc: 0.8533\n",
            "Epoch 140/500\n",
            "20109/20109 [==============================] - 6s 283us/sample - loss: 0.3026 - acc: 0.8759 - val_loss: 0.3402 - val_acc: 0.8537\n",
            "Epoch 141/500\n",
            "20109/20109 [==============================] - 6s 318us/sample - loss: 0.3011 - acc: 0.8766 - val_loss: 0.3392 - val_acc: 0.8551\n",
            "Epoch 142/500\n",
            "20109/20109 [==============================] - 6s 317us/sample - loss: 0.2989 - acc: 0.8768 - val_loss: 0.3391 - val_acc: 0.8555\n",
            "Epoch 143/500\n",
            "20109/20109 [==============================] - 6s 299us/sample - loss: 0.3022 - acc: 0.8771 - val_loss: 0.3394 - val_acc: 0.8537\n",
            "Epoch 144/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.2997 - acc: 0.8773 - val_loss: 0.3386 - val_acc: 0.8547\n",
            "Epoch 145/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.2991 - acc: 0.8782 - val_loss: 0.3383 - val_acc: 0.8555\n",
            "Epoch 146/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.2991 - acc: 0.8756 - val_loss: 0.3390 - val_acc: 0.8535\n",
            "Epoch 147/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.3005 - acc: 0.8795 - val_loss: 0.3397 - val_acc: 0.8527\n",
            "Epoch 148/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.2944 - acc: 0.8806 - val_loss: 0.3383 - val_acc: 0.8545\n",
            "Epoch 149/500\n",
            "20109/20109 [==============================] - 6s 291us/sample - loss: 0.2961 - acc: 0.8777 - val_loss: 0.3379 - val_acc: 0.8551\n",
            "Epoch 150/500\n",
            "20109/20109 [==============================] - 6s 323us/sample - loss: 0.2954 - acc: 0.8785 - val_loss: 0.3382 - val_acc: 0.8551\n",
            "Epoch 151/500\n",
            "20109/20109 [==============================] - 6s 304us/sample - loss: 0.2949 - acc: 0.8803 - val_loss: 0.3382 - val_acc: 0.8551\n",
            "Epoch 152/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.2936 - acc: 0.8796 - val_loss: 0.3376 - val_acc: 0.8561\n",
            "Epoch 153/500\n",
            "20109/20109 [==============================] - 6s 277us/sample - loss: 0.2913 - acc: 0.8822 - val_loss: 0.3376 - val_acc: 0.8559\n",
            "Epoch 154/500\n",
            "20109/20109 [==============================] - 6s 313us/sample - loss: 0.2957 - acc: 0.8793 - val_loss: 0.3379 - val_acc: 0.8559\n",
            "Epoch 155/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.2928 - acc: 0.8802 - val_loss: 0.3379 - val_acc: 0.8563\n",
            "Epoch 156/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.2939 - acc: 0.8810 - val_loss: 0.3384 - val_acc: 0.8537\n",
            "Epoch 157/500\n",
            "20109/20109 [==============================] - 6s 322us/sample - loss: 0.2913 - acc: 0.8818 - val_loss: 0.3378 - val_acc: 0.8557\n",
            "Epoch 158/500\n",
            "20109/20109 [==============================] - 6s 303us/sample - loss: 0.2913 - acc: 0.8800 - val_loss: 0.3376 - val_acc: 0.8557\n",
            "Epoch 159/500\n",
            "20109/20109 [==============================] - 6s 306us/sample - loss: 0.2908 - acc: 0.8824 - val_loss: 0.3376 - val_acc: 0.8559\n",
            "Epoch 160/500\n",
            "20109/20109 [==============================] - 6s 298us/sample - loss: 0.2901 - acc: 0.8822 - val_loss: 0.3377 - val_acc: 0.8553\n",
            "Epoch 161/500\n",
            "20109/20109 [==============================] - 6s 286us/sample - loss: 0.2908 - acc: 0.8792 - val_loss: 0.3380 - val_acc: 0.8541\n",
            "Epoch 162/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.2895 - acc: 0.8832 - val_loss: 0.3382 - val_acc: 0.8545\n",
            "Epoch 163/500\n",
            "20109/20109 [==============================] - 6s 290us/sample - loss: 0.2863 - acc: 0.8839 - val_loss: 0.3378 - val_acc: 0.8549\n",
            "Epoch 164/500\n",
            "20109/20109 [==============================] - 6s 302us/sample - loss: 0.2866 - acc: 0.8853 - val_loss: 0.3374 - val_acc: 0.8553\n",
            "Epoch 165/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.2882 - acc: 0.8833 - val_loss: 0.3377 - val_acc: 0.8547\n",
            "Epoch 166/500\n",
            "20109/20109 [==============================] - 6s 293us/sample - loss: 0.2869 - acc: 0.8826 - val_loss: 0.3375 - val_acc: 0.8547\n",
            "Epoch 167/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.2861 - acc: 0.8853 - val_loss: 0.3371 - val_acc: 0.8551\n",
            "Epoch 168/500\n",
            "20109/20109 [==============================] - 6s 310us/sample - loss: 0.2834 - acc: 0.8856 - val_loss: 0.3374 - val_acc: 0.8549\n",
            "Epoch 169/500\n",
            "20109/20109 [==============================] - 6s 297us/sample - loss: 0.2858 - acc: 0.8836 - val_loss: 0.3373 - val_acc: 0.8553\n",
            "Epoch 170/500\n",
            "20109/20109 [==============================] - 6s 292us/sample - loss: 0.2869 - acc: 0.8814 - val_loss: 0.3375 - val_acc: 0.8551\n",
            "Epoch 171/500\n",
            "20109/20109 [==============================] - 6s 300us/sample - loss: 0.2853 - acc: 0.8839 - val_loss: 0.3372 - val_acc: 0.8553\n",
            "Epoch 172/500\n",
            "20109/20109 [==============================] - 6s 296us/sample - loss: 0.2844 - acc: 0.8851 - val_loss: 0.3376 - val_acc: 0.8541\n",
            "Epoch 173/500\n",
            "20109/20109 [==============================] - 6s 277us/sample - loss: 0.2830 - acc: 0.8866 - val_loss: 0.3376 - val_acc: 0.8551\n",
            "Epoch 174/500\n",
            "20109/20109 [==============================] - 6s 301us/sample - loss: 0.2840 - acc: 0.8847 - val_loss: 0.3372 - val_acc: 0.8557\n",
            "Epoch 00174: early stopping\n",
            "--- 1089.4651737213135 seconds ---\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "20109/20109 [==============================] - 53s 3ms/sample - loss: 0.2663 - acc: 0.8920\n",
            "5027/5027 [==============================] - 13s 3ms/sample - loss: 0.3370 - acc: 0.8558\n",
            "Train Accuracy: 0.8920\n",
            "Train Loss: 0.2663\n",
            "Validation Accuracy:  0.8558\n",
            "Validation Loss: 0.3370\n",
            "5027/5027 [==============================] - 7s 1ms/sample\n",
            "[[1058  442]\n",
            " [ 283 3244]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.71      0.74      1500\n",
            "         1.0       0.88      0.92      0.90      3527\n",
            "\n",
            "    accuracy                           0.86      5027\n",
            "   macro avg       0.83      0.81      0.82      5027\n",
            "weighted avg       0.85      0.86      0.85      5027\n",
            "\n",
            "RMSE: 0.3798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhZW6ObDXI8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b9c890c5-a2dc-4ea5-a9e5-b643b50dda10"
      },
      "source": [
        "print(\"Average Train Accuracy: %.4f%% (+/- %.4f%%)\" % (np.mean(tr_acc_array), np.std(tr_acc_array)))\n",
        "print(\"Average Train Loss: %.4f%% (+/- %.4f%%)\" % (np.mean(tr_loss_array), np.std(tr_loss_array)))\n",
        "print(\"Average Validation Accuracy: %.4f%% (+/- %.4f%%)\" % (np.mean(te_acc_array), np.std(te_acc_array)))\n",
        "print(\"Average Validation Loss: %.4f%% (+/- %.4f%%)\" % (np.mean(te_loss_array), np.std(te_loss_array)))\n",
        "print(\"Average Time: %.4f%% (+/- %.4f%%)\" % (np.mean(time_array), np.std(time_array)))\n",
        "print(\"Average RMSE: %.4f%% (+/- %.4f%%)\" % (np.mean(rmse_array), np.std(rmse_array)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Train Accuracy: 0.8918% (+/- 0.0016%)\n",
            "Average Train Loss: 0.2689% (+/- 0.0034%)\n",
            "Average Validation Accuracy: 0.8518% (+/- 0.0043%)\n",
            "Average Validation Loss: 0.3453% (+/- 0.0063%)\n",
            "Average Time: 1045.6473% (+/- 30.4900%)\n",
            "Average RMSE: 0.3850% (+/- 0.0056%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaXCaRhZ7Otf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.DataFrame({'Train Acc': tr_acc_array, 'Train Loss': tr_loss_array, 'Validation Acc': te_acc_array, 'Validation Loss': te_loss_array, 'Time': time_array, 'RMSE': rmse_array})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzIFZ_ihR_Vk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "62cdcfd3-aaf3-45b9-db63-010331910a98"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train Acc</th>\n",
              "      <th>Train Loss</th>\n",
              "      <th>Validation Acc</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Time</th>\n",
              "      <th>RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.892282</td>\n",
              "      <td>0.270146</td>\n",
              "      <td>0.853023</td>\n",
              "      <td>0.344190</td>\n",
              "      <td>1021.375071</td>\n",
              "      <td>0.383376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.888607</td>\n",
              "      <td>0.274972</td>\n",
              "      <td>0.846628</td>\n",
              "      <td>0.348721</td>\n",
              "      <td>1004.690677</td>\n",
              "      <td>0.391627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.892834</td>\n",
              "      <td>0.266452</td>\n",
              "      <td>0.846827</td>\n",
              "      <td>0.355483</td>\n",
              "      <td>1045.573245</td>\n",
              "      <td>0.391373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.893033</td>\n",
              "      <td>0.266499</td>\n",
              "      <td>0.856574</td>\n",
              "      <td>0.341303</td>\n",
              "      <td>1067.131885</td>\n",
              "      <td>0.378716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.892038</td>\n",
              "      <td>0.266322</td>\n",
              "      <td>0.855779</td>\n",
              "      <td>0.337019</td>\n",
              "      <td>1089.465444</td>\n",
              "      <td>0.379765</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Train Acc  Train Loss  ...         Time      RMSE\n",
              "0   0.892282    0.270146  ...  1021.375071  0.383376\n",
              "1   0.888607    0.274972  ...  1004.690677  0.391627\n",
              "2   0.892834    0.266452  ...  1045.573245  0.391373\n",
              "3   0.893033    0.266499  ...  1067.131885  0.378716\n",
              "4   0.892038    0.266322  ...  1089.465444  0.379765\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3MQQcmILVxv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import time\n",
        "# from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "# start_time = time.time()\n",
        "\n",
        "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\n",
        "\n",
        "  \n",
        "# history = tpu_model.fit(x_train, y_train\n",
        "#                     ,epochs=epochs, verbose=1 \n",
        "#                     ,validation_split=0.2\n",
        "#                     ,batch_size=128 * 8\n",
        "#                     ,validation_data=(x_test, y_test)\n",
        "#                     ,callbacks=[es]\n",
        "#                    )\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# # tpu_model.save_weights('./tpu_model.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB09B2pkVxv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss, accuracy = tpu_model.evaluate(x_train, y_train, verbose=1)\n",
        "# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "# loss, accuracy = tpu_model.evaluate(x_test, y_test, verbose=1)\n",
        "# print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSoKam1eVxwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "0c5a2a4a-3b9a-48e3-ccbf-e0660cfc05ea"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAFACAYAAAD9D55TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKMA0luELvtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_df = pd.DataFrame(list(zip(df['review'].values, df['cleaned'].values, \\\n",
        "                                 df['sentiment'].values, y_pred)), \\\n",
        "                                  columns = ['review','cleaned','sentiment','predict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yBB1tI-SBqp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "dd1560da-9fe5-49b3-843b-f64000f21764"
      },
      "source": [
        "check_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>cleaned</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This movie is the beginning of the culmination...</td>\n",
              "      <td>beginning culmination masterfully woven cinema...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Over the past decade, Marvel has earned itself...</td>\n",
              "      <td>past decade earned benefit doubt consistently ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This film is way better than endgame!\\nThe act...</td>\n",
              "      <td>way better action better writing better dialog...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Summer movies often hype themselves as spectac...</td>\n",
              "      <td>summer often hype spectacular event missed ad ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I was amazed to see so many negative reviews; ...</td>\n",
              "      <td>amazed negative impossible please hour long co...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5022</th>\n",
              "      <td>Admittingly, I was not a fan of the original. ...</td>\n",
              "      <td>admittingly fan original found first eye candi...</td>\n",
              "      <td>0</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5023</th>\n",
              "      <td>Like the first round, it appears it is always ...</td>\n",
              "      <td>like first round appears always smart keep ori...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5024</th>\n",
              "      <td>With the exception of 'Captain America: The Wi...</td>\n",
              "      <td>exception america winter soldier cinematic uni...</td>\n",
              "      <td>1</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5025</th>\n",
              "      <td>I loved it! It was funny and witty and had all...</td>\n",
              "      <td>loved funny witty ingredient make best film fa...</td>\n",
              "      <td>1</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5026</th>\n",
              "      <td>There are a few minor spoilers. I will try to ...</td>\n",
              "      <td>minor spoiler try got first weekend got fun sp...</td>\n",
              "      <td>1</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5027 rows  4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 review  ...  predict\n",
              "0     This movie is the beginning of the culmination...  ...   [True]\n",
              "1     Over the past decade, Marvel has earned itself...  ...   [True]\n",
              "2     This film is way better than endgame!\\nThe act...  ...   [True]\n",
              "3     Summer movies often hype themselves as spectac...  ...   [True]\n",
              "4     I was amazed to see so many negative reviews; ...  ...   [True]\n",
              "...                                                 ...  ...      ...\n",
              "5022  Admittingly, I was not a fan of the original. ...  ...  [False]\n",
              "5023  Like the first round, it appears it is always ...  ...   [True]\n",
              "5024  With the exception of 'Captain America: The Wi...  ...  [False]\n",
              "5025  I loved it! It was funny and witty and had all...  ...   [True]\n",
              "5026  There are a few minor spoilers. I will try to ...  ...  [False]\n",
              "\n",
              "[5027 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX4xYExnsOex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_df.to_csv('check_lstm.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvFNRqJf2_ph",
        "colab_type": "text"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE6l-_ye928X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nan = pd.read_csv(\"nan.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46sUeQBIGzCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "96c990b1-490c-4656-acdf-ae62e02a635b"
      },
      "source": [
        "nan.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>rating</th>\n",
              "      <th>title</th>\n",
              "      <th>review</th>\n",
              "      <th>number</th>\n",
              "      <th>cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>69</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A tedious and dependent film</td>\n",
              "      <td>SPOILER: The plot is simple that the supervill...</td>\n",
              "      <td>0</td>\n",
              "      <td>spoiler simple supervillian succeeds russo bro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>126</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Thanos the Emo Crybaby...Thanos Mickey Mouse S...</td>\n",
              "      <td>12 jokes within 3 minutes in conversation betw...</td>\n",
              "      <td>0</td>\n",
              "      <td>joke within minute conversation doctor strange...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>139</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A snore -- literally</td>\n",
              "      <td>This film was complete and utter tosh. Judging...</td>\n",
              "      <td>0</td>\n",
              "      <td>complete utter tosh judging way positively rev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>146</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Thanos and Bubbles, Kiddie Mickey Mouse Style</td>\n",
              "      <td>I don't recognize the THANOS The MAD TITAN fro...</td>\n",
              "      <td>0</td>\n",
              "      <td>recognize thanos mad titan comic book thanos w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>176</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Good movie, but completely ruined</td>\n",
              "      <td>Spoilers!!!Spoilers: the movie was pretty good...</td>\n",
              "      <td>0</td>\n",
              "      <td>spoiler spoiler pretty good bit quippy good en...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                            cleaned\n",
              "0          69  ...  spoiler simple supervillian succeeds russo bro...\n",
              "1         126  ...  joke within minute conversation doctor strange...\n",
              "2         139  ...  complete utter tosh judging way positively rev...\n",
              "3         146  ...  recognize thanos mad titan comic book thanos w...\n",
              "4         176  ...  spoiler spoiler pretty good bit quippy good en...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqUZKx2pHjb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a26138ee-f7b8-46f4-d5b6-9d8fd6e52c47"
      },
      "source": [
        "nan.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(533, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZmDfVllGvgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sent in nan:\n",
        "    review = nan['review'].values\n",
        "    cleaned = nan['cleaned'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQDuXnbdG_bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_str = cleaned.astype(str)\n",
        "\n",
        "tokenizer = Tokenizer(char_level=False)\n",
        "tokenizer.fit_on_texts(cleaned_str)\n",
        "cleaned_bow = tokenizer.texts_to_sequences(cleaned_str) #text_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5sbZADIHAYc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "083b0f87-8d6c-4fbe-be73-72d50f8052c6"
      },
      "source": [
        "x_pred = pad_sequences(cleaned_bow, maxlen=max_len)\n",
        "print('Shape of data tensor:', x_pred.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (533, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tBYTrU1Z3Gee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c663d149-4908-498e-e5e9-eaf99d8862a7"
      },
      "source": [
        "pred = model.predict(x_pred, batch_size=100, verbose = 1)\n",
        "y_pred = (pred > 0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "533/533 [==============================] - 1s 1ms/sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4C7LWibHHRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df = pd.DataFrame(list(zip(review, y_pred)), columns = ['review','predict'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prz4vzbfHxgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "64f59196-3398-4256-bc99-4f2b225e6bf5"
      },
      "source": [
        "pred_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SPOILER: The plot is simple that the supervill...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12 jokes within 3 minutes in conversation betw...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This film was complete and utter tosh. Judging...</td>\n",
              "      <td>[False]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I don't recognize the THANOS The MAD TITAN fro...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spoilers!!!Spoilers: the movie was pretty good...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>528</th>\n",
              "      <td>Just finished watching this movie. My husband ...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>529</th>\n",
              "      <td>Ok at best. Big script let down from what's su...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530</th>\n",
              "      <td>Had this movie come out in 2009 close to \"Iron...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>531</th>\n",
              "      <td>I'm a middle-aged white male. Saw Captain Marv...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>PROS:Brie Larson's Is Great As Captain MarvelS...</td>\n",
              "      <td>[True]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>533 rows  2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                review  predict\n",
              "0    SPOILER: The plot is simple that the supervill...   [True]\n",
              "1    12 jokes within 3 minutes in conversation betw...   [True]\n",
              "2    This film was complete and utter tosh. Judging...  [False]\n",
              "3    I don't recognize the THANOS The MAD TITAN fro...   [True]\n",
              "4    Spoilers!!!Spoilers: the movie was pretty good...   [True]\n",
              "..                                                 ...      ...\n",
              "528  Just finished watching this movie. My husband ...   [True]\n",
              "529  Ok at best. Big script let down from what's su...   [True]\n",
              "530  Had this movie come out in 2009 close to \"Iron...   [True]\n",
              "531  I'm a middle-aged white male. Saw Captain Marv...   [True]\n",
              "532  PROS:Brie Larson's Is Great As Captain MarvelS...   [True]\n",
              "\n",
              "[533 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzteJqDNHyPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df.to_csv('pred_lstm.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3IJpIgSvefv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1ba45dcf-f785-49db-bfe5-657de33aa3bb"
      },
      "source": [
        "model.save(\"lstm.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMw2C2U3voMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
